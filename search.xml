<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Linux文本处理三剑客之awk]]></title>
      <url>%2Fblog%2FLinux%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bawk%2F</url>
      <content type="text"><![CDATA[awk 的名称源自其创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母，拥有强大的文本编辑能力。 awk 基本语法： awk &apos;BEGIN&#123;&#125; &#123;command&#125; END&#123;&#125;&apos; filename awk 是逐行处理文本内容的；BEGIN{} 是初识化代码块，在处理文件第一行内容之前，定义一些变量；{command} 为一些命令，对文件内容的每一行进行相应地处理； END{} 为结束代码块，在{command} 运行结束后执行。 BEGIN{} 和 END{} 都不是必须的，所以，往往可以这样写： awk &apos;&#123;command&#125;&apos; filename 数据前十行如下： awk 的输入被解析成多个记录（Record），默认的记录分隔符是 \n，因此可以认为一行就是一个记录，记录的分隔符（当前行和下一行之间的分隔符）可以通过内置变量 RS （record separator）更改。而对于当前行，默认分隔符为空格（包括 tab）；通过分隔符，将一行数据分割成许多列，如打印文件的第一列或第二列： 或第一列和第二列： 不错，可能你已经猜到，$1 就是只第一列，相应的地，$2 表示第二列等。而 $0 则表示所有的列： liuhui@LIUHUI:shengxinbaike$ awk &apos;&#123;print $0&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Drought Controlgene1 708 45 112 795gene2 341 617 304 556gene3 756 500 67 217gene4 685 100 57 400 awk 用内置变量 NR 记录当前的行号： 所以，只打印第一行，可以这样做： $ awk &apos;NR==1&#123;print $0&#125;&apos; sampledata.txtgenename Cold Heat Drought Control 不想打印第一行，则这样： $ awk &apos;NR!=1&#123;print $0&#125;&apos; sampledata.txt | head -n 5gene1 708 45 112 795gene2 341 617 304 556gene3 756 500 67 217gene4 685 100 57 400gene5 233 526 308 658 awk 可以很方便地对不同列之间地值进行加减乘除等运算： # 加$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2+$5,$3+$5,$4+$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 1503 840 907gene2 897 1173 860gene3 973 717 284gene4 1085 500 457# 减$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2-$5,$3-$5,$4-$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 -87 -750 -683gene2 -215 61 -252gene3 539 283 -150gene4 285 -300 -343# 乘$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2*$5,$3*$5,$4*$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 562860 35775 89040gene2 189596 343052 169024gene3 164052 108500 14539gene4 274000 40000 22800# 除（在这里是 试验组 vs 对照组 的 fold change）$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 0.890566 0.0566038 0.140881gene2 0.613309 1.10971 0.546763gene3 3.48387 2.30415 0.308756gene4 1.7125 0.25 0.1425 求 log （e 为底）也不在话下： $ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,log($2/$5),log($3/$5),log($4/$5)&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 -0.115898 -2.87168 -1.95984gene2 -0.488886 0.104101 -0.603741gene3 1.24814 0.834711 -1.1752gene4 0.537954 -1.38629 -1.94841 awk 中支持以下数学函数（还有补充吗）： atan2(y,x)：余切； cos(x)：余弦； sin(x)：正弦； ​exp(x)：以自然对数 e 为底指数幂； log(x)：计算以 e​ 为底的对数值； sqrt(x)：开方； int(x)：将数值转换成整数； rand()：返回 0 到 1 的一个随机数值，不包含 1； srand([expr])：设置随机种子，一般与 rand 配合使用，如果参数为空，默认使用当前时间为种子。 对其中某一列求和，也很简单： # 对第二列求和$ awk &apos;NR!=1&#123;sum += $2&#125; END&#123;print sum&#125;&apos; sampledata.txt16962 END 的作用是前面的部分 NR!=1{sum += $2} 运行完毕后，才执行的。 通过 awk，还可以非常迅速的统计出文件内容的列数： $ awk &apos;&#123;print NF&#125;&apos; sampledata.txt | head -n 555555 NF （number of fields，域的数量，域就是列），是 awk 的内置标量；其默认按空格分割，记录每一行有多少列。 所以打印最后一列可以这样： $ awk &apos;&#123;print $NF&#125;&apos; sampledata.txt | head -n 5Control795556217400 awk 可以改变文件内容的分隔符： $ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;print $1,$2,$3,$4,$5&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400 OFS （output field separator），可以指定输出内容的分隔符。 注意： awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;print $0&#125;&apos; sampledata.txt 不会改变输出内容的分隔符。为什么呢？ 如果文件内容有 100 列，应该怎么处理呢？ 可以这样： # 方法 1$ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;NF=NF; print $0&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400# 方法 2liuhui@LIUHUI:shengxinbaike$ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;$1=$1; print $0&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400 原理是当对 $1、$2 等等以及列数 NF 的赋值时，$0 会用 OFS 进行重构（即 OFS 生效）。 可以将重新指定分隔符的内容重定向到一个新的文件里： awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;$1=$1; print $0&#125;&apos; sampledata.txt &gt; sampledata_new.txt 看看这时直接打印第一列会怎样： 不是预期效果： 这时，就要指定输入分隔符的值了： $ awk &apos;BEGIN&#123;FS=&quot;;&quot;&#125;&#123;print $1&#125;&apos; sampledata_new.txt | head -n 5genenamegene1gene2gene3gene4 FS （field separator）的作用是指定输入内容的分隔符，与 OFS 相反。 注意，这样写是不行的： $ awk &apos;&#123;FS=&quot;;&quot;; print $1&#125;&apos; sampledata_new.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1gene2gene3gene4 第一行不对。 这是因为 BEGIN 的作用是初始化，即在读入文件内容的第一行之前，就定义好了 FS 的值；如果不用 BEGIN，显然会出点小 bug。 还有一个办法： $ awk -F &quot;;&quot; &apos;&#123;print $1&#125;&apos; sampledata_new.txt | head -n 5genenamegene1gene2gene3gene4 通过参数 -F 指定分隔符。 小结一下 awk 一部分内置变量： 变量 $n 当前记录的第 n 个字段，字段间由 FS 分隔，如 $1、$2 等 $0 完整的输入记录，即当前的那一行 NR 当前记录数，即行数 FS 当前记录分隔符 （默认是空格） NF 当前记录中的列数 OFS 输出列的分隔符（默认值是一个空格） awk 还可以做一些筛选，如： # 第二列的值大于 1 的所对应的行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$2&gt;1&apos;| head -n 5gene3 3.48387 2.30415 0.308756gene4 1.7125 0.25 0.1425gene6 1.34752 0.553191 1.82979gene10 2.89815 6.76852 2.4537gene11 3 23.3 38.4# 第二列的值小于 1 的所对应的行liuhui@LIUHUI:shengxinbaike$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$2&lt;1&apos;| head -n 5gene1 0.890566 0.0566038 0.140881gene2 0.613309 1.10971 0.546763gene5 0.354103 0.799392 0.468085gene7 0.829047 1.15129 0.121029gene8 0.297456 0.64775 1.32485# 只提取 &quot;gene1&quot; 所在的行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$1==&quot;gene1&quot;&apos;gene1 0.890566 0.0566038 0.140881# 第二行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;NR==2&apos;gene2 0.613309 1.10971 0.546763# 奇数行（% 为求余数）$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;NR%2 == 1&apos; | head -n 5gene1 0.890566 0.0566038 0.140881gene3 3.48387 2.30415 0.308756gene5 0.354103 0.799392 0.468085gene7 0.829047 1.15129 0.121029gene9 0.53317 1.00246 0.538084 由于 awk 内容较多，上面只是讲了一些基础及一些使用技巧，而文章篇幅有限，不便讲述太多的内容。本文的目的是作为一个引子，引导初学者入门，更多的内容，可以通过关键字在百度或谷歌搜索：“linux awk”，能查询到非常多的优秀的教程（这个方法对其它 linux 命令同样适用）。 也可以加入我们的 QQ 群：575383226 ，一起讨论各种生信问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux文本处理三剑客之grep]]></title>
      <url>%2Fblog%2FLinux%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bgrep%2F</url>
      <content type="text"><![CDATA[在 linux 中，有三种非常强大的文本处理命令，awk、sed 与 grep，号称 linux 三剑客。它们之间有不少相似点，但同样各具特色；三者均可以进行文本匹配，而 awk 和 sed 还可以进行文本编辑。 由于之前已经在生信入门系列之 linux 入门（三）：基础命令（中）文章中对 grep 命令进行了简单的讲解，所以今天主要给大家介绍一下三剑客中的第一剑：grep。 以拟南芥的 gff3 文件为例，进行讲解。 $ wget ftp://ftp.arabidopsis.org/home/tair/Genes/TAIR10_genome_release/TAIR10_gff3/TAIR10_GFF3_genes.gff 一般来说，对于一个文本文件，可以都要先用 less 简单看一下文件的内容： 利用之前学过的命令，并且可以很简单的统计出拟南芥有多少条染色体： $ cut -f 1 TAIR10_GFF3_genes.gff | uniqChr1Chr2Chr3Chr4Chr5ChrCChrM 查看各个染色体的基因数： $ grep &quot;^Chr1&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;9117$ grep &quot;^Chr2&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;6343$ grep &quot;^Chr3&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;7610$ grep &quot;^Chr4&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;5851$ grep &quot;^Chr5&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;8313$ grep &quot;ChrC&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;133$ grep &quot;^ChrM&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;146 其中， grep &quot;^Chr1&quot; TAIR10_GFF3_genes.gff 是将 TAIR10_GFF3_genes.gff 文件中所有含有以 Chr1 开头的行提取出来， 然后将这些信息通过管道传给 grep -c &quot;gene&quot;，统计其中含有 gene 的行数。 可以发现，上面的几个命令中，基本相同，只有“Chr”在变。所以可以通过一个简单的 for 循环，完成上面的操作。 for i in Chr1 Chr2 Chr3 Chr4 Chr5 ChrC ChrMdogrep &quot;^$i&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;done 如图所示： 其中的 $i 依次代表 Chr1、Chr2、Chr3、Chr4、Chr5、ChrC 和 ChrM。 下面继续讲解 grep 中的其他参数。 如下图所示，参数 -l 的作用是：如果文件中含有以 Chr1 开头的行（至少一行），则将文件名打印出来。我们之前已经知道，TAIR10_GFF3_genes.gff 文件中不含有以 chr1 开头的行（区分大小写），故没有结果。 试想，如果有成千上万个文件，我们想知道哪些文件含有某个特定的关键字（如上述的 Chr1），就可以结合 for 循环，将这些文件找出来。假如目录 test_dir 中含有几万个文件，则可以这样做： for i in $(ls test_dir)dogrep -l &quot;关键字&quot; $idone &gt; check_result.txt 把含有关键字的文件名存到 check_result.txt 中。这个 $(ls test_dir) 是什么意思呢？请看生信入门系列之——Shell 脚本编程（一）。 如果刚好相反，把不含某个关键字的文件名找出来，则用 -L 代替 -l。 也可以使用参数 -i 来忽略大小写： 在一些文件中，有时会有空白行，这时，用 grep 可以去掉： 其中的 ^$ 表示空白行，-v 则表示反向匹配，即将非空白行取出来。 如果你要问，^$ 为什么表示空白行，则可以理解：^ 表示匹配行首，$ 则表示匹配行尾，它们这样组合则表示行首和行为之间什么都没有，那就是空白行了。 其实，前面提到的 ^Chr1 和 ^$，都是正则表达式的用法（类似我们熟知的通配符 * 和 ?），sed、awk 和 grep 都支持正则表达式。 如果有文件 Chrs.txt，内容是： $ cat Chrs.txtchr1chr2chr3chr4chr5chr6chr7chr8chr9chr10 如果要匹配 chr1，按之前的做法： 显然，chr10 也含有 chr1，不是我们期望得到的，但可以这样做： \&gt; 表示“词尾锚定”，即限定右边的边界。 如果想一次将 chr2 和 chr3 匹配出来，可以这样做： $ grep &quot;chr[23]&quot; Chrs.txtchr2chr3 [23] 表示 2 或 3 中的任意一个，当然可以不止两个： $ grep &quot;chr[23456]&quot; Chrs.txtchr2chr3chr4chr5chr6 但这样不是好办法，可以这样做： grep &quot;chr[2-6]&quot; Chrs.txtchr2chr3chr4chr5chr6 [2-6] 表示 2 到 6 中的任意一个。 相应地 [a-z] 表示小写字母 a 到小写字母 z 中地任意一个，等。 而 $ grep &quot;chr[^2-6]&quot; Chrs.txtchr1chr7chr8chr9chr10 则表示把不含有 chr2 到 chr6 关键字其他行取出来，其中的 ^ 表示 非，而不是表示匹配行首。这时和 grep -v 作用一样： $ grep -v &quot;chr[2-6]&quot; Chrs.txtchr1chr7chr8chr9chr10]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[外业-影集]]></title>
      <url>%2Fblog%2F%E5%A4%96%E4%B8%9A-%E5%BD%B1%E9%9B%86%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从FASTA比对产生的氨基酸序列的alignment中删除frame shift]]></title>
      <url>%2Fblog%2F%E4%BB%8EFASTA%E6%AF%94%E5%AF%B9%E4%BA%A7%E7%94%9F%E7%9A%84%E6%B0%A8%E5%9F%BA%E9%85%B8%E5%BA%8F%E5%88%97%E7%9A%84alignment%E4%B8%AD%E5%88%A0%E9%99%A4frame-shift%2F</url>
      <content type="text"><![CDATA[用 FASTA 比对产生的protein-basedalignment中，会引入 frame shift，“/R” 表示在 R 对应的密码子中有一个碱基缺失，“\R” 则表示在 R 对应的密码子中有一个碱基插入（可能插入或缺失两个或以上，不确定；以下程序认为只有一个插入或缺失一个碱基）。这些 indel 的存在使得序列无法按 codon alignment 对齐。以下脚本从 FASTA 比对产生的含有 frame shift 的 alignment 中删除frame shift ，得到无 frame shift 的 alignment。 import sysfrom Bio import SeqIOfrom Bio.Seq import Seqdef posFind(pssdSeq, pareSeq, key): # input pssd seq and it's parental seq and # frame shift symbol, return the real position # of pssd seq and and it's parental seq, and the # position of frame shift in aligned seq count = 0 pssdSeq_raw = pssdSeq # make a copy pssd_frameshift = [] # real position of pssd seq pssdSeq_raw_frameshift = [] # the raw aligned position pare_to_frameshift = [] pssdSeq = pssdSeq.replace("-", "") control = True while control: if count &gt; 0: pos1 = pssdSeq.find(key, pos1 + 1) pos2 = pssdSeq_raw.find(key, pos2 + 1) if pos1 == -1: control = False else: pssd_frameshift.append(pos1) pssdSeq_raw_frameshift.append(pos2) count += 1 else: pos1 = pssdSeq.find(key) pos2 = pssdSeq_raw.find(key) pssd_frameshift.append(pos1) pssdSeq_raw_frameshift.append(pos2) count += 1 temp = [] # alignment frame shift pos (i.e. "/R" position) aligned_frameshif_pos = [[k, k+1] for k in pssdSeq_raw_frameshift] for pos in pssdSeq_raw_frameshift: temp.append(pos+1) for s in temp: num = 0 for index, i in enumerate(pareSeq): if i == "-": num += 1 if index == (s-1): pare_to_frameshift.append(s - num) pare_to_frameshift = [t+1 for t in pare_to_frameshift[:]] pare_to_frameshift_cds = [] for j in pare_to_frameshift: pare_to_frameshift_cds.append([j*3-2,j*3]) pssd_frameshift = [n-m for m, n in enumerate(pssd_frameshift[:])] pare_to_frameshift_cds = [[w[0] - 1, w[1] - 1] for w in pare_to_frameshift_cds[:]] return pssd_frameshift, pare_to_frameshift_cds, aligned_frameshif_posdef fshiftCoor(frameshiftPos, posIns, posDel): # return the coordinate of frameshift in seq # 将 frameshift 对应的序列的坐标找出来， frameshiftpos_1based = [i + 1 for i in frameshiftPos] # 1-based posIns_1based = [i + 1 for i in posIns] posDel_1based = [i + 1 for i in posDel] if frameshiftPos == posIns: # only insertion "\" frameshift_coor = [] for index1, pos1 in enumerate(frameshiftpos_1based): temp_lis = [pos1 * 3 - 2 + index1] + [pos1 * 3 + 1 + index1] frameshift_coor.append(temp_lis) elif frameshiftPos == posDel: # only deletion "/" frameshift_coor = [] for index2, pos2 in enumerate(frameshiftpos_1based): temp_lis = [pos2 * 3 - 2 - index2] + [pos2 * 3 - 1 - index2] frameshift_coor.append(temp_lis) else: if posIns[0] &gt; posDel[-1]: # Insertion in the right of the deletion xxx xx xxx xx xxxx frameshift_coor = [] for index3, pos3 in enumerate(posDel_1based): temp_lis = [pos3 * 3 -2 - index3] + [pos3 * 3 - 1 - index3] frameshift_coor.append(temp_lis) for index4, pos4 in enumerate(posIns_1based): temp_lis = [pos4 * 3 -2 + index4 - index3] + [pos4 * 3 + 1 + index4 - index3] frameshift_coor.append(temp_lis) elif posIns[-1] &lt; posDel[0]: # Insertion in the left of the deletion xxx xxxx xxx xxxx xx frameshift_coor = [] for index5, pos5 in enumerate(posIns_1based): temp_lis = [pos5 * 3 -2 + index5] + [pos5 * 3 + 1 + index5] frameshift_coor.append(temp_lis) for index6, pos6 in enumerate(posDel_1based): temp_lis = [pos6 * 3 -2 - index6 + index5] + [pos6 * 3 - 1 - index6 + index5] frameshift_coor.append(temp_lis) else: frameshift_coor = [] # xxx xxxx xxx xx xxx xxxx xxx xx xxx num_del = 0 num_ins = 0 for pos7 in frameshiftpos_1based: if pos7 in posIns_1based: temp_lis = [pos7 * 3 - 2 + num_ins - num_del] + [pos7 * 3 + 1 + num_ins - num_del] frameshift_coor.append(temp_lis) num_ins += 1 else: temp_lis = [pos7 * 3 - 2 - num_del + num_ins] + [pos7 * 3 + 1 - num_del + num_ins] frameshift_coor.append(temp_lis) num_del +=1 frameshift_coor = [[c[0]-1, c[1]-1]for c in frameshift_coor[:]] return frameshift_coordef cleanFrameshift(frameshiftCoor, chrCoor): # 根据 frameshift 在 序列上的坐标，找出不含frameshift # 的坐标 clean_frameshift = [] for index, j in enumerate(frameshiftCoor): if len(frameshiftCoor) == 1 and index == 0: clean_frameshift.append([int(chrCoor[0]) - 1, j[0]]) clean_frameshift.append([j[1] + 1, int(chrCoor[1]) + 1]) elif len(frameshiftCoor) &gt; 1 and index == 0: clean_frameshift.append([int(chrCoor[0]) - 1, j[0]]) elif len(frameshiftCoor) &gt; 1 and index == (len(frameshiftCoor) -1): clean_frameshift.append([frameshiftCoor[index-1][1] + 1, j[0]]) clean_frameshift.append([j[1] + 1, int(chrCoor[1]) + 1]) else: clean_frameshift.append([frameshiftCoor[index-1][1] + 1, j[0]]) # clean_frameshift = [[s[0]-1, s[1]-1] for s in clean_frameshift[:]] return clean_frameshift USAGE = "\nusage: python %s genPgeneResult_PSSD.txt ptaeda.v1.01_masked.trimmed.fa pta_cds.fas genFullAln_PSSD_frameshift.txt PSSD_frameshift_removed.txt PSSD_frameshift.fa\n" % sys.argv[0]if len(sys.argv) != 7: print USAGE sys.exit()# read the "genPgeneResult.txt" file and store the # coordinate informations of "pssd" into a dictionary "COOR_DICT" COOR_DICT = &#123;&#125;with open(sys.argv[1], 'r') as f1: for line1 in f1: lsplit1 = line1.split() if "PSSD1" == lsplit1[17] or "PSSD2" == lsplit1[17]: COOR_DICT[lsplit1[5]] = lsplit1[0:5] + lsplit1[6:8]# store the fasta file into memory using dictgenome_infa = SeqIO.parse(open(sys.argv[2]), 'fasta')cds_infa = SeqIO.parse(open(sys.argv[3]), 'fasta')#genome_DICT = &#123;&#125;for rec1 in genome_infa: genome_DICT[rec1.id] = str(rec1.seq)genome_infa.close()#cds_DICT = &#123;&#125;for rec2 in cds_infa: cds_DICT[rec2.id] = str(rec2.seq)cds_infa.close()#ALN_DICT = &#123;&#125;with open(sys.argv[4], 'r') as f2: for line2 in f2: if "&gt;" == line2[0]: aln_name = line2.split()[0][1:] ALN_DICT[aln_name] = [] else: ALN_DICT[aln_name].append(line2.rstrip())# extract the pair of partenal gene and pseudogeneOUT1 = open(sys.argv[5], 'w')OUT2 = open(sys.argv[6], 'w')for id, lis in COOR_DICT.items(): # scaffold249595===PITA_000071929-RA===1 ['scaffold249595', '10578', '10691', '+', 'PITA_000071929-RA', '131', '168'] if lis[4] not in cds_DICT: continue if id not in ALN_DICT: continue g_seq = genome_DICT[lis[0]][int(lis[1])-1: int(lis[2])] c_seq = cds_DICT[lis[4]][int(lis[5])*3-3: int(lis[6])*3] # reverse complement of sequences if seqStart larger than seqEnd if "-" == lis[3]: g_seq = str(Seq(g_seq).reverse_complement()) if int(lis[5]) &gt; int(lis[6]): c_seq = str(Seq(c_seq).reverse_complement())# chr_coor = lis[1:3] chr_coor = [1, len(g_seq)] # 1-based cds_coor = [1, len(c_seq)] align_range = [1, len(ALN_DICT[id][0])] pos_del = [] pos_ins = [] if "/" in ALN_DICT[id][1] and "\\" in ALN_DICT[id][1]: pos_del, parent_to_del, frameshif_pos_in_alignment_del = posFind(ALN_DICT[id][1], ALN_DICT[id][0], "/") pos_ins, parent_to_ins, frameshif_pos_in_alignment_ins = posFind(ALN_DICT[id][1], ALN_DICT[id][0], "\\") pos_lis = pos_del + pos_ins frameshift_pos = sorted(pos_lis) parent_to_indel = parent_to_del + parent_to_ins parent_to_indel = sorted(parent_to_indel) frameshif_pos_in_alignment = frameshif_pos_in_alignment_del + frameshif_pos_in_alignment_ins frameshif_pos_in_alignment = sorted(frameshif_pos_in_alignment)# print id, frameshift_pos elif "/" in ALN_DICT[id][1]: pos_del, parent_to_indel, frameshif_pos_in_alignment = posFind(ALN_DICT[id][1], ALN_DICT[id][0], "/") frameshift_pos = pos_del# print id, frameshift_pos elif "\\" in ALN_DICT[id][1]: pos_ins, parent_to_indel, frameshif_pos_in_alignment = posFind(ALN_DICT[id][1], ALN_DICT[id][0], "\\") frameshift_pos = pos_ins# print id, frameshift_pos # write out the alignment removed frame shift clean_frameshift_align = cleanFrameshift(frameshif_pos_in_alignment, align_range) g_seq_align = '' c_seq_align = '' for reg in clean_frameshift_align: g_seq_align += ALN_DICT[id][1][reg[0]: reg[1]] c_seq_align += ALN_DICT[id][0][reg[0]: reg[1]] OUT1.write("&gt;" + id + "\n" + c_seq_align + "\n" + g_seq_align + "\n") # frameshift_coor frameshift_coor = fshiftCoor(frameshift_pos, pos_ins, pos_del) # chr_coor removed frame shift site(s) # [[35911, 36065], [36067, 36196], [36198, 36240], [36242, 36280]] clean_frameshift = cleanFrameshift(frameshift_coor, chr_coor) clean_frameshift_to_cds = cleanFrameshift(parent_to_indel, cds_coor) # extract seq g_seq_clean = '' c_seq_clean = '' for frag1 in clean_frameshift: if len(frag1) == 1: g_seq_clean += g_seq[frag1[0]: frag1[1] + 1] else: g_seq_clean += g_seq[frag1[0]: frag1[1]] for frag2 in clean_frameshift_to_cds: if len(frag2) == 1: c_seq_clean += c_seq[frag2[0]: frag2[1] + 1] else: c_seq_clean += c_seq[frag2[0]: frag2[1]] OUT2.write("&gt;" + id + "\n" + c_seq_clean + "\n" + g_seq_clean + "\n")OUT1.close()OUT2.close()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R 抓取 plantrgdb 信息]]></title>
      <url>%2Fblog%2FR-%E6%8A%93%E5%8F%96-plantrgdb-%E4%BF%A1%E6%81%AF%2F</url>
      <content type="text"><![CDATA[尚未理解 htmlTreeParse 和 xpathSApply 的用法，有机会再研究。 # 载入包library(XML)species &lt;- "http://aegilops.wheat.ucdavis.edu/plantrgdb/download.php"# 解析网页doc &lt;- htmlTreeParse(species, useInternal=TRUE, encoding="UTF-8")# 抓取有用信息species &lt;- xpathSApply(doc, "//a", xmlGetAttr, 'href')# 物种的数量sp_num &lt;- length(species[grep("download",species)][-1])# 对所有的物种进行循环for (sp in 1:sp_num)&#123; # 获取物种名 sp_name &lt;- strsplit(species[grep("download",species)][-1], "/")[[sp]][2] print(sp_name) # 需要的信息有 9 列，先创建一个 9 列的空数组 sp_array &lt;- array(NA, c(0, 9)) # 为空数组列名重命名 colnames(sp_array) &lt;- c("Species", "Coordinates", "Strand", "Lost intron", "Parent coverage", "Parent identity", "Ka", "Ks", "Ka/Ks") sp_url &lt;- paste("http://aegilops.wheat.ucdavis.edu/plantrgdb/browse_result.php?type=total&amp;species=", sp_name, sep="") # 对每一物种的网页进行解析 sp_doc &lt;- htmlTreeParse(sp_url, useInternal=TRUE, encoding="UTF-8") temp &lt;- xpathSApply(sp_doc, "//a", xmlGetAttr, 'href') # 获取 retrocopies 数量 retro_num &lt;- length(grep("=",temp)) # 对每一个 retrocopy 依次进行解析，并将结果写进 sp_array 这个数组里 for (i in 1:retro_num)&#123; retro_id &lt;- strsplit(temp[grep("=",temp)],"=")[[i]][2] print(retro_id) retro_id_url &lt;- paste("http://aegilops.wheat.ucdavis.edu/plantrgdb/retrocopy_info.php?retrocopyID=", retro_id, sep = "") sp_array &lt;- rbind(sp_array, t(readHTMLTable(retro_id_url, which = 1)[retro_id])) &#125; # 将 sp_array 中的信息写出到 csv 文件中 write.csv(sp_array, paste(sp_name, ".csv", sep = ""))&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[生信入门系列之-linux-入门（四）：基础命令（下）]]></title>
      <url>%2Fblog%2F%E7%94%9F%E4%BF%A1%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B9%8B-linux-%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
      <content type="text"><![CDATA[在实际的 linux 服务器中运行软件时，经常会为一些长时间运行的任务而头疼。当关掉连接终端或者其它原因，如网络不稳定等，运行中的程序也会自动被中断。那么怎么去解决上述问题呢？可以用命令 nohup，像这样运行: nohup script.sh &amp; 其中 script.sh 是一个shell 脚本，当然也可以替换成其它的软件或命令。如下图所示： 其中的“13632”是PID，PID 是进程的代号，每个进程有唯一的 PID 编号。每个命令或程序运行后都会产生一个 PID。 可以用命令 ps 或 top 命令查看某个程序的 PID： ps 查看： top 查看： top 命令来实时监测系统正在运行什么程序以及系统现在的存储内存消耗等，其中有 PID 信息。 如果想中断某个程序，可以用 kill 命令： kill PID 有时需要在 Linux 下进行远程拷贝文件的命令，可以用命令 scp（本地有 linux 终端）： scp [参数] [原路径] [目标路径] 如果拷贝文件： # 将文件从本电脑上传到远程服务器 data 目录里# xxx.xxx.xxx.xxx 代表 ip 地址scp file1 liuhui@xxx.xxx.xxx.xxx:/home/liuhui/data# 将文件从远程服务器拷贝到本地电脑，并保存到 sample_data 目录里scp liuhui@xxx.xxx.xxx.xxx:/home/liuhui/data/file1 /home/liuhui/sample_data /home/liuhui/data/file1 的路径如果很长，可以用 readlink 命令获得（在这里不是必须的，但这个命令在 shell 脚本中用比较方便）： readlink -f filename ip 地址可以用 ifconfig 命令查看： 如果要拷贝目录， 则加一个参数 -r，和命令 cp 类似。 有时需要从网上下载软件，可以用 wget 命令（在 windows 也有同名软件）： wget url “url” 是文件的下载地址，鼠标右键可以获取： 运行 wget 命令： 如果文件很大，可以尝试用命令 axel 多线程下载（有些网站上的文件不支持这个命令，这是就用 wget）： 下载速度明显提升很多，因为我用参数 -n 指定适用 8 个线程（普通笔记本 4 个线程）。 在 shell 脚本（以后会有一个系列讲 shell 脚本）中经常会用到两个命令，basename 和 dirname。下面举例说明。 假如我有这个信息： /home/shengwu004/example/longReads.fa 通过 basename，可以得到： [shengwu004@fatnode example]$ basename /home/shengwu004/example/longReads.falongReads.fa 也可以去掉后缀： basename /home/shengwu004/example/longReads.fa .falongReads dirname 则相反： [shengwu004@fatnode example]$ dirname /home/shengwu004/example/longReads.fa/home/shengwu004/example 服务器的磁盘大小和使用情况是我们比较关心的，可以用 df 查看： 命令 du 可以查看某个目录中所有文件的大小，： 如果想了解某个命令的所有参数，可以用 man 查看，如： man du 按上下键翻页，按 q 退出。 用命令 history 可以查看所有用过命令： history 命令及简单描述 命令 nohup script.sh &amp; 后台运行命令或程序 top 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 ps 列出当前进程的快照，就是执行ps命令的那个时刻的那些进程 kill 根据PID，强制终止后台运行的命令或程序 scp 远程拷贝文件或目录 ifconfig 查看和配置网络设备 wget 下载文件 axel 多线程加速下载文件 readlink 获取文件的绝对路径和文件名的组合 basename 去掉路径信息 dirname 获得路径信息 df 查看磁盘空间占用情况 du 显示每个文件和目录的磁盘使用空间 man 查看命令帮助文档 history 查看历史命令]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[生信入门系列之 linux 入门（三）：基础命令（中）]]></title>
      <url>%2Fblog%2F%E7%94%9F%E4%BF%A1%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B9%8B-linux-%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%EF%BC%88%E4%B8%AD%EF%BC%89%2F</url>
      <content type="text"><![CDATA[这次接着讲 linux 基础命令。上次讲到了一些文件及目录操作相关的命令，这次接着讲。 首先要讲的一个命令是 ln，它的功能是为某一个文件建立一个同步的链接。当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的目录下用 ln 命令链接它就可以，这样就不会重复地占用磁盘空间了。它又分为软链接和硬链接。 注意：由于链接的同步性，当链接或源文件被修改时，其它的链接和源文件也发生相应的改动。不希望这样，就用命令 cp 代替，当然，代价是占用磁盘空间。 假如 sample_data 目录下有两个子目录 test 和 transcriptome （还记得怎么判断它们时文件还是目录吗？）： liuhui@ginkgo:~/sample_data$ ls -lhtotal 0drwxrwxrwx 2 root root 0 Apr 8 17:34 testdrwxrwxrwx 2 root root 0 Apr 8 17:34 transcriptome 也可以指定查看指定目录的属性： liuhui@ginkgo:~/sample_data$ ls -lh test/total 0liuhui@ginkgo:~/sample_data$ ls -lh * ls -lh transcriptome/total 208K-rwxrwxrwx 1 root root 206K Apr 8 17:37 Trinity.fasta 也可以这样看，（* 是通配符，代表任意字符，即 0 到 多个）： liuhui@ginkgo:~/sample_data$ ls -lh *test:total 0transcriptome:total 208K-rwxrwxrwx 1 root root 206K Apr 8 17:37 Trinity.fasta 这样就知道每个目录下有什么文件或子目录了。 回到正题，首先讲软链接，软链接有如下属性： 1.软链接，以路径的形式存在。类似于 windows 中的快捷方式；所以，当移动源文件的位置或修改源文件名称时，软链接会失效。 2.软链接可以对一个不存在的文件名进行链接； 3.软链接可以对目录进行链接； 4.软链接可以 跨文件系统 ，硬链接不可以； 5.不增加源文件的链接数。 用法如下： ln -s file1 file2 其中 file1 是源文件，file2 是软链接文件。如图： 一般给源文件加上绝对路径，或者相对路径，但要类似这样： 因为软链接文件本质上就是一个指向源文件的路径；无论怎样操作，都要使得在软链接文件位置，能通过这个路径访问到源文件。 和软链接不同，硬链接有如下属性：1.硬链接，与源文件名称互为别名，不占用实际空间，直接指向文件在磁盘上的物理地址；所以无论怎样移动源文件或修改其名称，硬链接都不会失效。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 4.增加源文件的链接数。 用法如下： ln file1 file2 之前提到过文件或目录的权限，但没有细讲，这里把它讲清楚： 在上图文件或目录权限那一列信息中，权限分为三组，分别是：所有者权限、所属组权限以及其它用户的权限。其中的 r、w 以及x 分别代表文件的“读权限”，“写权限“以及”执行权限“，”-“指没有相应的权限。更详细的信息，请看以下表格： 代表字符 权限 数字 对文件的含义 对目录的含义 r 读权限 4 可以查看文件内容 可以查看目录中的内容 w 写权限 2 可以修改文件内容 可以在目录中创建、删除文件或目录 x 执行权限 1 可以执行文件 可以进入目录 用数字代表相应的字符，可方便的用命令行修改某个文件或目录的权限（三个数字的组合在一起，相加的和是唯一的）。要修改权限，需要用到命令 chmod，用法如下： chmod [mode] file 如： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rw-rw-r-- 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ chmod +x Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rwxrwxr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta 也可以通过数字： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rwxrwxr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ chmod 555 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta 有时，我们需要找一个文件在哪，这时可以用命令 find： find [path...] [expression] 如： liuhui@ginkgo:~/sample_data$ find . -name Trinity.fasta./transcriptome/Trinity.fasta . 代表当前目录，即在当前目录这个路径下，搜索名为”Trinity.fasta“的文件。当不太记得文件名时，可以用通配符 *: liuhui@ginkgo:~/sample_data$ find . -name *fasta./test/Trinity_hard.fasta./test/Trinity_soft_1.fasta./test/Trinity_soft.fasta./transcriptome/Trinity.fasta 当要搜索的文件在环境变量里时，可以用命令 which 来搜索： which filename 如： liuhui@ginkgo:~/sample_data$ which chmod/bin/chmodliuhui@ginkgo:~/sample_data$ which find/usr/bin/findliuhui@ginkgo:~/sample_data$ which ln/bin/lnliuhui@ginkgo:~/sample_data$ which ls/bin/lsliuhui@ginkgo:~/sample_data$ which vcftools/home/liuhui/bin/vcftools_0.1.13/bin/vcftoolsliuhui@ginkgo:~/sample_data$ which samtools/home/liuhui/bin/samtools/samtools-0.1.19/samtools 注意：如有有童鞋不知道环境变量是什么的话，请先看本系列第一篇文章，“初识 linux 系统”。 如果我们想在文件中查找特定的内容时，可以用命令 grep 来实现： grep [参数] file 我们知道，fasta 文件的序列号以 &gt; 开头（不是重定向操作符），如果想讲序列号提取出来，可以这样： grep &quot;&gt;&quot; Trinity.fasta 显然，直接将搜索到的序列号打印到屏幕上不是一个明智的做法，所以，应该这样做： 即是将输出的结果重定向到一个文件里，然后可以这样查看： less -S Trinity.fasta 如果只想查看一下，可以这样做： grep &quot;&gt;&quot; Trinity.fasta | less -S 上述命令用到了一个在 linux 中很重要的操作：管道，即 |。它的作用是把一个命令的输出直接连接到另一个命令的输入。管道在 linux 中是重中之重。可以这样想象：前一个命令对文件操作所产生的信息流，从管道的前端流入，然后从管道的后端流出，这时后一个命令会接住这些信息流，并可以对这些信息流进行操作。 以上述命令为例，grep &quot;&gt;&quot; Trinity.fasta 中，grep 提取 Trinity.fasta 文件中含有 “&gt;” 的行，则所有包含有 “&gt;” 的行所组成的集合就会形成一股信息流，如果没有其它操作，这股信息流就会输出到屏幕上；当然了，也可以讲这股信息流保存到一个文件里，即重定向；还可以将其”灌输到“一个管道里 |，似的这股信息流在管道里流通，知道有另个以命令来”接收“这股信息，并做相应的处理，如这里是用 less -S 来进行不换行地分页浏览这股信息流。 也可以查看前几行： grep &quot;&gt;&quot; Trinity.fasta | head 在这里，我们指关心序列号，气后地那一串数字往往不是我们关心的，那有什么办法能将其剔除掉吗？看： liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | head&gt;TRINITY_DN63_c0_g1_i1&gt;TRINITY_DN66_c0_g1_i1&gt;TRINITY_DN66_c0_g2_i1&gt;TRINITY_DN66_c0_g2_i2&gt;TRINITY_DN68_c0_g1_i1&gt;TRINITY_DN68_c0_g1_i2&gt;TRINITY_DN68_c0_g2_i1&gt;TRINITY_DN68_c0_g3_i1&gt;TRINITY_DN68_c0_g4_i1&gt;TRINITY_DN42_c0_g1_i1 这里用到了命令 cut，将多余的部分 ”cut“ 掉了；怎么实现的，首先，序列号及其后面的那些数字串间的间隔，是有空格来分割的（即分隔符是空格）。cut 命令通过参数 -d 指定分隔符，即空格，用引号引住；然后再加一个参数 -f，指定前面由空格作为分隔符，所产生的很多列中的第一列（术语往往是说”域“）。如果这样， liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1,2 | head&gt;TRINITY_DN63_c0_g1_i1 len=484&gt;TRINITY_DN66_c0_g1_i1 len=709&gt;TRINITY_DN66_c0_g2_i1 len=316&gt;TRINITY_DN66_c0_g2_i2 len=292&gt;TRINITY_DN68_c0_g1_i1 len=7194&gt;TRINITY_DN68_c0_g1_i2 len=7076&gt;TRINITY_DN68_c0_g2_i1 len=520&gt;TRINITY_DN68_c0_g3_i1 len=508&gt;TRINITY_DN68_c0_g4_i1 len=542&gt;TRINITY_DN42_c0_g1_i1 len=280liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1-2 | head&gt;TRINITY_DN63_c0_g1_i1 len=484&gt;TRINITY_DN66_c0_g1_i1 len=709&gt;TRINITY_DN66_c0_g2_i1 len=316&gt;TRINITY_DN66_c0_g2_i2 len=292&gt;TRINITY_DN68_c0_g1_i1 len=7194&gt;TRINITY_DN68_c0_g1_i2 len=7076&gt;TRINITY_DN68_c0_g2_i1 len=520&gt;TRINITY_DN68_c0_g3_i1 len=508&gt;TRINITY_DN68_c0_g4_i1 len=542&gt;TRINITY_DN42_c0_g1_i1 len=280 则取出第一和第二列，这里用逗号或横杠连接 ”1“ 和 ”2“，其中横杠用于链接连续的列，而逗号都行： # 横杠cut -f 1-10# 逗号cut -f 1,2,3,4,5,6,7,8,9,10# 当然逗号也有优势cut -f 1,3,5,8,10# 逗号与横杠混用cut -f 1,3,5,8-10# 这样不行哈cut -f 1-3-5-8-10 可能由同学想问，grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | head 的输出结果还有 ”&gt;” 啊，怎么去除呢？其实命令你学过啦，就是 cut： 这时，你这样操作： grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | cut -d &quot;&gt;&quot; -f 2 &gt; Trinity_ID 就得到你想要地序列号了。 对了，我想数一下”Trinity.fasta“ 文件中有几条序列，怎么做。这样做： liuhui@ginkgo:~/sample_data/transcriptome$ grep -c &quot;&gt;&quot; Trinity.fasta107 我只有序列号的那个文件呢？可以用命令 wc： wc -l file liuhui@ginkgo:~/sample_data/transcriptome$ wc -l Trinity_ID107 Trinity_ID 接着讲其它命令的用法。假如我有一个文件，其内容如下： liuhui@ginkgo:~/sample_data/test2$ cat fileaa 1ac 3ab 2ba 2ad 4ba 2ca 9da 10 我想按第一列排序，可以这样做： liuhui@ginkgo:~/sample_data/test2$ sort fileaa 1ab 2ac 3ad 4ba 2ba 2ca 9da 10 我想按第二列排序，那可以学学 cut，指定按第几列排序嘛，这样？ liuhui@ginkgo:~/sample_data/test2$ sort -k 2 fileaa 1da 10ab 2ba 2ba 2ac 3ad 4ca 9 实际上没有错，只是不是我们想要的，所以，得加个参数： liuhui@ginkgo:~/sample_data/test2$ sort -k 2 -n fileaa 1ab 2ba 2ba 2ac 3ad 4ca 9da 10 这里 -n 指按数值大小排列。 也可以按第二列逆序： liuhui@ginkgo:~/sample_data/test2$ sort -k 2 -n -r fileda 10ca 9ad 4ac 3ba 2ba 2ab 2aa 1 很明显，在上述文件中有一个重复项，可以这样删除： sort -u file sort 命令可以和 uniq 命令结合起来用， 去重（等同于 sort -u file）： 指显示重复行： 统计每行出现的次数： 如果只关心第二列的重复情况，可以这样： 接下来要讲的内容是文件的压缩或解压。但需要注意的是，不能对有硬链接的源文件或硬链接文件进行压缩或解压： 压缩命令可以用 gzip: gzip file 直接对文件进行压缩： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ gzip Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 44K-r-xr-xr-x 1 liuhui liuhui 43K Apr 8 17:05 Trinity.fasta.gz 也可以加上参数 -c，这样可以保存原来的文件了。 liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ gzip -c Trinity.fasta &gt; Trinity.fasta.gzliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 252K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta-rw-rw-r-- 1 liuhui liuhui 43K Apr 9 10:13 Trinity.fasta.gz 解压命令用 gunzip： gunzip file.gz 如图所示： 参数 -c 对 gunzip 同样适用： 可以这样查看压缩文件： zcat Trinity.fasta.gz | less -S 还有其它的压缩或解压命令，如 zip/unzip，bzip2/bunzip2 等，限于篇幅，在这里就不一一讲述了。 最后再讲一个命令，tar。这个命令的功能是将一个目录打包： -c 表示压缩，-v 表示显示压缩过程，-f 表示指定压缩文件，-z 表示用 gzip 压缩文件。 其逆向操作，只需改变一个参数，-z 变为 -x，后者表示从压缩的文件中提取文件： 命令及简单描述 命令 ln 创建软链接或硬链接 chmod 修改文件权限 find 查找文件 which 查找再环境变量中的文件 grep 利用关键字符进行文本搜索 cut 提取文件指定的列 wc 统计文件行数 sort 文件内容排序 uniq 对文本内容进行去重计数 gzip/gunzip 压缩或解压 zcat 查看 gzip 压缩的文件 tar 压缩或解压目录]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[生信入门系列之 linux 入门（一）：基础命令（上）]]></title>
      <url>%2Fblog%2F%E7%94%9F%E4%BF%A1%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B9%8B-linux-%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
      <content type="text"><![CDATA[生信入门系列之 linux 入门（一）：基础命令（上）我们在启动终端时，会显示类似这样的内容：liuhui@ginkgo:~$ ，如图所示： ​ 其中，liuhui 表示用户名，gingko 表示计算机名称，~ 代表家目录，表示我当前的位置是 /home/liuhui，$ 为命令输入提示符（root 用户，为 #），表示在其后的光标提示符中输入命令。比如在家目录下有一个叫 sample_data 目录，那我输入： cd sample_data 就可以进到 sample_data 这个目录里面了，其中 cd 是英文 “change directory” 的缩写，用于切换工作目录，如图所示： ​ 命令 cd 对目录进行操作，可接相对路径或绝对路径。上述的 sample_data 其实就是一个相对路径；接绝对路径可以这样做： cd /home/liuhui/sample_data 结果是一样的。 注意：不要混淆了目录和相对路径这两个概念；目录就是一个文件夹，而相对路径，从字面上理解就是一个“路径”，可以从A到 B的路径，路径有一个个目录连接而成，单个目录是最简单的路径；就上述的例子而言，如果家目录下没有 sample_data 这个目录的话，那就会报错： -bash: cd: sample_data: No such file or directory 如果家目录下没有 sample_data 这个目录，而有一个叫 test 的目录，这个目录里有一个叫 sample_data 的目录，那么就应该这样做： cd test/sample_data 如图所示： 好的，回到正题，我们通过 cd 命令进入了 sample_data 这个目录中，我们首先想要知道的就是这个目录里有没有文件，如果有，有那些文件，而这些文件的内容又是什么？这时，可以用命令 ls（是 list directory contents 的缩写）结果如下图： 可以看出 sample_data 目录下有两个文件 / 目录，但其具体的属性，即详细信息我们却不知道。可以给命令 ls 加一个参数 -l，如图： 这就详细多了，比如我知道了文件的大小，文件最后修改的时间，知道除了文件之外，还有一个目录，说的不是太清楚，看图： 文件或目录的权限及链接数下次再讲，这次有个映像即可。但是还有一点问题，就是我们比较关心的文件大小似乎没有单位，有点看不懂啊。所以，还可以加个参数 -h（官方解释：human-readable），显示如下，不用说相信大家都清楚文件的大小是多少了： 当然还可以以时间排序或逆序： liuhui@ginkgo:~/sample_data$ ls -lhttotal 212Kdrwxrwxr-x 2 liuhui liuhui 4.0K Apr 5 16:24 transcriptome-rw-rw-r-- 1 liuhui liuhui 206K Apr 5 12:38 Trinity.fastaliuhui@ginkgo:~/sample_data$ ls -lhtrtotal 212K-rw-rw-r-- 1 liuhui liuhui 206K Apr 5 12:38 Trinity.fastadrwxrwxr-x 2 liuhui liuhui 4.0K Apr 5 16:24 transcriptome 所以，我们到这就明白了，ls 是查看某个目录里面有什么文件或子目录，但如果我想查看一个文件里有什么内容，该怎么办呢，这时有几种办法： 查看文件的头几行，用 head 命令（默认前 10 行）： liuhui@ginkgo:~/sample_data$ head Trinity.fasta&gt;TRINITY_DN63_c0_g1_i1 len=484 path=[663:0-65 669:66-161 667:162-216 660:217-233 661:234-257 666:258-332 642:333-356 670:357-368 668:369-395 665:396-419 647:420-423 648:424-483] [-1, 663, 669, 667, 660, 661, 666, 642, 670, 668, 665, 647, 648, -2]CCAGCGTGGGGCCGGGGGCCGGTGAGTGGCTACCCAGCACCGCGGACAGAGGGGCACCCCAAGACCTGTACTCTCTGCCTCTAGGAGGAAGGAGAGTGAAGGGGATAGGATATGAAGTGGGTGCCAGACAAGGTGTGGGGATGCTACCACCGATTTGTTCTCCCTACGGCACCAGCTGTAGCTTTGGAAGCCGCGCAGCCCCATCTTCCCTAATCTTAGCCCATCCCGTTACTATTTCCAGGATAGCCCATCATGCATAGAAAGAAAACAGAGCCCTAGGCAGAGGGAGCCATAACCCACAAGGCATTTGTAGAGAAATGGAAAAAGAGTCGCCCTAGGGTAGCAGCGCAGGGAGCAGGAGTCTCCTGTGTCCTGGTGAAGATGCACAGAACAGAAAGCTGGGCCTGCAAGCTGCCTGGCTTGTTTGCTTTGGCTTCCTCTCTCCAAAGCCTGACTGTCCTTGGAGTATTCTGGTCCTCTGTGG 当然也可以查看文件的最末尾的几行，用tail 命令（默认最后 10 行）： liuhui@ginkgo:~/sample_data$ tail Trinity.fastaATCAGATCTAATTCTTACATTTTGAGATAACTGTAAAAAGAGTGAAATTATGAAATGGCCAATATCTTTTATCAGTCTATTCTTTTGGAAGCTGTCATGCACTATACATTGTGTACAGTTAAAAGTATATATATATATATATTCTTACTGAGTGAACGCCTCCTCTCCCCACGCCTGTATGTCACTAGCATCTAAGGAGAATGCTCAAGGCCCAGTGCTGCTGCTGCTGTGGTTTATATGGGTTTTGTTCTGTTTTGTTTTTGTGTGGTAAATTGATATTTAAAAACAACAAAAACCACGACTACTGTTTACAGACTGAAAAAAAACAATCACTGCTTTTTATACTACTGAGATCCTAAGTCAAGACTTTGCAAAGCAGGAATCGGGTTCAAGTTACTTCTTTGCTGTGGACGGATAGTCCTCTGTAGTATCTCCACATGATGGAGAGTGCACAAACCTAGGTGTGCTGCCATCAATTTTGTATATTTTCATAATTTTAATTGTTCGAAATTGCATTATATTTTGCAATCACCACATTCAATCTGTATATGTCTTTCATTTCAACTTTTTCAATACAAAAAGGGG 查看整个文件中的内容，用 cat 命令（对于几 kb 以上的文件，不建议用这个）： cat Trinity.fasta 分页浏览文件内容，用 less 命令（输入 q 退出）： less Trinity.fasta 显示如下： 但有个问题，就是太长的行，如上图中的那些数字，会折成两行或以上（即折行），这个文件还好，但对于其它文件，如 vcf 文件，就有可能看起来很费眼神，这是可以给 less 加个参数 -S（大写的），效果如下： 这样就好很多了。 有时我们想创建一个文件，可以通过以下方法： touch 命令： touch new_file 还可以用重定向操作符，&gt;，创建一个文件： &gt; new_file2 文件编辑命令，nano，这样还可以直接在里面添加内容了（相当于 windows 的记事本），操作如下所示： nano new_file3 输入内容如图所示： 保存（y），然后回车： ls 查看一下： 对于小文件，可以用 cat 命令查看内容： 前面提到的重定向操作符 &gt;，还可以这样用，相当于将 new_file3 的内容拷贝给了 new_file2： 另外，cat 命令也可以同时查看两个或两个以上文件的内容： 这时，再通过重定向操作符 &gt; 就可以把两个或两个以上的内容合并到一个文件里了： 这几个命令颠来倒去的重合在一起用是不是很有趣，也很强大；其实这也是 linux 命令的强大之处：通过几个不同的命令组合在一起，往往会发挥出强大的功能。这个概念很重要，上面演示的只不过时冰山一角罢了。 有时我们想删除一个文件，这时可以用命令 rm来操作： 注意：这是永久性删除。 通过 linux 命令，可以很方便地对文件进行复制操作，命令是 cp： cp old_file new_copy_file 修改文件名同样很简单，通过 mv 命令实现： mv old_file_name new_file_name 当然，还可以通过 mv 命令将一个文件移到另一个文件夹里（相当于在 windows 里将一个文件剪切，然后粘贴到另一个文件夹里），下图就演示了将文件 Trinity.fasta 移到了目录 transcriptome 里： 注意：这里的 transcriptome 实际上就是一个相对路径；如果 sample_data 里没有这个目录，那当然会不会报错，只不过就相当于修改文件名了。 之前提到了怎么创建一个文件，这里当然要说一下怎么创建一个目录： mkdir directory_name 也可以优雅地删除一个目录，只比删除文件多了一个参数，-r： rm -r directory_name 好了，这次先讲到这里。 注意： （1）命令与文件之间要有一个空格； （2） 创建、复制或重命名一个文件或目录，如果文件或目录前没有路径名，则产生的文件就在当前目录里； （3）文件的命名不要出现空格或其它特殊字符，如“!”，“&amp;”等。 浓缩版： # (1) 切换工作目录 (最后一个斜杠可有可无，命令与路径之间要有空格)cd ~/sample_data/transcriptome/# 大家试下以下的三个用法，看看效果cdcd -cd ~# (2) 显示当前工作目录pwd# (3) 创建目录mkdir folder_name# (4) 创建文件touch file_name1&gt; file_name2nano file_name3# (5) 打印文件内容及合并另个两个或以上文件的内容# 打印文件内容cat file_name# 同时打印两个文件的内容，第二个文件的内容会紧跟在第一个文件内容的最后面cat file_name1 file_name2# 利用上一个用法并结合从定向操作符“&gt;”，可以合并两个文件的内容cat file_name1 file_name2 &gt; combined_file# (6) 分页浏览较大文件的内容less file_name# 不折行less -S file_name# 显示行号less -SN file_name# (7) 查看文件头几行head file_name# 查看前 20 行head -n 20 file_name# (8) 查看文件末尾几行tail file_name# 查看前 20 行tail -n 20 file_name# (9) 查看文件及目录属性lsls -lh# 只查看某个文件或目录ls -lh file_namels -lh directory_name# (10) 删除文件及目录# 删除文件rm file_name# 删除目录rm -r directory_name# (11) 移动文件目录或修改文件目录的名称# 移动文件到目录“directory_name” 里的子目录“directory_name” 里（前提是这个路径存在）mv file_name directory_name/sub_directory_name# 修改文件名mv old_file_name new_file_name# 移动目录，将“directory_name” 移到目录“directory_name” 里的子目录“sub_directory_name” 里（前提是这个路径存在）mv directory_name1 directory_name/sub_directory_name# 修改目录名，将“directory_name1” 的名称改为“directory_name2”（directory_name2不存在，否则就将“directory_name1” 移动到“directory_name2”里了）mv directory_name1 directory_name2# (12) 复制文件或目录# 复制文件到当前目录下，并重命名成 “file_name2”（必须重命名）cp file_name1 file_name2# 复制文件到另外一个目录里cp file_name1 directory_name/# 复制文件到另外一个目录里并重命名cp file_name1 directory_name/file_name2# 复制目录cp -r directory_name1 directory_name2 命令及简单描述 目录操作 cd 切换工作目录 pwd 显示当前工作目录 mkdir 创建目录 文件操作 touch、&gt; 创建文件 nano 编辑文件 cat 打印文件内容及合并另个两个或以上文件的内容 less 分页浏览文件内容 head 查看文件头几行 tail 查看文件末尾几行 文件及目录操作 ls 查看文件及目录属性 rm 删除文件及目录 mv 移动文件目录或修改文件目录的名称 cp 复制文件或目录]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux 入门（一）：初识 linux 系统]]></title>
      <url>%2Fblog%2Flinux-%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9D%E8%AF%86-linux-%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[生信入门系列之 linux 入门（一）：初识 linux 系统​ Linux 是一个免费的操作系统，是生物信息分析中必不可少的工具。在 linux 系统中，所有的管理任务均可以在一个叫终端（terminal）的控制面板里完成，包括文件处理，软件安装以及用户管理。这个终端是交互的，即你运行特定的命令，相应的结果会在这个终端上显示出来。运行命令的方式是：在终端上输入你希望运行的命令，然后按回车键（Enter）。如果你想终止正在运行的命令，可以按 Chrl + C。 ​ 不同于 windows 系统，linux 的文件系统是一个目录树（directory tree）；如下图 所示，其文件系统为一个树状结构。最顶端 “root”，用斜杠 “/” 表示。一般来说，普通用户，无论是直接打开终端还是远程登陆服务器，所在的位置一般是在 /home/foo 下，其中的 foo 在这里指代用户名。 ​ linux 的树状文件系统（图片来自维基百科） ​ 对于 windows 用户来说，可以通过一个轻量级的软件 putty （约 500 kb；百度可下载；双击软件即可使用，无需安装）来远程登陆服务器，登陆方法如下图，图中的序号为操作顺序： 首先在 Host Name（or IP address）下方的框框中输入服务器的的 ID 地址，并单击 “Open” 选项； putty 会弹出一个警告框，点击否； 最后最出现下图 3 中的界面，在 “login as：“ 后输入你的用户名，回车后输入登陆密码（直接输入就行了，是看不到显示的）。 ​ 打开终端或是远程登陆服务器时，如果你想查看当前所在的位置，可以在终端输入如下命令，并按回车键： pwd ​ 这个 linux 命令意思是“打印当前工作目录”，是 “print working directory” 的英文缩写；其返回结果是一个绝对路径（就是从根目录开始，依次将各级子目录的名字组合起来），应该类似这样： /home/foo ​ 与上述的树状文件系统相互比照，是不是立马清楚自己到底在哪里了？就好像 windows 下，到底在哪个盘的哪个文件夹里一样。这个绝对路径很有用，它不仅让我们知道自己在哪儿，同时还可以告诉系统某个软件在哪儿，以及告诉软件要操作的文件在哪儿。举个例子吧，比如说我想调用一个软件，叫 vcftools，那么，我要运行它，只需要在终端输入如下命令并回车： vcftools --vcf input_data.vcf ​ 就可以轻轻松松算出 variants 的数目和 individuals 的数目。但很不幸的是，你也有可能得到如下结果： bash: vcftools: command not found ​ 大概意思就是，系统找不到这个命令在哪儿。可能有人会问了，既然是一个命令，为什么系统会找不到呢？其实，在linux 系统中，有一个非常核心的概念：一切皆文件！即在linux环境下，任何事物都以文件的形式存在。所以，如果你的从 vcftools 安装在 /home/foo/biosoft/vcftool-0.1.13/bin 这个绝对路径下，那么，你就可以这样运行它： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 就可以的结果啦。但也有可能得到如下结果： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf input_data.vcfstat error: No such file or directoryError: Can&apos;t determine file type of input_data.vcf ​ 这时也不要慌，只要在输入文件前加上绝对路径即可。加入 input_data.vcf 文件在 /home/foo/vcffile 下，可以这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 这时，如无意外，就可以得到如下结果了： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf /home/foo/vcffile/input_data.vcfUsing zlib version: 1.2.3.4Versions of zlib &gt;= 1.2.4 will be *much* faster when reading zipped VCF files.After filtering, kept 16 out of 16 IndividualsAfter filtering, kept 1116595 out of a possible 1116595 SitesRun Time = 5.00 seconds ​ 如果还报出一些奇奇怪怪的错误提示，那就首先检查一下您的输入法中是否为纯英文状态，中文和全角状态下的输入的空格都是会报错的。同时，linux 里，软件对字母大小写是敏感的，即 linux 认为 A 和 a 是两个不同的事物；也就是说大小写也是会造成错误的。 ​ 前面，我多次提到了绝对路径这个概念，不少心思敏捷的童鞋就会想了，有没有相对路径？有的。 ​ 举例说明，假如我们在 /home/foo 这个路径下，并且我们知道该路径下有 vcffile 和 biosoft 这两个目录；那么，我们可以这样运行上述的命令： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 可以看到，biosoft/vcftool-0.1.13/bin 和 vcffile 这两个路径都不是以斜杠 / 开头的，所以这两个路径都是相对路径。当然了，你也可也这样运行： biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 亦或这样： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 也可以进到 vcffile 这个目录里，这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 或者这样（ “..” 在这里代表上级目录，相应的，”../..“ 代表上级目录的上级目录）： ../biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 总之，想怎么运行，看心情！ ​ 相信看到这里，会有记忆力超好的童鞋会问了，我该如何像运行 pwd 那样运行 vcftools 呢？而不是在它前面加上一大串绝对路径或是相对路径！ ​ 要回答这个问题，小编先给大家展示两个命令（不深入讲解）： ​ 第一个是： which pwd ​ 返回的应该是： /bin/pwd ​ 第二个是： echo $PATH ​ 会返回类似下面的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 眼尖的童鞋会发现，上面一串结果其实就是许多绝对路径通过 “:” 连接在一起的（叫做环境变量），其中有一个路径 “/bin” ；而 which pwd 返回的结果是 /bin/pwd。这两个有什么关系呢？其实不必深究，我们只需要知道，linux 把 pwd 看作一个文件（还记得前面说的“一切皆文件”吗），linux 系统会在上述的环境变量中从左往右依次查找，看某个路径下是否有 pwd 这个文件，然后执行这个命令。并且，环境变量是可以编辑的， 即可在环境变量 “PATH” 中添加特定的路径。同理，如果我们的 vcftools 软件（其实就是个文件）的路径也在上述的路径中，就可以在终端直接输入 vcftools 就可以运行了。 ​ 那么问题来了，我们该如何将特定软件的路径发到上述的环境变量 “PATH” 中呢？ ​ 只需要通过 export 命令，在终端中输入以下内容，回车后，就可以将 vcftools 的路径导入到上述的环境变量中： # vcftools （井号后的内容 linux 系统不会读取，可以做注释）export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; # 添加这一行就行了，export 后要加空格，不要换行。 ​ 就会得到类似这样的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 当然，如果你是这样输入的， export PATH=&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos;:$PATH ​ 那就应该得到类似这样的结果： /home/foo/biosoft/vcftool-0.1.13/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games ​ 就可以不加绝对路径运行 vcftools 了。像这样： vcftools --vcf input_data.vcf ​ 但上述做法有一个弊端，就是这个做法是暂时性的，就是说每次打开或登陆终端时，都要运行一下这个命令才行，非常麻烦。所以怎么办呢？ ​ 其实不难，在每个用户的家目录下，即上述的 /home/foo 下，都有一个非常重要的隐藏文件 “.bashrc”，里面有许多我们每次启动或登陆终端时，linux 系统都会默认自动运行的命令。所以，只需将 export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; 添加到 “.bashrc” 文件的最后一行即可。这样我们每次启动或登陆终端时，系统就会自动运行这个命令了，这样就免去可多次手动添加的麻烦。如果不想重启终端，可以执行这个命令（相当于让系统执行一遍 “.bashrc” 中的命令）： source .bashrc ​ 怎么加呢？可以在家目录下，运行一个文本编辑命令 vim 或 nano： vim .bashrc 对于 vim 怎么使用，可以自行百度，有详细教程，这里不做赘诉（使用起来比较复杂）。 或另一个命令 nano： nano .bashrc ​ 这个比较简单，只需回车后： 按向下箭头（一直到文件最底部） 黏贴 export PATH=$PATH:&#39;/home/foo/biosoft/vcftool-0.1.13/bin&#39; 依次按 ctrl + x，y，Enter（即保存退出） 注意1：/home/foo/biosoft/vcftool-0.1.13/bin 要做根据自己的实际路径做相应地改动。 注意2： 在 linux 系统里，通过 nano 或是 vim 一般是通过上下左右等方向键或其它快捷方式移动光标。 重要知识点回顾： 终端 目录树 家目录 大小写敏感 绝对路径和相对路径 一切皆文件 环境变量及其编辑 ​下期预告：linux 基础命令]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 学习笔记]]></title>
      <url>%2Fblog%2Fpython-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[python 笔记1. 基础1.1 基本语法控制流相关# (1) 为何1 in [1,0] == True执行结果是False1 in [1,0] == True# 将被转为(1 in [1, 0]) and ([1, 0] == True)###### 同样的a &lt; b &lt; c# 会被转为(a &lt; b) and (b &lt; c) # b不会被解析两次# (2) 如何检测一个变量是否存在# (2.1) 检测本地变量if 'myVar' in locals(): # myVar exists. # (2.2) 检测全局变量if 'myVar' in globals(): # myVar exists.# (2.3) 检测一个对象是否包含某个属性if hasattr(obj, 'attr_name'): # obj.attr_name exists.# (3) Python中的三元运算符 a if test else b # 如果 test 为 True，返回 a，否则返回 b# 使用:&gt;&gt;&gt; 'true' if True else 'false''true'&gt;&gt;&gt; 'true' if False else 'false''false' 1.2 字符串相关# (1) 如何反向输出一个字符串&gt;&gt;&gt; 'hello world'[::-1]'dlrow olleh'# (2) 如何随机生成大写字母和数字组成的字符串'''6U1S754Z4UKKU911K4'''import string, random''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(6))# (3) 字符串的contains# (3.1)使用in关键字if not "blah" in somestring: continueif "blah" not in somestring: continue# (3.2) 使用字符串的find/index (注意index查找失败抛异常)s = "This be a string"if s.find("is") == -1: print "No 'is' here!"else: print "Found 'is' in the string." # (4) 如何判断一个字符串是数字def is_number(s): try: float(s) return True except ValueError: return False# (5) 字符串格式化 % vs format# 下列输出一致#!/usr/bin/pythonsub1 = "python string!"sub2 = "an arg"a = "i am a %s"%sub1b = "i am a &#123;0&#125;".format(sub1)c = "with %(kwarg)s!"%&#123;'kwarg':sub2&#125;d = "with &#123;kwarg&#125;!".format(kwarg=sub2)print aprint bprint cprint d# .format 还可以这样用，但用 % 时无法做到这点e = "i am a &#123;0&#125; &#123;0&#125;".format(sub1)# %只处理一个变量或一个元组, 你或许会认为下面的语法是正确的"hi there %s" % name#但当name恰好是(1,2,3)时，会抛出 TypeError 异常.为了保证总是正确的，你必须这么写"hi there %s" % (name,) # supply the single argument as a single-item tuple# (5) 将一个包含有字典的字符串转为一个字典&gt;&gt;&gt; s = "&#123;'muffin' : 'lolz', 'foo' : 'kitty'&#125;"&gt;&gt;&gt; import ast&gt;&gt;&gt; ast.literal_eval(s)&#123;'muffin': 'lolz', 'foo': 'kitty'&#125;# (6) 如何填充0到数字字符串中保证统一长度# (6.1) 对于字符串&gt;&gt;&gt; n = '4'&gt;&gt;&gt; print n.zfill(3)&gt;&gt;&gt; '004'# (6.2) 对于数字&gt;&gt;&gt; n = 4&gt;&gt;&gt; print '%03d' % n&gt;&gt;&gt; 004&gt;&gt;&gt; print "&#123;0:03d&#125;".format(4) # python &gt;= 2.6&gt;&gt;&gt; 004 1.3 文件相关# (1) 如何检查一个文件是否存在import os.pathprint os.path.isfile(fname)print os.path.exists(fname)# (2) 如何创建不存在的目录结构import os.pathif not os.path.exists(directory): os.makedirs(directory) # 需要注意的是，当目录在exists和makedirs两个函数调用之间被创建时，makedirs将抛出OSError# (3) 如何拷贝一个文件from shutil import copyfilecopyfile(src, dst)# (4) 如何找到一个目录下所有.txt文件# (4.1) 使用globimport globimport osos.chdir("/mydir")for files in glob.glob("*.txt"): print files# (4.2) 使用os.listdirimport osos.chdir("/mydir")for files in os.listdir("."): if files.endswith(".txt"): print files# (4.3) 或者遍历目录import osfor r,d,f in os.walk("/mydir"): for files in f: if files.endswith(".txt"): print os.path.join(r,files)# (5) 如何逐行读取文件# (5.1) 先将文件读入内存，然后逐行读取for line in open("test.txt").readlines(): print line# (5.2) 利用file的迭代器for line in open("test.txt"): #use file iterators print line 2. 基本数据结构2.1 列表# (1) Python 中如何复制一个列表# (1) 切片操作&gt;&gt;&gt; a = [1, 2, 3, [4, 5]]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(a), id(b)(4292218700L, 4292555596L)# 当列表中还有列表时，则无法实现真正的拷贝了&gt;&gt;&gt; a[2] = 0&gt;&gt;&gt; a[3][1] = 6&gt;&gt;&gt; a[1, 2, 0, [4, 6]]&gt;&gt;&gt; b[1, 2, 3, [4, 6]]# (2) list()函数&gt;&gt;&gt; c = ["a", "b", "c", ["d", "e"]]&gt;&gt;&gt; c['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; d = list(c)&gt;&gt;&gt; d['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; id(c), id(d)(4292555596L, 4292218732L)# 当列表中还有列表时，同样无法实现真正的拷贝了&gt;&gt;&gt; c[1] = 0&gt;&gt;&gt; c[3][1] = 0&gt;&gt;&gt; c['a', 0, 'c', ['d', 0]]&gt;&gt;&gt; d['a', 'b', 'c', ['d', 0]]# (3) “乘法”操作&gt;&gt;&gt; e = [1, 2, 3, [4, 5]]&gt;&gt;&gt; f = e * 1&gt;&gt;&gt; f[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(e), id(f)(4292721772L, 4292721260L)# 同样的问题&gt;&gt;&gt; e[1] = 0&gt;&gt;&gt; e[3][1] = 0&gt;&gt;&gt; e[1, 0, 3, [4, 0]]&gt;&gt;&gt; f[1, 2, 3, [4, 0]]# (4) copy.copy&gt;&gt;&gt; import copy&gt;&gt;&gt; g = [1, 2, 3, [4, 5]]&gt;&gt;&gt; h = copy.copy(g)&gt;&gt;&gt; h[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(g), id(h)(4292218860L, 4292218764L)# 还是一样&gt;&gt;&gt; g[1] = 0&gt;&gt;&gt; g[3][1] = 0&gt;&gt;&gt; g[1, 0, 3, [4, 0]]&gt;&gt;&gt; h[1, 2, 3, [4, 0]]# copy.deepcopy&gt;&gt;&gt; i = [1, 2, 3, [4, 5]]&gt;&gt;&gt; j = copy.deepcopy(i)&gt;&gt;&gt; j[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(i), id(j)(4292215020L, 4292598732L)# 完全新的拷贝&gt;&gt;&gt; i[1] = 0&gt;&gt;&gt; i[3][1] = 0&gt;&gt;&gt; i[1, 0, 3, [4, 0]]&gt;&gt;&gt; j[1, 2, 3, [4, 5]]# (2) 列表的 append 和 extend 的区别&gt;&gt;&gt; x = [1, 2]&gt;&gt;&gt; x.append(3)&gt;&gt;&gt; x[1, 2, 3]&gt;&gt;&gt; x.append([4,5])&gt;&gt;&gt; x[1, 2, 3, [4, 5]]&gt;&gt;&gt;&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; x.extend([4, 5])&gt;&gt;&gt; x[1, 2, 3, 4, 5]# (3) 如何随机地从列表中抽取变量foo = ['a', 'b', 'c', 'd', 'e']from random import choiceprint choice(foo)# (4) 如何将一个列表切分成若干个长度相同的子序列# 想要得到这样的效果lis = range(1, 1000)print chunks(lis, 10) -&gt; [ [ 1..10 ], [ 11..20 ], .., [ 991..999 ] ]# (4.1) 使用yield:def chunks(lis, n): """ Yield successive n-sized chunks from lis. """ for i in xrange(0, len(lis), n): yield lis[i:i+n]list(chunks(range(10, 75), 10))# (4.2) 直接处理def chunks(l, n): return [l[i:i+n] for i in range(0, len(l), n)] 2.2 字典# (1) 使用列表解析创建一个字典d = &#123;key: value for (key, value) in sequence&#125;&gt;&gt;&gt; &#123;i : chr(65+i) for i in range(4)&#125;&#123;0: 'A', 1: 'B', 2: 'C', 3: 'D'&#125;&gt;&gt;&gt; &#123;(k, v): k+v for k in range(4) for v in range(4)&#125;&#123;(0, 1): 1, (1, 2): 3, (3, 2): 5, (0, 0): 0, (3, 3): 6, (3, 0): 3, (3, 1): 4, (2, 1): 3, (0, 2): 2, (2, 0): 2, (1, 3): 4, (2, 3): 5, (2, 2): 4, (1, 0): 1, (0, 3): 3, (1, 1): 2&#125;# (2) 如何在单一表达式中合并两个Python字典&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = dict(x.items() + y.items())&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125;# (3) 如何映射两个列表成为一个字典&gt;&gt;&gt; keys = ['a', 'b', 'c']&gt;&gt;&gt; values = [1, 2, 3]&gt;&gt;&gt; dictionary = dict(zip(keys, values))&gt;&gt;&gt; print(dictionary)&#123;'a': 1, 'b': 2, 'c': 3&#125;# (4) 根据 dict 内值, 排序一个列表中的所有 dictlist_to_be_sorted = [&#123;'name':'Homer', 'age':39&#125;, &#123;'name':'Bart', 'age':10&#125;]# (4.1) 简单的做法newlist = sorted(list_to_be_sorted, key=lambda k: k['name'])# (4.2) 高效的做法from operator import itemgetternewlist = sorted(list_to_be_sorted, key=itemgetter('name'))# 根据值给字典排序x = &#123;1: 2, 3: 4, 4:3, 2:1, 0:0&#125;sorted(x.iteritems(), key=lambda k: k[1])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[networkx 笔记]]></title>
      <url>%2Fblog%2Fnetworkx-%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[networkx 复杂网络分析笔记主要参考1，2，3。 数据 facebook_combined.csv，relation_weight_sam.csv。 1. 建图networkx可以建立简单无向图graph，有向图digraph，可重复边的multi-graph。 1.1 简单无向图 graphimport networkx as nx#建立一个空的无向图GG=nx.Graph()#添加一个节点1G.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）G.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边G.add_edge(3,2)#输出全部的节点： [1, 2, 3]print G.nodes()#输出全部的边：[(2, 3)]print G.edges()#输出边的数量：1print G.number_of_edges()#输出点数print G.number_of_nodes() 无向图 1.2 有向图 digraph有向图的建立方式和无向图基本类似，只是在上述代码的第二行，将G = nx.Graph() 改为 G = nx.DiGraph() 。需要注意的是，此时再添加边3-2与边2-3，则被认为是两条不同的 import networkx as nxD=nx.DiGraph()#添加一个节点1D.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）D.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边D.add_edge(3,2)#输出全部的节点： [1, 2, 3]print D.nodes()#输出全部的边：[(2, 3), (3, 2)]print D.edges()#输出边的数量：2print D.number_of_edges()#输出点数print D.number_of_nodes() 同时，有向图和无向图是可以相互转化的，分别用到Graph.to_undirected() 和 Graph.to_directed()两个方法。 有向图 1.3 加权图有向图和无向图都可以给边赋予权重，用到的方法是add_weighted_edges_from，它接受1个或多个三元组[u,v,w]作为参数，其中u是起点，v是终点，w是权重。例如： # 添加0-1、1-2和2-3三条边，权重分别是3.0和7.5G.add_weighted_edges_from([(0,1,3.0),(1,2,7.5),(2,3,1.0)])# 如果想读取权重，可以使用get_edge_data方法，它接受两个参数u和v，即边的起始点。print G.get_edge_data(1,2)# 选出边的权重超过一个阈值的边estrong = [(u,v) for (u,v,d) in G.edges(data=True) if d["weight"] &gt; 3.0]print estrong # [(1, 2)] 加权图 这是一个加权图的代码 from matplotlib import useuse("Agg")import matplotlib.pyplot as pltimport networkx as nximport pandas as pddf = pd.read_csv("../data/relation_weight_sam.csv")G = nx.from_pandas_dataframe(df, 'node1', 'node2', edge_attr='weight' )# saves the positions of the nodes on the visualization# In detail positions is a dictionary where each node is# a key and the value is a position on the graph# &#123;'Fam38a_predicted': array([ 0.52246857, 0.4412573 ], dtype=float32),...&#125;positions = nx.spring_layout(G)# pass positions and set hold=Truenx.draw(G, pos=positions, hold=True, with_labels=False, node_size=30)weights = [w[2]['weight']*5 for w in G.edges(data=True)]#width can be array of floatsnx.draw_networkx_edges(G, pos=positions, width=weights)plt.savefig("../output/net_weight.png")# 前十行数据，weight是通过"numpy.random.rand()"模拟的。"""node1 node2 weightAblim2 Acsl6 0.0656480910603Ablim2 Apeg1 0.0727940253706Ablim2 Atp2a2 0.5280955211Ablim2 Boll_predicted 0.200347948345Ablim2 Cap2 0.108700562945Ablim2 Copb1 0.546335670525Ablim2 Creld2 0.420412397031Ablim2 Dtna_predicted 0.24546480993Ablim2 Dusp8_predicted 0.932345236242""" 1.4 可重复边的 multi-graphimport networkx as nxM=nx.MultiGraph() 1.5 点这里的点可以是任意可区分的对象（hashable），比如数字，字符串，对象等。 G.add_node(1)G.add_node('first_node')#这里用一个对象多为key来唯一区别一个点#我们还能够用一个列表来批量加入点G.add_nodes_from([1,2,3])#还可以用一个图对象作为点，嵌入到其他图中G.add_node(D) #这里D作为一个点的key#或者把一个图的所有点赋予另一个图G.add_nodes_from(D) #这里返回D的所有点，赋予G#与加入相同的传递方法，我们也可以删除点G.remove_node(1)G.remove_nodes_from([1,2,3]) 1.6 边这里的边可以使用两个单独的对象作为输入 G.add_edge(1,2) #表示1，2之间有一条边。#如果不存在点1，2，则会自动加入点集合。#或者以元组的形式作为输入e=(1,2)G.add_edge(*e)#这里的*代表吧元组解包（unpack），当作一个个的值扔到函数中去。#如果不解包，等价于#G.add_edge(e)=G.add_edge((1,2))与参数传递的方式不符。#类似的，我们还可以使用包含元组的列表来传递参数G.add_edges_from([(1,2),(2,3)])#我们还可以报一个图的边赋予另一个图G.add_edges_from(H)#删除G.remove_edge(1,2)G.remove_edges_from([(1,2),(2,3)]) 1.7 访问node_list = G.nodes()edge_list = G.edges()#可以返回包含点与边的列表node = G.node[‘first_node’]#如上根据key返回点edge = G.edge['first_node']['second_node']#同样的方法，返回两个key之间的边 1.8 属性我们可以给图，点，边赋予各种属性，最简单的就是权值属性 G.add_node(1,time='5pm')#在添加时跟上属性G.add_nodes_from([1,2,3],time='5pm')#批量添加点是，跟上统一的属性G.add_nodes_from([(3,&#123;'time':'5pm'&#125;), (4,&#123;'time':'4pm'&#125;)])#或者写成元组列表[（key,dict），（key,dict）]的形式G.node[1]['time']#根据字典key访问属性值。#给边添加属性也类似G.add_edge(1,2,time='3am')G.add_edges_from([(1,2,&#123;'time'='5pm'&#125;),(2,3,&#123;'time'=3am&#125;)])#批量赋予属性G.edge[1][2][‘time’]#访问#我们还可以使用特定的函数批量返回属性，如time = nx.get_edge_attributes(G,'time')#返回得到以元组为key,time属性为值得一个字典time[(1,2)] 1.9 图算法NetworkX提供了常用的图论经典算法，例如DFS、BFS、最短路、最小生成树、最大流等等，非常丰富，如果不做复杂网络，只作图论方面的工作，也可以应用NetworkX作为基本的开发包。 #调用多源最短路径算法，计算图G所有节点间的最短路径path=nx.all_pairs_shortest_path(G)#输出节点0、2之间的最短路径序列： [0, 1, 2]print path[0][2] 1.10 画图nx.draw(G) # 方法，至少接受一个参数：待绘制的网络G matplotlib.show() #显示出来 画图参数运行样式 node_size: 指定节点的尺寸大小(默认是300) node_color: 指定节点的颜色 (默认是红色，可以用字符串简单标识颜色，例如’r’为红色，’b’为绿色等) node_shape: 节点的形状（默认是圆形，用字符串’o’标识） alpha: 透明度 (默认是1.0，不透明，0为完全透明) width: 边的宽度 (默认为1.0) edge_color: 边的颜色(默认为黑色) style: 边的样式(默认为实现，可选： solid|dashed|dotted,dashdot) with_labels: 节点是否带标签（默认为True） font_size: 节点标签字体大小 (默认为12) font_color: 节点标签字体颜色（默认为黑色） 运用布局 circular_layout：节点在一个圆环上均匀分布 random_layout：节点随机分布 shell_layout：节点在同心圆上分布 spring_layout： 用Fruchterman-Reingold算法排列节点（样子类似多中心放射状） spectral_layout：根据图的拉普拉斯特征向量排列节点 添加文本 用plt.title()方法可以为图形添加一个标题，该方法接受一个字符串作为参数。 fontsize参数用来指定标题的大小。例如：plt.title(“BA Networks”, fontsize = 20)。 如果要在任意位置添加文本，则可以采用plt.text()方法。 2. 四种网络模型NetworkX提供了4种常见网络的建模方法，分别是：规则图，ER随机图，WS小世界网络和BA无标度网络。 2.1 规则图规则图差不多是最没有复杂性的一类图，random_graphs.random_regular_graph(d, n)方法可以生成一个含有n个节点，每个节点有d个邻居节点的规则图。 下面一段示例代码，生成了包含20个节点、每个节点有3个邻居的规则图： import networkx as nximport matplotlib.pyplot as plt# regular graphy# generate a regular graph which has 20 nodes &amp; each node has 3 neghbour nodes.RG = nx.random_graphs.random_regular_graph(3, 20)# the spectral layoutpos = nx.spectral_layout(RG)# draw the regular graphynx.draw(RG, pos, with_labels = False, node_size = 30)plt.show() 2.2 ER随机图ER随机图是早期研究得比较多的一类“复杂”网络，模型的基本思想是以概率p连接N个节点中的每一对节点。用random_graphs.erdos_renyi_graph(n,p)方法生成一个含有n个节点、以概率p连接的ER随机图： import networkx as nximport matplotlib.pyplot as plt# erdos renyi graph# generate a graph which has n=20 nodes, probablity p = 0.2.ER = nx.random_graphs.erdos_renyi_graph(20, 0.2)# the shell layoutpos = nx.shell_layout(ER)nx.draw(ER, pos, with_labels = False, node_size = 30)plt.show() 2.3 WS小世界网络 用random_graphs.watts_strogatz_graph(n, k, p)方法生成一个含有n个节点、每个节点有k个邻居、以概率p随机化重连边的WS小世界网络。 下面是一个例子： networkx-笔记/import networkx as nximport matplotlib.pyplot as plt# WS network# generate a WS network which has 20 nodes,# each node has 4 neighbour nodes,# random reconnection probability was 0.3.WS = nx.random_graphs.watts_strogatz_graph(20, 4, 0.3)# circular layoutpos = nx.circular_layout(WS)nx.draw(WS, pos, with_labels = False, node_size = 30)plt.show() 2.4 BA无标度网络用random_graphs.barabasi_albert_graph(n, m)方法生成一个含有n个节点、每次加入m条边的BA无标度网络。 下面是一个例子： import networkx as nximport matplotlib.pyplot as plt# BA scale-free degree network# generalize BA network which has 20 nodes, m = 1BA = nx.random_graphs.barabasi_albert_graph(20, 1)# spring layoutpos = nx.spring_layout(BA)nx.draw(BA, pos, with_labels = False, node_size = 30)plt.show() 对BA模型实现代码的分析 #定义一个方法，它有两个参数：n - 网络节点数量；m - 每步演化加入的边数量def barabasi_albert_graph(n, m): # 生成一个包含m个节点的空图 (即BA模型中t=0时的m0个节点) G=empty_graph(m) # 定义新加入边要连接的m个目标节点 targets=range(m) # 将现有节点按正比于其度的次数加入到一个数组中，初始化时的m个节点度均为0，所以数组为空 repeated_nodes=[] # 添加其余的 n-m 个节点，第一个节点编号为m（Python的数组编号从0开始） source=m # 循环添加节点 while source&lt;n: # 从源节点连接m条边到选定的m个节点targets上（注意targets是上一步生成的） G.add_edges_from(zip([source]*m,targets)) # 对于每个被选择的节点，将它们加入到repeated_nodes数组中（它们的度增加了1） repeated_nodes.extend(targets) # 将源点m次加入到repeated_nodes数组中（它的度增加了m） repeated_nodes.extend([source]*m) # 从现有节点中选取m个节点 ，按正比于度的概率（即度优先连接） targets=set() while len(targets)&lt;m: #按正比于度的概率随机选择一个节点，见注释1 x=random.choice(repeated_nodes) #将其添加到目标节点数组targets中 targets.add(x) #挑选下一个源点，转到循环开始，直到达到给定的节点数n source += 1 #返回所得的图G return G from matplotlib import useuse("Agg")import randomimport networkx as nxfrom networkx.generators.classic import empty_graphimport matplotlib.pyplot as pltdef barabasi_albert_graph(n, m): G=empty_graph(m) targets=range(m) repeated_nodes=[] source=m while source&lt;n: G.add_edges_from(zip([source]*m,targets)) repeated_nodes.extend(targets) repeated_nodes.extend([source]*m) targets=set() while len(targets)&lt;m: x=random.choice(repeated_nodes) targets.add(x) source += 1 return G ##G=nx.Graph()G = barabasi_albert_graph(400,6)pos = nx.spring_layout(G)nx.draw(G, pos, with_labels = False, node_size = 30)plt.savefig("../output/BA_400_6.png") 3. 统计指标计算3.1 度、度分布 NetworkX可以用来统计图中每个节点的度，并生成度分布序列。 import networkx as nximport matplotlib.pyplot as plt #生成一个n=1000，m=3的BA无标度网络G = nx.random_graphs.barabasi_albert_graph(1000,3)#返回某个节点的度print G.degree(0)#返回所有节点的度print G.degree()#返回图中所有节点的度分布序列（从1至最大度的出现频次）print nx.degree_histogram(G)#返回图中所有节点的度分布序列degree = nx.degree_histogram(G)#生成x轴序列，从1到最大度x = range(len(degree))#将频次转换为频率y = [z / float(sum(degree)) for z in degree]#在双对数坐标轴上绘制度分布曲线plt.loglog(x,y,color="blue",linewidth=2)#显示图表plt.show() 3.2 群聚系数# 平均群聚系数nx.average_clustering(G)# 各个节点的群聚系数nx.clustering(G) 3.3 直径和平均距离# 图G的直径（最长最短路径的长度）nx.diameter(G)# 图G所有节点间平均最短路径长度nx.average_shortest_path_length(G) 3.4 中心性一个图的直径是所有点之间最长的最短路径。在连接中心度，我们需要寻找一个点，这个点出现在很多点的最短路径中。出现的次数越多，连接中心性越高。这样的点，可以作为一个桥梁作用。意义：分析该节点对网络信息流动的影响，如：考察此人的社交能力或对于社会网络中信息流动的影响力。 betweenness centralityimport networkx as nximport matplotlib.pyplot as pltG=nx.Graph()print G.edges() # []G.add_edges_from([(1,2),(2,3),(2,4),(2,5),(1,3),(1,4),(3,5),(4,6)])print G.edges()#[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5), (3, 5), (4, 6)]# betweenness centralitybc = nx.betweenness_centrality(G)print sorted(bc.items(), key=lambda k: k[1], reverse=True)# [(4, 0.4), (2, 0.35000000000000003), (1, 0.1), (3, 0.05), (5, 0.0), (6, 0.0)]# spring layoutpos = nx.spring_layout(G)nx.draw(G, pos, with_labels = True, node_size = 100,font_size=6,font_color='b')plt.show() 另一个复杂的例子 import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")def most_important(G): """ returns a copy of G with the most important nodes according to the pagerank """ ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')plt.savefig("../output/FB_BetCen.png", dpi = 300)###"""node1,node20,10,20,30,40,50,60,70,8...2420,25432420,25552420,25672420,25922420,25972420,25982420,26092420,26172420,26292420,26422420,26432420,26532421,24372421,26342422,24412422,2558...""" 4. 社区发现（Community detection）import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig("../output/FB_commu.png", dpi = 300)# get modularitymod = community.modularity(part,G)print("modularity:", mod) 也可以在betweenness centrality的基础上画community detection: import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")def most_important(G): """ returns a copy of G with the most important nodes according to the pagerank """ ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_networkx(G, pos = spring_pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig("../output/FB_BC_commu.png", dpi = 300)# get modularitymod = community.modularity(part,G)print("modularity:", mod)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何在遍历时，正确删除列表中的 items]]></title>
      <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E5%9C%A8%E9%81%8D%E5%8E%86%E6%97%B6%EF%BC%8C%E6%AD%A3%E7%A1%AE%E5%88%A0%E9%99%A4%E5%88%97%E8%A1%A8%E4%B8%AD%E7%9A%84-items%2F</url>
      <content type="text"><![CDATA[错误的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x: if i in y: x.remove(i)print x-----------------['a', 'c', 'd'] 正确的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x[:]: if i in y: x.remove(i)print x-----------------['a', 'd'] 实际上，id(x)与id(x[:])是不同的，所以只有在x的副本（x[:]）中遍历，然后在x中删除，才不会造成错误。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[根据一个list文件生成一个组合]]></title>
      <url>%2Fblog%2F%E6%A0%B9%E6%8D%AE%E4%B8%80%E4%B8%AAlist%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%E7%BB%84%E5%90%88%2F</url>
      <content type="text"><![CDATA[假设有一个 list 如下： $ cat aaabcd 期望生成如下组合： a ba ca db cb dc d 实现方法如下： #!/bin/bashset -- $(cat $1) # 将输入文件的每一行依次赋值给位置变量，如第一行赋值给 $1，第二行给 $2。。。for i in $* # $* 为所有位置变量的 listdoshift for j in $* do printf "%s\t%s\n" "$i" "$j" donedone $ ./pair_combination.sh aaa ba ca db cb dc d]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Detecting pervasive positive selection step by step]]></title>
      <url>%2Fblog%2FDetecting-pervasive-positive-selection%2F</url>
      <content type="text"><![CDATA[Site-model: assumsing that the dn/ds rato is the same across branches, but different between sites(1) multiple sequence alignment speed: muscle &gt; mafft &gt; clustalW &gt; T-Coffee accuracy: mafft &gt; muscle &gt; T-Coffee &gt; clustalW cd /home/liuhui/nature_selection/exemple/data_for_codemlmafft-linsi ../input/HLA_DQB1.aa.fasta &gt; HLA_DQB1.aa.mafft.fasta (2) convert protein alignment to cds alignmentperl /home/liuhui/nature_selection/exemple/bin/pepMfa_to_cdsMfa.pl HLA_DQB1.aa.mafft.fasta ../input/HLA_DQB1.cds.fasta &gt; HLA_DQB1.cds.mafft.fasta (3) Remove spurious sequences and columns aa sequences (construct gene tree) trimal -automated1 -in HLA_DQB1.aa.mafft.fasta -out HLA_DQB1.aa.mafft.trimal.fasta -htmlout HLA_DQB1.aa.mafft.trimal.html -colnumbering &gt; HLA_DQB1.aa.mafft.trimal.cols cds sequences (for codeml) python /home/liuhui/nature_selection/bin/MSA_triplets_gaps_removed.py HLA_DQB1.cds.mafft.fasta HLA_DQB1.cds.mafft_removed_trigaps.fasta (4) convert fasta to phylip format/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.aa.mafft.trimal.fasta HLA_DQB1.aa.mafft.trimal.phy # construct tree/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.cds.mafft_removed_trigaps.fasta HLA_DQB1.cds.mafft_removed_trigaps.phy # for codeml (5) construct treephyml -i HLA_DQB1.aa.mafft.trimal.phy -q -d aa -m JTT -c 4 -a esed 's/\()\)[0-9]\.[^:]*:/\1:/g' HLA_DQB1.aa.mafft.trimal.phy_phyml_tree.txt &gt; HLA_DQB1.aa.mafft.trimal.tree (6) codeml# M0M1M2M3M7M8cd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M0M1M2M3M7M8cd HLA_DQB1_M0M1M2M3M7M8codeml HLA_DQB1_M0M1M2M3M7M8.ctl# M8acd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M8acd HLA_DQB1_M8acodeml HLA_DQB1_M8a.ctl (7) significant test np: the number of parameters lnL: log-likelihood value LRT: likelihood-ratio test Model_compared Model0 np0 lnL0 Model1 np1 lnL1 df LRT pvalueM7-M8 M7 44 -5047.785978 M8 46 -5011.936805 2 71.6983 2.69719269066922e-16M0-M3 M0 43 -5214.976615 M3 47 -5011.542624 4 406.868 9.12618975872726e-87M8-M8a M8a 45 -5031.655392 M8 46 -5011.936805 1 39.4372 3.38781154892534e-10M1a-M2a M1a 44 -5036.170805 M2a 46 -5014.302814 2 43.736 3.18308524710324e-10 (8) identification of sitesM2a Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 38 F 0.938 3.258 +- 0.761 55 L 0.999** 3.408 +- 0.518 66 Y 0.837 2.994 +- 0.991 86 D 0.997** 3.404 +- 0.527 99 G 0.978* 3.356 +- 0.615 99 G 0.978* 3.356 +- 0.615116 F 0.935 3.243 +- 0.766118 G 0.662 2.542 +- 1.171123 R 0.690 2.646 +- 1.182256 P 0.998** 3.406 +- 0.523257 Q 0.864 3.073 +- 0.947258 G 0.968* 3.334 +- 0.659259 P 0.776 2.838 +- 1.079260 P 0.971* 3.342 +- 0.644 M8 Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 14 T 0.539 1.770 +- 1.006 38 F 0.985* 2.688 +- 0.473 42 G 0.649 1.992 +- 0.956 55 L 1.000** 2.715 +- 0.417 66 Y 0.962* 2.641 +- 0.547 86 D 0.999** 2.714 +- 0.419 99 G 0.996** 2.709 +- 0.431113 E 0.518 1.714 +- 0.948116 F 0.989* 2.694 +- 0.459117 R 0.585 1.855 +- 0.957118 G 0.927 2.564 +- 0.632123 R 0.837 2.393 +- 0.822256 P 1.000** 2.715 +- 0.418257 Q 0.956* 2.630 +- 0.569258 G 0.992** 2.700 +- 0.451259 P 0.947 2.609 +- 0.588260 P 0.993** 2.703 +- 0.443]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[creat a blog]]></title>
      <url>%2Fblog%2Fcreat%20a%20blog%2F</url>
      <content type="text"><![CDATA[1. hexo new "new blog title"2. edit your text using Typora3. hexo generate4. hexo deploy]]></content>
    </entry>

    
  
  
</search>
