<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[linux 入门（一）：初识 linux 系统]]></title>
      <url>%2Fblog%2Flinux-%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9D%E8%AF%86-linux-%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[生信入门系列之 linux 入门（一）：初识 linux 系统​ Linux 是一个免费的操作系统，是生物信息分析中必不可少的工具。在 linux 系统中，所有的管理任务均可以在一个叫终端（terminal）的控制面板里完成，包括文件处理，软件安装以及用户管理。这个终端是交互的，即你运行特定的命令，相应的结果会在这个终端上显示出来。运行命令的方式是：在终端上输入你希望运行的命令，然后按回车键（Enter）。如果你想终止正在运行的命令，可以按 Chrl + C。 ​ 不同于 windows 系统，linux 的文件系统是一个目录树（directory tree）；如图 所示，其文件系统为一个树状结构。最顶端 “root”，用斜杠 “/” 表示。一般来说，普通用户，无论是直接打开终端还是远程登陆，所在的位置一般是在 /home/foo 下，其中的 foo 在这里指代用户名。 ​ linux 的树状文件系统（图片来自维基百科） ​ 如果你想查看当前所在的位置，可以在终端输入如下命令，并按回车键： pwd ​ 这个 linux 命令意思是“打印当前工作目录”，是 “print working directory” 的英文缩写；其返回结果是一个绝对路径（就是从根目录开始，依次将各级子目录的名字组合起来），应该类似这样： /home/foo ​ 与上述的树状文件系统相互比照，是不是立马清楚自己到底在哪里了？就好像 windows 下，到底在哪个盘的哪个文件夹里一样。这个绝对路径很有用，它不经让我们知道自己在哪儿，同时还可以告诉系统某个软件在哪儿，以及告诉软件要操作的文件在哪儿。举个例子吧，比如说我想调用一个软件，叫 vcftools，那么，我要运行它，只需要在终端输入如下命令并回车： vcftools --vcf input_data.vcf ​ 就可以轻轻松松算出 variants 的数目和 individuals 的数目。但很不幸的是，你也有可能得到如下结果： bash: vcftools: command not found ​ 大概意思就是，系统找不到这个命令在哪儿。可能有人会问了，既然是一个命令，为什么系统会找不到呢？其实，在linux 系统中，有一个非常核心的概念：一切皆文件！即在linux环境下，任何事物都以文件的形式存在。所以，如果你的从 vcftools 安装在 /home/foo/biosoft/vcftool-0.1.13/bin 这个绝对路径下，那么，你就可以这样运行它： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 就可以的结果啦。但也有可能得到如下结果： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf input_data.vcfstat error: No such file or directoryError: Can&apos;t determine file type of input_data.vcf ​ 这时也不要慌，只要在输入文件前加上绝对路径即可。加入 input_data.vcf 文件在 /home/foo/vcffile 下，可以这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 这时，如无意外，就可以得到如下结果了： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf /home/foo/vcffile/input_data.vcfUsing zlib version: 1.2.3.4Versions of zlib &gt;= 1.2.4 will be *much* faster when reading zipped VCF files.After filtering, kept 16 out of 16 IndividualsAfter filtering, kept 1116595 out of a possible 1116595 SitesRun Time = 5.00 seconds ​ 如果还报出一些奇奇怪怪的错误提示，那就首先检查一下您的输入法中是否为纯英文状态，中文和全角状态下的输入的空格都是会报错的。同时，linux 里，软件对字母大小写是敏感的，即 linux 认为 A 和 a 是两个不同的事物；也就是说大小写也是会造成错误的。 ​ 前面，我多次提到了绝对路径这个概念，不少心思敏捷的童鞋就会想了，有没有相对路径？有的。 ​ 举例说明，假如我们在 /home/foo 这个路径下，并且我们知道该路径下有 vcffile 和 biosoft 这两个目录；那么，我们可以这样运行上述的命令： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 可以看到，biosoft/vcftool-0.1.13/bin 和 vcffile 这两个路径都不是以斜杠 / 开头的，所以这两个路径都是相对路径。当然了，你也可也这样运行： biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 亦或这样： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 也可以进到 vcffile 这个目录里，这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 或者这样（ “..” 在这里代表上级目录，相应的，”../..“ 代表上级目录的上级目录）： ../biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 总之，想怎么运行，看心情！ ​ 相信看到这里，会有记忆力超好的童鞋会问了，我该如何像运行 pwd 那样运行 vcftools 呢？而不是在它前面加上一大串绝对路径或是相对路径！ ​ 要回答这个问题，小编先给大家展示两个命令（不深入讲解）： ​ 第一个是： which pwd ​ 返回的应该是： /bin/pwd ​ 第二个是： echo $PATH ​ 会返回类似下面的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 眼尖的童鞋会发现，上面一串结果其实就是许多绝对路径通过 “:” 连接在一起的（叫做环境变量），其中有一个路径 “/bin” ；而 which pwd 返回的结果是 /bin/pwd。这两个有什么关系呢？其实不必深究，我们只需要知道，linux 把 pwd 看作一个文件（还记得前面说的“一切皆文件”吗），linux 系统会在上述的环境变量中从左往右依次查找，看某个路径下是否有 pwd 这个文件，然后执行这个命令。并且，环境变量是可以编辑的， 即可在环境变量 “PATH” 中添加特定的路径。同理，如果我们的 vcftools 软件（其实就是个文件）的路径也在上述的路径中，就可以在终端直接输入 vcftools 就可以运行了。 ​ 那么问题来了，我们该如何将特定软件的路径发到上述的环境变量 “PATH” 中呢？ ​ 只需要通过 export 命令，在终端中输入以下内容，回车后，就可以将 vcftools 的路径导入到上述的环境变量中： export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; ​ 就会得到类似这样的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 当然，如果你是这样输入的， export PATH=&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos;:$PATH ​ 那就应该得到类似这样的结果： /home/foo/biosoft/vcftool-0.1.13/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games ​ 就可以不加绝对路径运行 vcftools 了。像这样： vcftools --vcf input_data.vcf ​ 但上述做法有一个弊端，就是这个做法是暂时性的，就是说每次打开或登陆终端时，都要运行一下这个命令才行，非常麻烦。所以怎么办呢？ ​ 其实不难，在每个用户的家目录下，即上述的 /home/foo 下，都有一个非常重要的隐藏文件 “.bashrc”，里面有许多我们每次启动或登陆终端时，linux 系统都会默认自动运行的命令。所以，只需将 export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; 添加到 “.bashrc” 文件的最后一行即可。这样我们每次启动或登陆终端时，系统就会自动运行这个命令了，这样就免去可多次手动添加的麻烦。如果不想重启终端，可以执行这个命令（相当于让系统执行一遍 “.bashrc” 中的命令）： source .bashrc ​ 怎么加呢？可以在家目录下，运行一个文本编辑命令 vim 或 nano： vim .bashrc 对于 vim 怎么使用，可以自行百度，有详细教程，这里不做赘诉（使用起来比较复杂）。 或另一个命令 nano： nano .bashrc ​ 这个比较简单，只需回车后： 按向下箭头（一直到文件最底部） 黏贴 export PATH=$PATH:&#39;/home/foo/biosoft/vcftool-0.1.13/bin&#39; 依次按 ctrl + x，y，Enter（即保存退出） 注意：/home/foo/biosoft/vcftool-0.1.13/bin 要做根据自己的实际路径做相应地改动。 ​ 下期预告：linux 基础命令]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 学习笔记]]></title>
      <url>%2Fblog%2Fpython-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[python 笔记1. 基础1.1 基本语法控制流相关# (1) 为何1 in [1,0] == True执行结果是False1 in [1,0] == True# 将被转为(1 in [1, 0]) and ([1, 0] == True)###### 同样的a &lt; b &lt; c# 会被转为(a &lt; b) and (b &lt; c) # b不会被解析两次# (2) 如何检测一个变量是否存在# (2.1) 检测本地变量if 'myVar' in locals(): # myVar exists. # (2.2) 检测全局变量if 'myVar' in globals(): # myVar exists.# (2.3) 检测一个对象是否包含某个属性if hasattr(obj, 'attr_name'): # obj.attr_name exists.# (3) Python中的三元运算符 a if test else b # 如果 test 为 True，返回 a，否则返回 b# 使用:&gt;&gt;&gt; 'true' if True else 'false''true'&gt;&gt;&gt; 'true' if False else 'false''false' 1.2 字符串相关# (1) 如何反向输出一个字符串&gt;&gt;&gt; 'hello world'[::-1]'dlrow olleh'# (2) 如何随机生成大写字母和数字组成的字符串'''6U1S754Z4UKKU911K4'''import string, random''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(6))# (3) 字符串的contains# (3.1)使用in关键字if not "blah" in somestring: continueif "blah" not in somestring: continue# (3.2) 使用字符串的find/index (注意index查找失败抛异常)s = "This be a string"if s.find("is") == -1: print "No 'is' here!"else: print "Found 'is' in the string." # (4) 如何判断一个字符串是数字def is_number(s): try: float(s) return True except ValueError: return False# (5) 字符串格式化 % vs format# 下列输出一致#!/usr/bin/pythonsub1 = "python string!"sub2 = "an arg"a = "i am a %s"%sub1b = "i am a &#123;0&#125;".format(sub1)c = "with %(kwarg)s!"%&#123;'kwarg':sub2&#125;d = "with &#123;kwarg&#125;!".format(kwarg=sub2)print aprint bprint cprint d# .format 还可以这样用，但用 % 时无法做到这点e = "i am a &#123;0&#125; &#123;0&#125;".format(sub1)# %只处理一个变量或一个元组, 你或许会认为下面的语法是正确的"hi there %s" % name#但当name恰好是(1,2,3)时，会抛出 TypeError 异常.为了保证总是正确的，你必须这么写"hi there %s" % (name,) # supply the single argument as a single-item tuple# (5) 将一个包含有字典的字符串转为一个字典&gt;&gt;&gt; s = "&#123;'muffin' : 'lolz', 'foo' : 'kitty'&#125;"&gt;&gt;&gt; import ast&gt;&gt;&gt; ast.literal_eval(s)&#123;'muffin': 'lolz', 'foo': 'kitty'&#125;# (6) 如何填充0到数字字符串中保证统一长度# (6.1) 对于字符串&gt;&gt;&gt; n = '4'&gt;&gt;&gt; print n.zfill(3)&gt;&gt;&gt; '004'# (6.2) 对于数字&gt;&gt;&gt; n = 4&gt;&gt;&gt; print '%03d' % n&gt;&gt;&gt; 004&gt;&gt;&gt; print "&#123;0:03d&#125;".format(4) # python &gt;= 2.6&gt;&gt;&gt; 004 1.3 文件相关# (1) 如何检查一个文件是否存在import os.pathprint os.path.isfile(fname)print os.path.exists(fname)# (2) 如何创建不存在的目录结构import os.pathif not os.path.exists(directory): os.makedirs(directory) # 需要注意的是，当目录在exists和makedirs两个函数调用之间被创建时，makedirs将抛出OSError# (3) 如何拷贝一个文件from shutil import copyfilecopyfile(src, dst)# (4) 如何找到一个目录下所有.txt文件# (4.1) 使用globimport globimport osos.chdir("/mydir")for files in glob.glob("*.txt"): print files# (4.2) 使用os.listdirimport osos.chdir("/mydir")for files in os.listdir("."): if files.endswith(".txt"): print files# (4.3) 或者遍历目录import osfor r,d,f in os.walk("/mydir"): for files in f: if files.endswith(".txt"): print os.path.join(r,files)# (5) 如何逐行读取文件# (5.1) 先将文件读入内存，然后逐行读取for line in open("test.txt").readlines(): print line# (5.2) 利用file的迭代器for line in open("test.txt"): #use file iterators print line 2. 基本数据结构2.1 列表# (1) Python 中如何复制一个列表# (1) 切片操作&gt;&gt;&gt; a = [1, 2, 3, [4, 5]]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(a), id(b)(4292218700L, 4292555596L)# 当列表中还有列表时，则无法实现真正的拷贝了&gt;&gt;&gt; a[2] = 0&gt;&gt;&gt; a[3][1] = 6&gt;&gt;&gt; a[1, 2, 0, [4, 6]]&gt;&gt;&gt; b[1, 2, 3, [4, 6]]# (2) list()函数&gt;&gt;&gt; c = ["a", "b", "c", ["d", "e"]]&gt;&gt;&gt; c['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; d = list(c)&gt;&gt;&gt; d['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; id(c), id(d)(4292555596L, 4292218732L)# 当列表中还有列表时，同样无法实现真正的拷贝了&gt;&gt;&gt; c[1] = 0&gt;&gt;&gt; c[3][1] = 0&gt;&gt;&gt; c['a', 0, 'c', ['d', 0]]&gt;&gt;&gt; d['a', 'b', 'c', ['d', 0]]# (3) “乘法”操作&gt;&gt;&gt; e = [1, 2, 3, [4, 5]]&gt;&gt;&gt; f = e * 1&gt;&gt;&gt; f[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(e), id(f)(4292721772L, 4292721260L)# 同样的问题&gt;&gt;&gt; e[1] = 0&gt;&gt;&gt; e[3][1] = 0&gt;&gt;&gt; e[1, 0, 3, [4, 0]]&gt;&gt;&gt; f[1, 2, 3, [4, 0]]# (4) copy.copy&gt;&gt;&gt; import copy&gt;&gt;&gt; g = [1, 2, 3, [4, 5]]&gt;&gt;&gt; h = copy.copy(g)&gt;&gt;&gt; h[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(g), id(h)(4292218860L, 4292218764L)# 还是一样&gt;&gt;&gt; g[1] = 0&gt;&gt;&gt; g[3][1] = 0&gt;&gt;&gt; g[1, 0, 3, [4, 0]]&gt;&gt;&gt; h[1, 2, 3, [4, 0]]# copy.deepcopy&gt;&gt;&gt; i = [1, 2, 3, [4, 5]]&gt;&gt;&gt; j = copy.deepcopy(i)&gt;&gt;&gt; j[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(i), id(j)(4292215020L, 4292598732L)# 完全新的拷贝&gt;&gt;&gt; i[1] = 0&gt;&gt;&gt; i[3][1] = 0&gt;&gt;&gt; i[1, 0, 3, [4, 0]]&gt;&gt;&gt; j[1, 2, 3, [4, 5]]# (2) 列表的 append 和 extend 的区别&gt;&gt;&gt; x = [1, 2]&gt;&gt;&gt; x.append(3)&gt;&gt;&gt; x[1, 2, 3]&gt;&gt;&gt; x.append([4,5])&gt;&gt;&gt; x[1, 2, 3, [4, 5]]&gt;&gt;&gt;&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; x.extend([4, 5])&gt;&gt;&gt; x[1, 2, 3, 4, 5]# (3) 如何随机地从列表中抽取变量foo = ['a', 'b', 'c', 'd', 'e']from random import choiceprint choice(foo)# (4) 如何将一个列表切分成若干个长度相同的子序列# 想要得到这样的效果lis = range(1, 1000)print chunks(lis, 10) -&gt; [ [ 1..10 ], [ 11..20 ], .., [ 991..999 ] ]# (4.1) 使用yield:def chunks(lis, n): """ Yield successive n-sized chunks from lis. """ for i in xrange(0, len(lis), n): yield lis[i:i+n]list(chunks(range(10, 75), 10))# (4.2) 直接处理def chunks(l, n): return [l[i:i+n] for i in range(0, len(l), n)] 2.2 字典# (1) 使用列表解析创建一个字典d = &#123;key: value for (key, value) in sequence&#125;&gt;&gt;&gt; &#123;i : chr(65+i) for i in range(4)&#125;&#123;0: 'A', 1: 'B', 2: 'C', 3: 'D'&#125;&gt;&gt;&gt; &#123;(k, v): k+v for k in range(4) for v in range(4)&#125;&#123;(0, 1): 1, (1, 2): 3, (3, 2): 5, (0, 0): 0, (3, 3): 6, (3, 0): 3, (3, 1): 4, (2, 1): 3, (0, 2): 2, (2, 0): 2, (1, 3): 4, (2, 3): 5, (2, 2): 4, (1, 0): 1, (0, 3): 3, (1, 1): 2&#125;# (2) 如何在单一表达式中合并两个Python字典&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = dict(x.items() + y.items())&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125;# (3) 如何映射两个列表成为一个字典&gt;&gt;&gt; keys = ['a', 'b', 'c']&gt;&gt;&gt; values = [1, 2, 3]&gt;&gt;&gt; dictionary = dict(zip(keys, values))&gt;&gt;&gt; print(dictionary)&#123;'a': 1, 'b': 2, 'c': 3&#125;# (4) 根据 dict 内值, 排序一个列表中的所有 dictlist_to_be_sorted = [&#123;'name':'Homer', 'age':39&#125;, &#123;'name':'Bart', 'age':10&#125;]# (4.1) 简单的做法newlist = sorted(list_to_be_sorted, key=lambda k: k['name'])# (4.2) 高效的做法from operator import itemgetternewlist = sorted(list_to_be_sorted, key=itemgetter('name'))# 根据值给字典排序x = &#123;1: 2, 3: 4, 4:3, 2:1, 0:0&#125;sorted(x.iteritems(), key=lambda k: k[1])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[networkx 笔记]]></title>
      <url>%2Fblog%2Fnetworkx-%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[networkx 复杂网络分析笔记主要参考1，2，3。 数据 facebook_combined.csv，relation_weight_sam.csv。 1. 建图networkx可以建立简单无向图graph，有向图digraph，可重复边的multi-graph。 1.1 简单无向图 graphimport networkx as nx#建立一个空的无向图GG=nx.Graph()#添加一个节点1G.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）G.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边G.add_edge(3,2)#输出全部的节点： [1, 2, 3]print G.nodes()#输出全部的边：[(2, 3)]print G.edges()#输出边的数量：1print G.number_of_edges()#输出点数print G.number_of_nodes() 无向图 1.2 有向图 digraph有向图的建立方式和无向图基本类似，只是在上述代码的第二行，将G = nx.Graph() 改为 G = nx.DiGraph() 。需要注意的是，此时再添加边3-2与边2-3，则被认为是两条不同的 import networkx as nxD=nx.DiGraph()#添加一个节点1D.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）D.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边D.add_edge(3,2)#输出全部的节点： [1, 2, 3]print D.nodes()#输出全部的边：[(2, 3), (3, 2)]print D.edges()#输出边的数量：2print D.number_of_edges()#输出点数print D.number_of_nodes() 同时，有向图和无向图是可以相互转化的，分别用到Graph.to_undirected() 和 Graph.to_directed()两个方法。 有向图 1.3 加权图有向图和无向图都可以给边赋予权重，用到的方法是add_weighted_edges_from，它接受1个或多个三元组[u,v,w]作为参数，其中u是起点，v是终点，w是权重。例如： # 添加0-1、1-2和2-3三条边，权重分别是3.0和7.5G.add_weighted_edges_from([(0,1,3.0),(1,2,7.5),(2,3,1.0)])# 如果想读取权重，可以使用get_edge_data方法，它接受两个参数u和v，即边的起始点。print G.get_edge_data(1,2)# 选出边的权重超过一个阈值的边estrong = [(u,v) for (u,v,d) in G.edges(data=True) if d["weight"] &gt; 3.0]print estrong # [(1, 2)] 加权图 这是一个加权图的代码 from matplotlib import useuse("Agg")import matplotlib.pyplot as pltimport networkx as nximport pandas as pddf = pd.read_csv("../data/relation_weight_sam.csv")G = nx.from_pandas_dataframe(df, 'node1', 'node2', edge_attr='weight' )# saves the positions of the nodes on the visualization# In detail positions is a dictionary where each node is# a key and the value is a position on the graph# &#123;'Fam38a_predicted': array([ 0.52246857, 0.4412573 ], dtype=float32),...&#125;positions = nx.spring_layout(G)# pass positions and set hold=Truenx.draw(G, pos=positions, hold=True, with_labels=False, node_size=30)weights = [w[2]['weight']*5 for w in G.edges(data=True)]#width can be array of floatsnx.draw_networkx_edges(G, pos=positions, width=weights)plt.savefig("../output/net_weight.png")# 前十行数据，weight是通过"numpy.random.rand()"模拟的。"""node1 node2 weightAblim2 Acsl6 0.0656480910603Ablim2 Apeg1 0.0727940253706Ablim2 Atp2a2 0.5280955211Ablim2 Boll_predicted 0.200347948345Ablim2 Cap2 0.108700562945Ablim2 Copb1 0.546335670525Ablim2 Creld2 0.420412397031Ablim2 Dtna_predicted 0.24546480993Ablim2 Dusp8_predicted 0.932345236242""" 1.4 可重复边的 multi-graphimport networkx as nxM=nx.MultiGraph() 1.5 点这里的点可以是任意可区分的对象（hashable），比如数字，字符串，对象等。 G.add_node(1)G.add_node('first_node')#这里用一个对象多为key来唯一区别一个点#我们还能够用一个列表来批量加入点G.add_nodes_from([1,2,3])#还可以用一个图对象作为点，嵌入到其他图中G.add_node(D) #这里D作为一个点的key#或者把一个图的所有点赋予另一个图G.add_nodes_from(D) #这里返回D的所有点，赋予G#与加入相同的传递方法，我们也可以删除点G.remove_node(1)G.remove_nodes_from([1,2,3]) 1.6 边这里的边可以使用两个单独的对象作为输入 G.add_edge(1,2) #表示1，2之间有一条边。#如果不存在点1，2，则会自动加入点集合。#或者以元组的形式作为输入e=(1,2)G.add_edge(*e)#这里的*代表吧元组解包（unpack），当作一个个的值扔到函数中去。#如果不解包，等价于#G.add_edge(e)=G.add_edge((1,2))与参数传递的方式不符。#类似的，我们还可以使用包含元组的列表来传递参数G.add_edges_from([(1,2),(2,3)])#我们还可以报一个图的边赋予另一个图G.add_edges_from(H)#删除G.remove_edge(1,2)G.remove_edges_from([(1,2),(2,3)]) 1.7 访问node_list = G.nodes()edge_list = G.edges()#可以返回包含点与边的列表node = G.node[‘first_node’]#如上根据key返回点edge = G.edge['first_node']['second_node']#同样的方法，返回两个key之间的边 1.8 属性我们可以给图，点，边赋予各种属性，最简单的就是权值属性 G.add_node(1,time='5pm')#在添加时跟上属性G.add_nodes_from([1,2,3],time='5pm')#批量添加点是，跟上统一的属性G.add_nodes_from([(3,&#123;'time':'5pm'&#125;), (4,&#123;'time':'4pm'&#125;)])#或者写成元组列表[（key,dict），（key,dict）]的形式G.node[1]['time']#根据字典key访问属性值。#给边添加属性也类似G.add_edge(1,2,time='3am')G.add_edges_from([(1,2,&#123;'time'='5pm'&#125;),(2,3,&#123;'time'=3am&#125;)])#批量赋予属性G.edge[1][2][‘time’]#访问#我们还可以使用特定的函数批量返回属性，如time = nx.get_edge_attributes(G,'time')#返回得到以元组为key,time属性为值得一个字典time[(1,2)] 1.9 图算法NetworkX提供了常用的图论经典算法，例如DFS、BFS、最短路、最小生成树、最大流等等，非常丰富，如果不做复杂网络，只作图论方面的工作，也可以应用NetworkX作为基本的开发包。 #调用多源最短路径算法，计算图G所有节点间的最短路径path=nx.all_pairs_shortest_path(G)#输出节点0、2之间的最短路径序列： [0, 1, 2]print path[0][2] 1.10 画图nx.draw(G) # 方法，至少接受一个参数：待绘制的网络G matplotlib.show() #显示出来 画图参数运行样式 node_size: 指定节点的尺寸大小(默认是300) node_color: 指定节点的颜色 (默认是红色，可以用字符串简单标识颜色，例如’r’为红色，’b’为绿色等) node_shape: 节点的形状（默认是圆形，用字符串’o’标识） alpha: 透明度 (默认是1.0，不透明，0为完全透明) width: 边的宽度 (默认为1.0) edge_color: 边的颜色(默认为黑色) style: 边的样式(默认为实现，可选： solid|dashed|dotted,dashdot) with_labels: 节点是否带标签（默认为True） font_size: 节点标签字体大小 (默认为12) font_color: 节点标签字体颜色（默认为黑色） 运用布局 circular_layout：节点在一个圆环上均匀分布 random_layout：节点随机分布 shell_layout：节点在同心圆上分布 spring_layout： 用Fruchterman-Reingold算法排列节点（样子类似多中心放射状） spectral_layout：根据图的拉普拉斯特征向量排列节点 添加文本 用plt.title()方法可以为图形添加一个标题，该方法接受一个字符串作为参数。 fontsize参数用来指定标题的大小。例如：plt.title(“BA Networks”, fontsize = 20)。 如果要在任意位置添加文本，则可以采用plt.text()方法。 2. 四种网络模型NetworkX提供了4种常见网络的建模方法，分别是：规则图，ER随机图，WS小世界网络和BA无标度网络。 2.1 规则图规则图差不多是最没有复杂性的一类图，random_graphs.random_regular_graph(d, n)方法可以生成一个含有n个节点，每个节点有d个邻居节点的规则图。 下面一段示例代码，生成了包含20个节点、每个节点有3个邻居的规则图： import networkx as nximport matplotlib.pyplot as plt# regular graphy# generate a regular graph which has 20 nodes &amp; each node has 3 neghbour nodes.RG = nx.random_graphs.random_regular_graph(3, 20)# the spectral layoutpos = nx.spectral_layout(RG)# draw the regular graphynx.draw(RG, pos, with_labels = False, node_size = 30)plt.show() 2.2 ER随机图ER随机图是早期研究得比较多的一类“复杂”网络，模型的基本思想是以概率p连接N个节点中的每一对节点。用random_graphs.erdos_renyi_graph(n,p)方法生成一个含有n个节点、以概率p连接的ER随机图： import networkx as nximport matplotlib.pyplot as plt# erdos renyi graph# generate a graph which has n=20 nodes, probablity p = 0.2.ER = nx.random_graphs.erdos_renyi_graph(20, 0.2)# the shell layoutpos = nx.shell_layout(ER)nx.draw(ER, pos, with_labels = False, node_size = 30)plt.show() 2.3 WS小世界网络 用random_graphs.watts_strogatz_graph(n, k, p)方法生成一个含有n个节点、每个节点有k个邻居、以概率p随机化重连边的WS小世界网络。 下面是一个例子： networkx-笔记/import networkx as nximport matplotlib.pyplot as plt# WS network# generate a WS network which has 20 nodes,# each node has 4 neighbour nodes,# random reconnection probability was 0.3.WS = nx.random_graphs.watts_strogatz_graph(20, 4, 0.3)# circular layoutpos = nx.circular_layout(WS)nx.draw(WS, pos, with_labels = False, node_size = 30)plt.show() 2.4 BA无标度网络用random_graphs.barabasi_albert_graph(n, m)方法生成一个含有n个节点、每次加入m条边的BA无标度网络。 下面是一个例子： import networkx as nximport matplotlib.pyplot as plt# BA scale-free degree network# generalize BA network which has 20 nodes, m = 1BA = nx.random_graphs.barabasi_albert_graph(20, 1)# spring layoutpos = nx.spring_layout(BA)nx.draw(BA, pos, with_labels = False, node_size = 30)plt.show() 对BA模型实现代码的分析 #定义一个方法，它有两个参数：n - 网络节点数量；m - 每步演化加入的边数量def barabasi_albert_graph(n, m): # 生成一个包含m个节点的空图 (即BA模型中t=0时的m0个节点) G=empty_graph(m) # 定义新加入边要连接的m个目标节点 targets=range(m) # 将现有节点按正比于其度的次数加入到一个数组中，初始化时的m个节点度均为0，所以数组为空 repeated_nodes=[] # 添加其余的 n-m 个节点，第一个节点编号为m（Python的数组编号从0开始） source=m # 循环添加节点 while source&lt;n: # 从源节点连接m条边到选定的m个节点targets上（注意targets是上一步生成的） G.add_edges_from(zip([source]*m,targets)) # 对于每个被选择的节点，将它们加入到repeated_nodes数组中（它们的度增加了1） repeated_nodes.extend(targets) # 将源点m次加入到repeated_nodes数组中（它的度增加了m） repeated_nodes.extend([source]*m) # 从现有节点中选取m个节点 ，按正比于度的概率（即度优先连接） targets=set() while len(targets)&lt;m: #按正比于度的概率随机选择一个节点，见注释1 x=random.choice(repeated_nodes) #将其添加到目标节点数组targets中 targets.add(x) #挑选下一个源点，转到循环开始，直到达到给定的节点数n source += 1 #返回所得的图G return G from matplotlib import useuse("Agg")import randomimport networkx as nxfrom networkx.generators.classic import empty_graphimport matplotlib.pyplot as pltdef barabasi_albert_graph(n, m): G=empty_graph(m) targets=range(m) repeated_nodes=[] source=m while source&lt;n: G.add_edges_from(zip([source]*m,targets)) repeated_nodes.extend(targets) repeated_nodes.extend([source]*m) targets=set() while len(targets)&lt;m: x=random.choice(repeated_nodes) targets.add(x) source += 1 return G ##G=nx.Graph()G = barabasi_albert_graph(400,6)pos = nx.spring_layout(G)nx.draw(G, pos, with_labels = False, node_size = 30)plt.savefig("../output/BA_400_6.png") 3. 统计指标计算3.1 度、度分布 NetworkX可以用来统计图中每个节点的度，并生成度分布序列。 import networkx as nximport matplotlib.pyplot as plt #生成一个n=1000，m=3的BA无标度网络G = nx.random_graphs.barabasi_albert_graph(1000,3)#返回某个节点的度print G.degree(0)#返回所有节点的度print G.degree()#返回图中所有节点的度分布序列（从1至最大度的出现频次）print nx.degree_histogram(G)#返回图中所有节点的度分布序列degree = nx.degree_histogram(G)#生成x轴序列，从1到最大度x = range(len(degree))#将频次转换为频率y = [z / float(sum(degree)) for z in degree]#在双对数坐标轴上绘制度分布曲线plt.loglog(x,y,color="blue",linewidth=2)#显示图表plt.show() 3.2 群聚系数# 平均群聚系数nx.average_clustering(G)# 各个节点的群聚系数nx.clustering(G) 3.3 直径和平均距离# 图G的直径（最长最短路径的长度）nx.diameter(G)# 图G所有节点间平均最短路径长度nx.average_shortest_path_length(G) 3.4 中心性一个图的直径是所有点之间最长的最短路径。在连接中心度，我们需要寻找一个点，这个点出现在很多点的最短路径中。出现的次数越多，连接中心性越高。这样的点，可以作为一个桥梁作用。意义：分析该节点对网络信息流动的影响，如：考察此人的社交能力或对于社会网络中信息流动的影响力。 betweenness centralityimport networkx as nximport matplotlib.pyplot as pltG=nx.Graph()print G.edges() # []G.add_edges_from([(1,2),(2,3),(2,4),(2,5),(1,3),(1,4),(3,5),(4,6)])print G.edges()#[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5), (3, 5), (4, 6)]# betweenness centralitybc = nx.betweenness_centrality(G)print sorted(bc.items(), key=lambda k: k[1], reverse=True)# [(4, 0.4), (2, 0.35000000000000003), (1, 0.1), (3, 0.05), (5, 0.0), (6, 0.0)]# spring layoutpos = nx.spring_layout(G)nx.draw(G, pos, with_labels = True, node_size = 100,font_size=6,font_color='b')plt.show() 另一个复杂的例子 import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")def most_important(G): """ returns a copy of G with the most important nodes according to the pagerank """ ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')plt.savefig("../output/FB_BetCen.png", dpi = 300)###"""node1,node20,10,20,30,40,50,60,70,8...2420,25432420,25552420,25672420,25922420,25972420,25982420,26092420,26172420,26292420,26422420,26432420,26532421,24372421,26342422,24412422,2558...""" 4. 社区发现（Community detection）import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig("../output/FB_commu.png", dpi = 300)# get modularitymod = community.modularity(part,G)print("modularity:", mod) 也可以在betweenness centrality的基础上画community detection: import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df["node1"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis("off")def most_important(G): """ returns a copy of G with the most important nodes according to the pagerank """ ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_networkx(G, pos = spring_pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig("../output/FB_BC_commu.png", dpi = 300)# get modularitymod = community.modularity(part,G)print("modularity:", mod)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何在遍历时，正确删除列表中的 items]]></title>
      <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E5%9C%A8%E9%81%8D%E5%8E%86%E6%97%B6%EF%BC%8C%E6%AD%A3%E7%A1%AE%E5%88%A0%E9%99%A4%E5%88%97%E8%A1%A8%E4%B8%AD%E7%9A%84-items%2F</url>
      <content type="text"><![CDATA[错误的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x: if i in y: x.remove(i)print x-----------------['a', 'c', 'd'] 正确的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x[:]: if i in y: x.remove(i)print x-----------------['a', 'd'] 实际上，id(x)与id(x[:])是不同的，所以只有在x的副本（x[:]）中遍历，然后在x中删除，才不会造成错误。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[根据一个list文件生成一个组合]]></title>
      <url>%2Fblog%2F%E6%A0%B9%E6%8D%AE%E4%B8%80%E4%B8%AAlist%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%E7%BB%84%E5%90%88%2F</url>
      <content type="text"><![CDATA[假设有一个 list 如下： $ cat aaabcd 期望生成如下组合： a ba ca db cb dc d 实现方法如下： #!/bin/bashset -- $(cat $1) # 将输入文件的每一行依次赋值给位置变量，如第一行赋值给 $1，第二行给 $2。。。for i in $* # $* 为所有位置变量的 listdoshift for j in $* do printf "%s\t%s\n" "$i" "$j" donedone $ ./pair_combination.sh aaa ba ca db cb dc d]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Detecting pervasive positive selection step by step]]></title>
      <url>%2Fblog%2FDetecting-pervasive-positive-selection%2F</url>
      <content type="text"><![CDATA[Site-model: assumsing that the dn/ds rato is the same across branches, but different between sites(1) multiple sequence alignment speed: muscle &gt; mafft &gt; clustalW &gt; T-Coffee accuracy: mafft &gt; muscle &gt; T-Coffee &gt; clustalW cd /home/liuhui/nature_selection/exemple/data_for_codemlmafft-linsi ../input/HLA_DQB1.aa.fasta &gt; HLA_DQB1.aa.mafft.fasta (2) convert protein alignment to cds alignmentperl /home/liuhui/nature_selection/exemple/bin/pepMfa_to_cdsMfa.pl HLA_DQB1.aa.mafft.fasta ../input/HLA_DQB1.cds.fasta &gt; HLA_DQB1.cds.mafft.fasta (3) Remove spurious sequences and columns aa sequences (construct gene tree) trimal -automated1 -in HLA_DQB1.aa.mafft.fasta -out HLA_DQB1.aa.mafft.trimal.fasta -htmlout HLA_DQB1.aa.mafft.trimal.html -colnumbering &gt; HLA_DQB1.aa.mafft.trimal.cols cds sequences (for codeml) python /home/liuhui/nature_selection/bin/MSA_triplets_gaps_removed.py HLA_DQB1.cds.mafft.fasta HLA_DQB1.cds.mafft_removed_trigaps.fasta (4) convert fasta to phylip format/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.aa.mafft.trimal.fasta HLA_DQB1.aa.mafft.trimal.phy # construct tree/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.cds.mafft_removed_trigaps.fasta HLA_DQB1.cds.mafft_removed_trigaps.phy # for codeml (5) construct treephyml -i HLA_DQB1.aa.mafft.trimal.phy -q -d aa -m JTT -c 4 -a esed 's/\()\)[0-9]\.[^:]*:/\1:/g' HLA_DQB1.aa.mafft.trimal.phy_phyml_tree.txt &gt; HLA_DQB1.aa.mafft.trimal.tree (6) codeml# M0M1M2M3M7M8cd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M0M1M2M3M7M8cd HLA_DQB1_M0M1M2M3M7M8codeml HLA_DQB1_M0M1M2M3M7M8.ctl# M8acd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M8acd HLA_DQB1_M8acodeml HLA_DQB1_M8a.ctl (7) significant test np: the number of parameters lnL: log-likelihood value LRT: likelihood-ratio test Model_compared Model0 np0 lnL0 Model1 np1 lnL1 df LRT pvalueM7-M8 M7 44 -5047.785978 M8 46 -5011.936805 2 71.6983 2.69719269066922e-16M0-M3 M0 43 -5214.976615 M3 47 -5011.542624 4 406.868 9.12618975872726e-87M8-M8a M8a 45 -5031.655392 M8 46 -5011.936805 1 39.4372 3.38781154892534e-10M1a-M2a M1a 44 -5036.170805 M2a 46 -5014.302814 2 43.736 3.18308524710324e-10 (8) identification of sitesM2a Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 38 F 0.938 3.258 +- 0.761 55 L 0.999** 3.408 +- 0.518 66 Y 0.837 2.994 +- 0.991 86 D 0.997** 3.404 +- 0.527 99 G 0.978* 3.356 +- 0.615 99 G 0.978* 3.356 +- 0.615116 F 0.935 3.243 +- 0.766118 G 0.662 2.542 +- 1.171123 R 0.690 2.646 +- 1.182256 P 0.998** 3.406 +- 0.523257 Q 0.864 3.073 +- 0.947258 G 0.968* 3.334 +- 0.659259 P 0.776 2.838 +- 1.079260 P 0.971* 3.342 +- 0.644 M8 Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 14 T 0.539 1.770 +- 1.006 38 F 0.985* 2.688 +- 0.473 42 G 0.649 1.992 +- 0.956 55 L 1.000** 2.715 +- 0.417 66 Y 0.962* 2.641 +- 0.547 86 D 0.999** 2.714 +- 0.419 99 G 0.996** 2.709 +- 0.431113 E 0.518 1.714 +- 0.948116 F 0.989* 2.694 +- 0.459117 R 0.585 1.855 +- 0.957118 G 0.927 2.564 +- 0.632123 R 0.837 2.393 +- 0.822256 P 1.000** 2.715 +- 0.418257 Q 0.956* 2.630 +- 0.569258 G 0.992** 2.700 +- 0.451259 P 0.947 2.609 +- 0.588260 P 0.993** 2.703 +- 0.443]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[creat a blog]]></title>
      <url>%2Fblog%2Fcreat%20a%20blog%2F</url>
      <content type="text"><![CDATA[1. hexo new "new blog title"2. edit your text using Typora3. hexo generate4. hexo deploy]]></content>
    </entry>

    
  
  
</search>
