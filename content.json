{"meta":{"title":"liuhui | 刘辉","subtitle":"不忘初心","description":"北京林业大学在读硕士<br>liuhui@bjfu.edu.cn","author":"Hui Liu","url":"https://hui-liu.github.io"},"pages":[{"title":"categories","date":"2017-03-02T14:34:26.000Z","updated":"2017-03-02T14:34:54.101Z","comments":true,"path":"categories/index.html","permalink":"https://hui-liu.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2017-03-02T14:28:37.000Z","updated":"2017-06-08T13:23:06.204Z","comments":true,"path":"about/index.html","permalink":"https://hui-liu.github.io/about/index.html","excerpt":"","text":"关于我求学经历： 2017.09 - ：北京林业大学林木遗传育种专业在读博士 2015.09 - 2017.07：北京林业大学林木遗传育种专业在读硕士 2011.09-2015.07：北京林业大学学士 实践经历： 微信公众号生信百科创始人之一 联系方式：liuhui@bjfu.edu.cn"},{"title":"tags","date":"2017-03-02T14:31:33.000Z","updated":"2017-03-02T14:32:50.317Z","comments":true,"path":"tags/index.html","permalink":"https://hui-liu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"blast知多少","slug":"blast知多少","date":"2017-06-04T08:17:00.000Z","updated":"2017-06-05T13:09:22.443Z","comments":true,"path":"blog/blast知多少/","link":"","permalink":"https://hui-liu.github.io/blog/blast知多少/","excerpt":"","text":"BLAST 的简单介绍BLAST 是 NCBI 开发的一个基于序列相似性的数据库搜索程序。BLAST 是 Basic Local Alignment Search Tool 的缩写。而 BLAST+ 可以说是 BLAST 的升级版，核心算法是一样的，但在运行速度上有了较大的提升。由于 BLAST 和 BLAST+ 核心算法一样，为了方便，文章统一用 BLAST。 BLAST+ 的下载地址为：ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.2.31/，有 linux 版和 windows 版。对于 linux 版，解压即可使用，对于 windows，双击，按默认设置安装即可。 BLAST 比对，所需要的数据分为两部分： query sequence，自己感兴趣的序列，一般以 fasta 格式存储。 database，一般也是 fasta 格式，可以是某个物种的基因组，也可以是其它数据库，如 Nr 数据库，Swiss-Prot 数据库等。 BLAST 比对基于生物学原理需要了解的两个概念： 同源基因：指基因来自一个共同祖先。可分为直系同源基因（ortholog）和旁系同源基因（paralog）。直系同源基因指基因通过物种形成事件而产生的物种间的拷贝；而旁系同源基因指同一个物种中，基因通过基因复制事件，产生一份新的拷贝。 下图是一个简单的例子，species w 中的基因 A 进过复制事件，产生一个新的拷贝；为了区分，原基因 A 命名为 A1，新产生的拷贝命名为 A2，则 A1 和 A2 互为旁系同源基因。随后，species w 经过物种形成事件，产生 species x 和 species y；则 A1 和 A2 分别在 species x 中记为 A1x 和 A2x，在 species y 中记为 A1y 和 A2y。那么，A1x 和 A1y 为直系同源基因；A2x 和 A2y 也是直系同源基因。 相似性：指序列之间的相似度。无论是直系同源基因还是旁系同源基因产生后，经过漫长的演化历程，这些基因会通过突变产生分化，使得不同物种间的直系同源基因或同一物种中的旁系同源基因不再一致，但具有很高的相似性；如过演化时间足够长或这个基因的保守性不高，那么，同源基因的相似性就会很低；还有可能由于趋同进化，使得两个基因具有相似性，但这在序列水平上，发生的可能性较低。 BLAST （或其它相似性比对软件）的生物学原理就是基于序列的相似性，推断该序列的同源基因，又根据同源基因具有相同功能，从而达到给 query sequence 进行功能注释的目的，这个方法往往是有效的。尽管方法会误判（假阳性）或漏判（假阴性），但目前还没有能替代相似性推断同源性的方法，只能通过设置合理的阈值，尽量减少假阳性和假阴性的发生。 BLAST 算法的简单介绍从名字可以看出，BLAST 是基于局部比对的，其算法的主要分以下两步： 第一步： 将 query sequence 和 database 中的序列打碎成小片段，在 BLAST 中，称之为 words；然后将 query 中的每个 word 比对到 database 中相似的 words，并为联配（alignment）的结果打分；打分的依据是核苷酸或氨基酸替换矩阵，这个矩阵可以根据核苷酸的替换或颠换的特点，或氨基疏水性或正负电等特点，规定核苷酸间替换或氨基酸间替换的得分。如果得分超过设置的阈值，则认为 query word 和 database word 间具有相似性，进而形成 query words 和 database words 的列表。然后，BLAST 会根据 database words，扫描整个数据库，找出这些 database words 来自 database 的哪些序列。 把序列打断成 words 并找出这些 words 与 database 中的哪些序列相似，这个策略可以实现蛋白质序列和氨基酸序列之间的相似性比对。如 query sequence 是一条蛋白质序列，想知道这条序列是 DNA database 中的哪条 DNA 序列编码形成的，BLAST 中的子程序 tblastn 可以实现这个工作。tblastn 按照 six reading frames 的原则，将 DNA database 翻译成氨基酸数据库。然后，将 query sequence 打断成 words，与氨基酸数据做比对。 第二步： 这一步的目的是 words 的延伸和结果得分排序。BLAST 的算法，尽可能的延伸 words，使得 words 形成更长的片段，同时，计算延伸后的得分，当得分小于指定的阈值，停止延伸。延伸过程中的打分方式一般和第一步中的方法一样。由于 query sequence 会形成很多个 words，这样，就有可能产生多个匹配结果；所以，通过某种方式，如总的比对得分或 E-value，进行排序，将最好的几个结果保留下来（通过参数指定阈值）。 BLAST 的分类 程序 query database blastn 核酸 核酸 blastp 蛋白质 蛋白质 blastx 核酸 蛋白质 tblastn 蛋白质 核酸 tblastx 核酸 核酸 blastx 、tblastn 和 tblastx 中的核酸序列按 six reading frames 的原则翻译成蛋白质序列，然后进行比对 BLAST 的简单使用这里讲 BLAST+ 的用法，分两步，第一步是建库： makeblastdb -in genome.fasta -dbtype nucl 这里的 genome.fasta 就是上面说的 datatase；参数 -dbtype：指定数据库的类型，nucl 指的是核苷酸序列，如果是蛋白质序列，则用 prot 代替。 第二步是比对： tblastn -query query_pep.fasta -db genome.fasta -evalue 1e-5 -num_threads 20 -max_target_seqs 5 -out query_pep.outfmt6 -outfmt \"6 qseqid sseqid pident qcovs mismatch gapopen qstart qend sstart send evalue bitscore\" -evalue 指定 E-value的值，E-value 用于评价 bitscore （输出文件 query_pep.outfmt6 的第12列）的可靠性，该值越小可靠性越高。 -num_threads 指定线程数，根据自己的计算机资源调整。 -num_threads指定保留多少个结果，例子表示保留最好的 5 个比对结果。 -outfmt 指定输出格式，如下 0 = pairwise, 1 = query-anchored showing identities, 2 = query-anchored no identities, 3 = flat query-anchored, show identities, 4 = flat query-anchored, no identities, 5 = XML Blast output, 6 = tabular, 7 = tabular with comment lines, 8 = Text ASN.1, 9 = Binary ASN.1,10 = Comma-separated values,11 = BLAST archive format (ASN.1) 6 表示以列表的方式输出，而每一列的含义，按引号中的字符指定： qseqid sseqid pident qcovs mismatch gapopen qstart qend sstart send evalue bitscore 依次表示： 列名 含义 qseqid query sequence 的 id sseqid database sequence 的 id pident 相似度 qcovs 覆盖度（比对上的片段占query sequence长度的百分比）， mismatch 错配数 gapopen gap 数 qstart query sequence 比对的起始位置 qend query sequence 比对的终止位置 sstart database sequence 比对的起始位置 send database sequence 比对的终止位置 evalue 评价 bitscore 的可靠性，值越小越好 bitscore 比对得分，越高越好 格式 6, 7, 和 10 支持输出的列及含义如下： qseqid means Query Seq-id qgi means Query GI qacc means Query accesion qaccver means Query accesion.version qlen means Query sequence length sseqid means Subject Seq-id sallseqid means All subject Seq-id(s), separated by a ';' sgi means Subject GI sallgi means All subject GIs sacc means Subject accession saccver means Subject accession.version sallacc means All subject accessions slen means Subject sequence length qstart means Start of alignment in query qend means End of alignment in query sstart means Start of alignment in subject send means End of alignment in subject qseq means Aligned part of query sequence sseq means Aligned part of subject sequence evalue means Expect value bitscore means Bit score score means Raw score length means Alignment length pident means Percentage of identical matches nident means Number of identical matches mismatch means Number of mismatches positive means Number of positive-scoring matches gapopen means Number of gap openings gaps means Total number of gaps ppos means Percentage of positive-scoring matches frames means Query and subject frames separated by a '/' qframe means Query frame sframe means Subject frame btop means Blast traceback operations (BTOP) staxids means unique Subject Taxonomy ID(s), separated by a ';' (in numerical order) sscinames means unique Subject Scientific Name(s), separated by a ';' scomnames means unique Subject Common Name(s), separated by a ';'sblastnames means unique Subject Blast Name(s), separated by a ';' (in alphabetical order)sskingdoms means unique Subject Super Kingdom(s), separated by a ';' (in alphabetical order) stitle means Subject Titlesalltitles means All Subject Title(s), separated by a '&lt;&gt;' sstrand means Subject Strand qcovs means Query Coverage Per Subject qcovhsp means Query Coverage Per HSP 可以运行： tblastn -help 查看 tblastn 的所有参数。 如何设定阈值需要考虑的值有三个 E-value：做功能注释时，通常将序列比对到 swiss-prot 数据库和 NCBI 的 Nr 数据库，这时，设置 E-value 为 1e-5（10的-5次幂）就可以了；如果是其他较小的数据库，如拟南芥基因组，建议将 E-value 设置为 1e-10。因为 E-value 的大小与数据库的大小有关（公式如下），其中 n 代表数据库大小，S 为 bitscore 值。当然 E-value 值设置的大小应按不同的研究，设置不同的大小，可以按特定研究的文献进行设置。 相似度：即 pident。在氨基酸水平上，相似度可以设置为 30，严格一些可以设置为 70；在核苷酸水上，可以设置 70 或 90。没有统一的标准，还是以特定研究的参考文献为准。 覆盖度：即 qcovs。由于 BLAST 是局部相似性比对，即考虑 query sequence 中的某个片段与 database sequence 中的某个片段的相似性，而 E-value和相似度都是评价这些片段的相似性，是一个局部评价体系，所以，还要有一个全局的评价体系，如覆盖度。因此，想要查找某个基因在 database 中的相似序列，还需要做一个全局覆盖度不能太低；可以按特定要求，设置为 50，70 或 90。 无论如何，需要记住的一点是，相似性并不能证明同源性。","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"software","slug":"software","permalink":"https://hui-liu.github.io/tags/software/"}]},{"title":"笔记本上的生信分析：根据区间批量提取序列","slug":"笔记本上的生信分析：根据区间批量提取序列","date":"2017-05-27T08:56:14.000Z","updated":"2017-06-07T03:51:48.092Z","comments":true,"path":"blog/笔记本上的生信分析：根据区间批量提取序列/","link":"","permalink":"https://hui-liu.github.io/blog/笔记本上的生信分析：根据区间批量提取序列/","excerpt":"","text":"给定 chromosome 或 scaffold 的坐标（起始和终止位置），如何将对应序列提取出来？ 这个问题需要分类讨论，以下面的数据为例： 1 18870555 188725671 14014796 140150291 14015227 140155051 27611499 276121791 6053214 6053029 a. 如果起始坐标小于终止位置，则直接提取 如 1 18870555 18872567 表示 1 号染色体的 18870555 到 18872567 这个区间的序列（包括起始）。 提取方法是应用 python 的序列切片语法；用 chr1 表示一号染色体的序列，则这样提取这个区间的序列： chr1[18870555-1: 18872567] 起始坐标减 1，是因为 python 的计数是从 0 开始的；终止位置不减 1，是因为 python 的序列切片语法遵循左闭右开的原则，即切片时包括起始坐标，但不包括终止坐标。 b. 如果起始位置大于终止位置，则提取其反向互补序列 以1 6053214 6053029 为例。 这个情况，实现方法分两步： b1. 首先互换起始和终止坐标，并提取这个区间的序列 seq = chr1[6053029-1: 6053214] b2. 然后， 用一个函数，将 seq 转成其反向互补的序列 revcomp(seq) 那么，应该如何写这个函数呢？看下边例子 def revcomp(dna): bases = 'ATGCTACG' complement_dict = &#123;bases[i]:bases[i+4] for i in range(4)&#125; dna = reversed(dna) result = [complement_dict[base] for base in dna] return ''.join(result)my_dna = 'AAATTTCGCGCG'print revcomp(my_dna) 结果是： CGCGCGAAATTT 其实现步骤分两步： 第一步：反向 这步直接用 python 的内置函数：reversed。可以将 AAATTTCGCGCG 的顺序反过来：GCGCGCTTTAAA。 第二步：互补 这一步的思路是，首先构建一个互补的字典，如 {&quot;A&quot; : &quot;T&quot;} ，而 complement_dict 则内容如下： &#123;'A': 'T', 'C': 'G', 'T': 'A', 'G': 'C'&#125; 所以，当对序列 GCGCGCTTTAAA 进行遍历时，可以通过 complement_dict，转换成相应的互补碱基，如 G 转换成 C。 好了，上面讲的是思路，下面的程序的实现了上面思路。运行方法是： python extract_seqs_by_coordinates.py sequences.fasta regions.txt out.fasta 其中， extract_seqs_by_coordinates.py 为 python 程序，其中的的代码在后面贴出来； sequences.fasta 是啥，大家应该知道了；regions.txt 的数据格式前面已经给出来了，通过 awk 等 linux 命令，可以很容易地从 blast 等软件的结果中提取出来，这里不再赘述（如果软件跑出来的数据格式比较复杂，那就是另外一个问题了）； out.fasta 是输出文件，自己定义。 extract_seqs_by_coordinates.py 程序的代码如下： 其中，coortoDict 函数将 regions.txt 文件内容转换成 python 的字典，如 {&#39;2&#39;: [[9025486, 9025542, &#39;-&#39;], [180481, 180543, &#39;-&#39;]]} import sysUSAGE = \"\\nusage: python %s sequences.fasta regions.txt out.fasta\\n\" % sys.argv[0]if len(sys.argv) != 4: print USAGE sys.exit()def parseFasta(filename): fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasdef coortoDict(filename): coor = &#123;&#125; with open(filename, 'r') as f: for line in f: lsplit = line.split() strand = \"+\" if int(lsplit[1]) &gt; int(lsplit[2]): lsplit[1], lsplit[2] = lsplit[2], lsplit[1] strand = \"-\" coor.setdefault(lsplit[0],[]).append([int(lsplit[1]), int(lsplit[2]), strand]) return coor def revcomp(seq): bases = 'ABCDGHKMNRSTUVWXYabcdghkmnrstuvwxyTVGHCDMKNYSAABWXRtvghcdmknysaabwxr' complement_dict = &#123;bases[i]:bases[i+34] for i in range(34)&#125; seq = reversed(seq) result = [complement_dict[base] for base in seq] return ''.join(result)fas = parseFasta(sys.argv[1])coors = coortoDict(sys.argv[2])OUT = open(sys.argv[3], 'w')for i in coors: for j in coors[i]: if j[2] ==\"-\": seq = revcomp(fas[i][j[0]-1: j[1]]) OUT.write(\"&gt;\" + i + \":\" + str(j[0]) + \"-\" + str(j[1]) + \"\\n\" + seq + \"\\n\") else: seq = fas[i][j[0]-1: j[1]] OUT.write(\"&gt;\" + i + \":\" + str(j[0]) + \"-\" + str(j[1]) + \"\\n\" + seq + \"\\n\")OUT.close()","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"批量提取序列小程序","slug":"批量提取序列小程序","date":"2017-05-20T12:11:06.000Z","updated":"2017-05-21T03:23:25.007Z","comments":true,"path":"blog/批量提取序列小程序/","link":"","permalink":"https://hui-liu.github.io/blog/批量提取序列小程序/","excerpt":"","text":"linux 系统自带有 python 2.7，在 windows 上需要自己安装。 windows 用户安装 python 需要两个步骤： 第一步：下载及安装 python 1 到这个网址里：https://www.python.org/downloads/ 2 点击：Download Python 2.7.13 3 下载到了这个文件后，双击并按默认安装即可 第二步：配置 python 环境变量 1 右键 =&gt; 属性 2 点击：高级系统设置 3 选：环境变量 4 双击箭头处的 Path 5 点击：新建 6 将 C:\\Python27 复制到箭头处 7 同 5 和 6，将 C:\\Python27\\Scripts 复制到箭头处 点击确定即可： 运行 python安装好 python 后，接下来将今天的主题，根据 ID 号，批量提取序列；用到的程序便是 python 脚本。 程序的名称是：extract_seqs_by_ids.py 输入文件： fasta 格式文件 (拟南芥所有蛋白质序列)：TAIR10_pep_20110103.fas 存储 id 号的文件为：id_list 上述文件都在一个叫 shengxinbaike 文件夹里。 到这个文件夹外面，按以下步骤进行操作： 1 选中文件夹 shengxinbaike 2 按住 shift 键 3 单击右键；会出现如下图所示的菜单： 4 点击：在此处打开命令窗口 5 输入：dir；并回车 可以看到 python 脚本和两个输入文件。 6 输入：python extract_seqs_by_ids.py TAIR10_pep_20110103.fas id_list out.fasta out.fasta 是输出文件的名称，自己定义。 python 脚本的代码如下： import sysUSAGE = \"\\nusage: python %s sequences.fasta id_list out.fasta\\n\" % sys.argv[0]if len(sys.argv) != 4: print USAGE sys.exit()def parseFasta(filename): fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasfas = parseFasta(sys.argv[1])IDs = open(sys.argv[2], 'r')OUT = open(sys.argv[3], 'w')for line in IDs: id = line.rstrip(\"\\n\") OUT.write(\"&gt;\" + id + \"\\n\" + fas[id] + \"\\n\") IDs.close()OUT.close()","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"基因组大小和GC含量(一)","slug":"基因组大小和GC含量-一","date":"2017-05-13T13:15:56.000Z","updated":"2017-05-16T08:38:10.353Z","comments":true,"path":"blog/基因组大小和GC含量-一/","link":"","permalink":"https://hui-liu.github.io/blog/基因组大小和GC含量-一/","excerpt":"","text":"本文以拟南芥基因组（tair 10）和人类基因组（hg38）为例，进行讲解。 拟南芥基因组下载： wget https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas 人类基因组下载： wget http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz 解压：gunzip hg38.fa.gz 基因组大小对于一个物种，我们首先感兴趣的是其基因组的大小。基因组大小为基因组中所有碱基的总和，显然不是文件大小。 $ ls -lhtotal 3.2Gdrwxrwxr-x 2 liuhui liuhui 4.0K May 13 17:04 bin-rw-rw-r-- 1 liuhui liuhui 3.1G May 13 17:00 hg38.fa-rw-rw-r-- 1 liuhui liuhui 116M May 13 16:39 TAIR10_chr_all.fas 拟南芥基因组大小 $ python bin/genome_size.py TAIR10_chr_all.fas119667750 bp119 Mbp 拟南芥基因组大小为 119 M 人类基因组大小 $ python bin/genome_size.py hg38.fa3209286105 bp3209 Mbp 人类基因组大小为 3209 M，大约 3 Gb。 计算基因组大小的程序是一个 python 脚本，其内容如下： import sysdef readGenome(filename): genome='' with open (filename,'r') as f: for line in f: if not line[0]=='&gt;': genome += line.rstrip() return genomegenome = readGenome(sys.argv[1])print len(genome), \"bp\"print len(genome)/ 1000000, \"Mbp\" 简单解析如下： import sys # 导入 sys 模块def readGenome(filename): # 定义一个名称为 readGenome 的函数 \"\"\" 该函数通过读取 FASTA 格式的文件， 然后将所有的序列拼接为一条长序列。 \"\"\" genome='' # 定义字符串型变量，用于存储序列 with open (filename,'r') as f: # 读取文件 for line in f: # 对文件的每一行进行遍历 if not line[0]=='&gt;': # 忽略 ”&gt;“ 开头的行，即只对序列进行操作 \"\"\" 为 genome = genome + line.rstrip() 的简写, 目的是将序列拼接起来；rstrip() 在这里的作用是 删除换行符”\\n“ \"\"\" genome += line.rstrip() return genome # 返回结果# sys.argv 为一个列表，sys.argv[0] 表示程序本身，# 在这里是”bin/genome_size.py“，sys.argv[1] 则# 表示第一个输入文件，依此类推。genome = readGenome(sys.argv[1])print len(genome), \"bp\" # len() 用于计算字符串的长度print len(genome)/ 1000000, \"Mbp\" GC 含量基因组的 GC 含量随物种的不同而改变，不同物种的往往有其独特的 GC 含量值。 但在计算 GC 含量之前，需要了解基因组的碱基组成情况，如拟南芥基因组的碱基组成情况为： $ python bin/genome_component.py TAIR10_chr_all.fasA 38223602C 21551439D 1G 21528650K 63M 84N 185738S 34R 47T 38177852W 144Y 96 人类基因组的碱基组成情况为： $ python bin/genome_component.py hg38.faa 463840423A 434444996C 295469343G 295683757g 330651380c 328257999N 159967178t 465881183n 3144T 435086702 A、T、G、C 和 N 大家应该都清楚，但其它字母又是什么意思呢？其实是 IUPAC code，具体看下表： IUPAC nucleotide code Base A Adenine C Cytosine G Guanine T (or U) Thymine (or Uracil) R A or G Y C or T S G or C W A or T K G or T M A or C B C or G or T D A or G or T H A or C or T V A or C or G N any base . or - gap 由于模糊碱基（ambiguous bases ）的存在会影响计算 GC 含量的准确性，所以在计算时往往忽略这些碱基： Count(G + C) / Count(A + T + G + C) 拟南芥基因组的 GC 含量为： $ python bin/genome_gc.py TAIR10_chr_all.fas0.360558525763 人类基因组的 GC 含量为： $ python bin/genome_gc.py hg38.fa0.409948515654 python 程序代码如下： import sysdef readGenome(filename): genome='' with open (filename,'r') as f: for line in f: if not line[0]=='&gt;': genome += line.rstrip() return genomegenome = readGenome(sys.argv[1])C = genome.count(\"C\") + genome.count(\"c\")G = genome.count(\"G\") + genome.count(\"g\")A = genome.count(\"A\") + genome.count(\"a\")T = genome.count(\"T\") + genome.count(\"t\")print (G + C) / float(A + C + G + T)","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"基因组大小和GC含量(二)","slug":"基因组大小和GC含量-二","date":"2017-05-13T13:15:49.000Z","updated":"2017-05-16T08:39:28.576Z","comments":true,"path":"blog/基因组大小和GC含量-二/","link":"","permalink":"https://hui-liu.github.io/blog/基因组大小和GC含量-二/","excerpt":"","text":"基因组染色体大小 拟南芥基因组各个染色体（或叶绿体基因组、线粒体基因组）大小 $ python bin/chr_size.py TAIR10_chr_all.faschloroplast 154478mitochondria 3669241 304276713 234598302 196982895 269755024 18585056 python程序代码如下： import sysdef parseFasta(filename): \"\"\" 这个函数的作用是解析 FASTA 格式的文件， 形成一个字典（即 perl 中的哈希），使得 一个序列的 id 对应一条序列。 \"\"\" fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasgenome = parseFasta(sys.argv[1])for chr, seq in genome.iteritems(): print chr, len(seq) 人类基因组各个染色体（或线粒体及其它片段）大小 $ python bin/chr_size.py hg38.fa &gt; hg38_chr_size.txt$ wc -l hg38_chr_size.txt326 hg38_chr_size.txt$ head hg38_chr_size.txtchr20_GL383577v2_alt 128386chr17_KI270861v1_alt 196688chr1_KI270713v1_random 40745chr9_KI270717v1_random 40062chrUn_KI270304v1 2165chr9_GL383542v1_alt 60032chr2_KI270773v1_alt 70887chrUn_GL000219v1 179198chr12_GL383551v1_alt 184319chr3_GL000221v1_random 155397 人类基因组中还有很多其它序列，现在只关注 22 条常染色体、性染色体以及线粒体基因组： $ python bin/hg38_chr_size.py hg38.fachr1 248956422chr2 242193529chr3 198295559chr4 190214555chr5 181538259chr6 170805979chr7 159345973chr8 145138636chr9 138394717chr10 133797422chr11 135086622chr12 133275309chr13 114364328chr14 107043718chr15 101991189chr16 90338345chr17 83257441chr18 80373285chr19 58617616chr20 64444167chr21 46709983chr22 50818468chrX 156040895chrY 57227415chrM 16569 实现代码如下： import sysdef parseFasta(filename): fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasgenome = parseFasta(sys.argv[1])hg38_chrs = list(range(1,23)) + ['X','Y','M'] # 注意 range(1,23) 表示 1 到 22，不包括 23hg38_chrs = [\"chr\" + str(i) for i in hg38_chrs] # 这里用到了列表综合表达式for chr in hg38_chrs: print chr, len(genome[chr]) 基因组染色体 GC 含量 拟南芥基因组各染色体（或叶绿体基因组、线粒体基因组）的 GC 含量 $ python bin/chr_gc.py TAIR10_chr_all.faschloroplast 0.421206839559mitochondria 0.4701555546021 0.4175333930763 0.4204985983362 0.4174206152065 0.4187043391524 0.419433848099 代码如下： import sysdef parseFasta(filename): fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasgenome = parseFasta(sys.argv[1])for chr, seq in genome.iteritems(): C = seq.count(\"C\") + seq.count(\"c\") G = seq.count(\"G\") + seq.count(\"g\") A = seq.count(\"A\") + seq.count(\"a\") T = seq.count(\"T\") + seq.count(\"t\") print chr, (G + C) / float(G + C + A + G) 人类基因组各个染色体（或线粒体及其它片段）GC 含量 $ python bin/hg38_chr_gc.py hg38.fachr1 0.455136117836chr2 0.446190349475chr3 0.442690716111chr4 0.433588116139chr5 0.441930413978chr6 0.442133594052chr7 0.449055751743chr8 0.445478935512chr9 0.452452573638chr10 0.454231966953chr11 0.454073025121chr12 0.449529596613chr13 0.435875506003chr14 0.450570263408chr15 0.457073952832chr16 0.472421408284chr17 0.476006762041chr18 0.444289464948chr19 0.490534826956chr20 0.468451893087chr21 0.450651750628chr22 0.485032452427chrX 0.44203229791chrY 0.445265639472chrM 0.416312659303 实现代码： import sysdef parseFasta(filename): fas = &#123;&#125; id = None with open(filename, 'r') as fh: for line in fh: if line[0] == '&gt;': header = line[1:].rstrip() id = header.split()[0] fas[id] = [] else: fas[id].append(line.rstrip()) for id, seq in fas.iteritems(): fas[id] = ''.join(seq) return fasgenome = parseFasta(sys.argv[1])hg38_chrs = list(range(1,23)) + ['X','Y','M']hg38_chrs = [\"chr\" + str(i) for i in hg38_chrs]for chr in hg38_chrs: seq = genome[chr] C = seq.count(\"C\") + seq.count(\"c\") G = seq.count(\"G\") + seq.count(\"g\") A = seq.count(\"A\") + seq.count(\"a\") T = seq.count(\"T\") + seq.count(\"t\") print chr, (G + C) / float(G + C + A + G) 基因组染色体 GC 含量可视化 人类基因组染色体 GC 含量可视化 $ python bin/hg38_chr_gc.py hg38.fa &gt; hg38_gc.txt$ python bin/barplot.py hg38_gc.txt barplot.py 的代码如下： import sysimport matplotlib.pyplot as pltfrom matplotlib.ticker import FormatStrFormattercounts = []with open(sys.argv[1], 'r') as f: for line in f: lsplit = line.split() counts.append(float(\"%.2f\" % float(lsplit[1])))xtick = list(range(1,23)) + ['X','Y','M']fig, ax = plt.subplots()plt.title('Chromosome GC content')ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f')) # y 轴的值为两位有效数字plt.ylim(0.30, 0.50)plt.xlabel(\"Chromosome\", fontsize = 12)plt.ylabel(\"gc content\", fontsize = 12)#plt.bar(range(25), counts)plt.xticks(range(25), xtick)plt.savefig(\"hg38_chr_gc.png\")","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"Linux文本处理三剑客之sed","slug":"Linux文本处理三剑客之sed","date":"2017-05-07T11:06:59.000Z","updated":"2017-05-07T14:19:46.504Z","comments":true,"path":"blog/Linux文本处理三剑客之sed/","link":"","permalink":"https://hui-liu.github.io/blog/Linux文本处理三剑客之sed/","excerpt":"","text":"sed 是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在称为“模式空间”（pattern space）的临时缓冲区中，接着用 sed 命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。 以下举例说明其用法。 替换$ head -n 5 exon.txt1 11868 12226 +1 12612 12720 +1 12974 13051 +1 13220 14500 +1 15004 15037 -$ head -n 5 exon.txt | sed &apos;s/\\t/,/&apos;1,11868 12226 +1,12612 12720 +1,12974 13051 +1,13220 14500 +1,15004 15037 - s 表示替换，将制表符 \\t 替换为 ,；默认只替换第一个匹配的制表符。若想匹配第二个，则： $ head -n 5 exon.txt | sed 's/\\t/,/2'1 11868,12226 +1 12612,12720 +1 12974,13051 +1 13220,14500 +1 15004,15037 - 全部替换： $ head -n 5 exon.txt | sed 's/\\t/,/g'1,11868,12226,+1,12612,12720,+1,12974,13051,+1,13220,14500,+1,15004,15037,- g 即为 global，意为全局替换。 删除删除特定字符$ grep &quot;&gt;&quot; est_Cadn.fasta | sed &apos;s/&gt;//&apos; | head -n 5est_Cadn_9607682est_Cadn_14202134est_Cadn_14202136est_Cadn_17975490est_Cadn_9607680 将 &gt; 替换为空，即删除。 删除空行sed &apos;/^$/d&apos; file 这里用到了正则，^$ 表示空行，放在 // 中表示在文件中匹配空行；d 为 delete，即删除；所以整体的意思就是将空行删除。 添加行$ head -n 5 exon.txt1 11868 12226 +1 12612 12720 +1 12974 13051 +1 13220 14500 +1 15004 15037 -$ head -n 5 exon.txt | sed &apos;1ichr\\tstart\\tend\\tstrand&apos;chr start end strand1 11868 12226 +1 12612 12720 +1 12974 13051 +1 13220 14500 +1 15004 15037 - 1i 表示在第一行之前插入一行（insertion），\\t 表制表符。 在第一行之后添加一行，则是： $ head -n 5 exon.txt | sed &apos;1achr\\tstart\\tend\\tstrand&apos;1 11868 12226 +chr start end strand1 12612 12720 +1 12974 13051 +1 13220 14500 +1 15004 15037 - 1a 即在第一行之后追加一行（append）。 显示指定行显示第二行$ zcat 1_SD_30_1.fq.gz | sed -n '2p'CAGCATCACATATTAGGCTTTATCCCTTTAAAGCAATATATTTTGAAATATCAATTATCATTTTCATTTATGGCCCGTAGGGCATTGCAGGGCACAACG -n 表示，不处理的行，不打印；p 即 print，2p 表示打印第二行。 显示指定区间内的行s$ zcat 1_SD_30_1.fq.gz | sed -n '4,8p'=DFFFFHHHHHJJJJJJJJJJJJJIJJJJJJJJJJJJJJIJJJJJJJJJJJJJIIJJJJJJJJJJJJJJJJJIFHJJHHFFFFDEEEEEDDDDDDDDDD@ERR569754.7 9L6V3M1:265:C06M9ACXX:3:1101:10900:1974 length=101TGCTGTTCATGGTGTTGTTGCTCTTGCTGTTGTTGTTGTTGCCCACGATGGGATCGCCGTTGATGGGGCCGTTAACGGGATTGCCATGAATCTTGGTGT+=DFFFEHHHHHJEGHHJIJJJJJJJIJJIIIIJJIIIIJJJJJJJIIIJJJJJJJJJJHHFFFEEEDDDDDDDBDCBDDBDDDCDDDDCDEDDDDDCDD 打印 4 到 8 行。 按一定规律显示指定的行$ zcat 1_SD_30_1.fq.gz | sed -n '1~4p' | head -n 5@ERR569754.4 9L6V3M1:265:C06M9ACXX:3:1101:6568:1985 length=101@ERR569754.7 9L6V3M1:265:C06M9ACXX:3:1101:10900:1974 length=101@ERR569754.18 9L6V3M1:265:C06M9ACXX:3:1101:1157:2083 length=101@ERR569754.19 9L6V3M1:265:C06M9ACXX:3:1101:1407:2070 length=101@ERR569754.20 9L6V3M1:265:C06M9ACXX:3:1101:1565:2062 length=101 打印第 1 行，然后跳过 4 行，打印第 5 行；如此循环到最后一行。在 fastq 中即为打印每条序列的 header。 1~4 和 p 之间可以加上替换操作。 $ zcat 1_SD_30_1.fq.gz | sed -n '1~4s/^@/&gt;/p' | head -n 5&gt;ERR569754.4 9L6V3M1:265:C06M9ACXX:3:1101:6568:1985 length=101&gt;ERR569754.7 9L6V3M1:265:C06M9ACXX:3:1101:10900:1974 length=101&gt;ERR569754.18 9L6V3M1:265:C06M9ACXX:3:1101:1157:2083 length=101&gt;ERR569754.19 9L6V3M1:265:C06M9ACXX:3:1101:1407:2070 length=101&gt;ERR569754.20 9L6V3M1:265:C06M9ACXX:3:1101:1565:2062 length=101 随便将序列打印出来： $ zcat 1_SD_30_1.fq.gz | sed -n '1~4s/^@/&gt;/p;2~4p' | head -n 5&gt;ERR569754.4 9L6V3M1:265:C06M9ACXX:3:1101:6568:1985 length=101CAGCATCACATATTAGGCTTTATCCCTTTAAAGCAATATATTTTGAAATATCAATTATCATTTTCATTTATGGCCCGTAGGGCATTGCAGGGCACAACG&gt;ERR569754.7 9L6V3M1:265:C06M9ACXX:3:1101:10900:1974 length=101TGCTGTTCATGGTGTTGTTGCTCTTGCTGTTGTTGTTGTTGCCCACGATGGGATCGCCGTTGATGGGGCCGTTAACGGGATTGCCATGAATCTTGGTGT&gt;ERR569754.18 9L6V3M1:265:C06M9ACXX:3:1101:1157:2083 length=101 这样即可将 fastq 转成 fasta 格式了。 删除空格即其后的内容： $ zcat 1_SD_30_1.fq.gz | sed -n '1~4s/^@/&gt;/p;2~4p' | sed 's/ .*//' | head -n 5&gt;ERR569754.4CAGCATCACATATTAGGCTTTATCCCTTTAAAGCAATATATTTTGAAATATCAATTATCATTTTCATTTATGGCCCGTAGGGCATTGCAGGGCACAACG&gt;ERR569754.7TGCTGTTCATGGTGTTGTTGCTCTTGCTGTTGTTGTTGTTGCCCACGATGGGATCGCCGTTGATGGGGCCGTTAACGGGATTGCCATGAATCTTGGTGT&gt;ERR569754.18 相信到这里，大家 linux 已经入门了，但由于本人及文章篇幅有限，难免遗漏一些知识点，故推荐给大家两个教程，以查漏补缺： https://www.tutorialspoint.com/unix/index.htm http://kodango.com/article-series","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"Linux文本处理三剑客之awk","slug":"Linux文本处理三剑客之awk","date":"2017-04-29T05:55:14.000Z","updated":"2017-04-30T06:23:22.288Z","comments":true,"path":"blog/Linux文本处理三剑客之awk/","link":"","permalink":"https://hui-liu.github.io/blog/Linux文本处理三剑客之awk/","excerpt":"","text":"awk 的名称源自其创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母，拥有强大的文本编辑能力。 awk 基本语法： awk &apos;BEGIN&#123;&#125; &#123;command&#125; END&#123;&#125;&apos; filename awk 是逐行处理文本内容的；BEGIN{} 是初识化代码块，在处理文件第一行内容之前，定义一些变量；{command} 为一些命令，对文件内容的每一行进行相应地处理； END{} 为结束代码块，在{command} 运行结束后执行。 BEGIN{} 和 END{} 都不是必须的，所以，往往可以这样写： awk &apos;&#123;command&#125;&apos; filename 数据前十行如下： awk 的输入被解析成多个记录（Record），默认的记录分隔符是 \\n，因此可以认为一行就是一个记录，记录的分隔符（当前行和下一行之间的分隔符）可以通过内置变量 RS （record separator）更改。而对于当前行，默认分隔符为空格（包括 tab）；通过分隔符，将一行数据分割成许多列，如打印文件的第一列或第二列： 或第一列和第二列： 不错，可能你已经猜到，$1 就是只第一列，相应的地，$2 表示第二列等。而 $0 则表示所有的列： liuhui@LIUHUI:shengxinbaike$ awk &apos;&#123;print $0&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Drought Controlgene1 708 45 112 795gene2 341 617 304 556gene3 756 500 67 217gene4 685 100 57 400 awk 用内置变量 NR 记录当前的行号： 所以，只打印第一行，可以这样做： $ awk &apos;NR==1&#123;print $0&#125;&apos; sampledata.txtgenename Cold Heat Drought Control 不想打印第一行，则这样： $ awk &apos;NR!=1&#123;print $0&#125;&apos; sampledata.txt | head -n 5gene1 708 45 112 795gene2 341 617 304 556gene3 756 500 67 217gene4 685 100 57 400gene5 233 526 308 658 awk 可以很方便地对不同列之间地值进行加减乘除等运算： # 加$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2+$5,$3+$5,$4+$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 1503 840 907gene2 897 1173 860gene3 973 717 284gene4 1085 500 457# 减$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2-$5,$3-$5,$4-$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 -87 -750 -683gene2 -215 61 -252gene3 539 283 -150gene4 285 -300 -343# 乘$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2*$5,$3*$5,$4*$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 562860 35775 89040gene2 189596 343052 169024gene3 164052 108500 14539gene4 274000 40000 22800# 除（在这里是 试验组 vs 对照组 的 fold change）$ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 0.890566 0.0566038 0.140881gene2 0.613309 1.10971 0.546763gene3 3.48387 2.30415 0.308756gene4 1.7125 0.25 0.1425 求 log （e 为底）也不在话下： $ awk &apos;NR==1&#123;print $1,$2,$3,$4&#125;NR!=1&#123;print $1,log($2/$5),log($3/$5),log($4/$5)&#125;&apos; sampledata.txt | head -n 5genename Cold Heat Droughtgene1 -0.115898 -2.87168 -1.95984gene2 -0.488886 0.104101 -0.603741gene3 1.24814 0.834711 -1.1752gene4 0.537954 -1.38629 -1.94841 awk 中支持以下数学函数（还有补充吗）： atan2(y,x)：余切； cos(x)：余弦； sin(x)：正弦； ​exp(x)：以自然对数 e 为底指数幂； log(x)：计算以 e​ 为底的对数值； sqrt(x)：开方； int(x)：将数值转换成整数； rand()：返回 0 到 1 的一个随机数值，不包含 1； srand([expr])：设置随机种子，一般与 rand 配合使用，如果参数为空，默认使用当前时间为种子。 对其中某一列求和，也很简单： # 对第二列求和$ awk &apos;NR!=1&#123;sum += $2&#125; END&#123;print sum&#125;&apos; sampledata.txt16962 END 的作用是前面的部分 NR!=1{sum += $2} 运行完毕后，才执行的。 通过 awk，还可以非常迅速的统计出文件内容的列数： $ awk &apos;&#123;print NF&#125;&apos; sampledata.txt | head -n 555555 NF （number of fields，域的数量，域就是列），是 awk 的内置标量；其默认按空格分割，记录每一行有多少列。 所以打印最后一列可以这样： $ awk &apos;&#123;print $NF&#125;&apos; sampledata.txt | head -n 5Control795556217400 awk 可以改变文件内容的分隔符： $ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;print $1,$2,$3,$4,$5&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400 OFS （output field separator），可以指定输出内容的分隔符。 注意： awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;print $0&#125;&apos; sampledata.txt 不会改变输出内容的分隔符。为什么呢？ 如果文件内容有 100 列，应该怎么处理呢？ 可以这样： # 方法 1$ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;NF=NF; print $0&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400# 方法 2liuhui@LIUHUI:shengxinbaike$ awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;$1=$1; print $0&#125;&apos; sampledata.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1;708;45;112;795gene2;341;617;304;556gene3;756;500;67;217gene4;685;100;57;400 原理是当对 $1、$2 等等以及列数 NF 的赋值时，$0 会用 OFS 进行重构（即 OFS 生效）。 可以将重新指定分隔符的内容重定向到一个新的文件里： awk &apos;BEGIN&#123;OFS=&quot;;&quot;&#125;&#123;$1=$1; print $0&#125;&apos; sampledata.txt &gt; sampledata_new.txt 看看这时直接打印第一列会怎样： 不是预期效果： 这时，就要指定输入分隔符的值了： $ awk &apos;BEGIN&#123;FS=&quot;;&quot;&#125;&#123;print $1&#125;&apos; sampledata_new.txt | head -n 5genenamegene1gene2gene3gene4 FS （field separator）的作用是指定输入内容的分隔符，与 OFS 相反。 注意，这样写是不行的： $ awk &apos;&#123;FS=&quot;;&quot;; print $1&#125;&apos; sampledata_new.txt | head -n 5genename;Cold;Heat;Drought;Controlgene1gene2gene3gene4 第一行不对。 这是因为 BEGIN 的作用是初始化，即在读入文件内容的第一行之前，就定义好了 FS 的值；如果不用 BEGIN，显然会出点小 bug。 还有一个办法： $ awk -F &quot;;&quot; &apos;&#123;print $1&#125;&apos; sampledata_new.txt | head -n 5genenamegene1gene2gene3gene4 通过参数 -F 指定分隔符。 小结一下 awk 一部分内置变量： 变量 $n 当前记录的第 n 个字段，字段间由 FS 分隔，如 $1、$2 等 $0 完整的输入记录，即当前的那一行 NR 当前记录数，即行数 FS 当前记录分隔符 （默认是空格） NF 当前记录中的列数 OFS 输出列的分隔符（默认值是一个空格） awk 还可以做一些筛选，如： # 第二列的值大于 1 的所对应的行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$2&gt;1&apos;| head -n 5gene3 3.48387 2.30415 0.308756gene4 1.7125 0.25 0.1425gene6 1.34752 0.553191 1.82979gene10 2.89815 6.76852 2.4537gene11 3 23.3 38.4# 第二列的值小于 1 的所对应的行liuhui@LIUHUI:shengxinbaike$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$2&lt;1&apos;| head -n 5gene1 0.890566 0.0566038 0.140881gene2 0.613309 1.10971 0.546763gene5 0.354103 0.799392 0.468085gene7 0.829047 1.15129 0.121029gene8 0.297456 0.64775 1.32485# 只提取 &quot;gene1&quot; 所在的行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;$1==&quot;gene1&quot;&apos;gene1 0.890566 0.0566038 0.140881# 第二行$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;NR==2&apos;gene2 0.613309 1.10971 0.546763# 奇数行（% 为求余数）$ awk &apos;NR!=1&#123;print $1,$2/$5,$3/$5,$4/$5&#125;&apos; sampledata.txt | awk &apos;NR%2 == 1&apos; | head -n 5gene1 0.890566 0.0566038 0.140881gene3 3.48387 2.30415 0.308756gene5 0.354103 0.799392 0.468085gene7 0.829047 1.15129 0.121029gene9 0.53317 1.00246 0.538084 由于 awk 内容较多，上面只是讲了一些基础及一些使用技巧，而文章篇幅有限，不便讲述太多的内容。本文的目的是作为一个引子，引导初学者入门，更多的内容，可以通过关键字在百度或谷歌搜索：“linux awk”，能查询到非常多的优秀的教程（这个方法对其它 linux 命令同样适用）。 也可以加入我们的 QQ 群：575383226 ，一起讨论各种生信问题。","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"Linux文本处理三剑客之grep","slug":"Linux文本处理三剑客之grep","date":"2017-04-22T04:48:47.000Z","updated":"2017-04-22T14:35:00.095Z","comments":true,"path":"blog/Linux文本处理三剑客之grep/","link":"","permalink":"https://hui-liu.github.io/blog/Linux文本处理三剑客之grep/","excerpt":"","text":"在 linux 中，有三种非常强大的文本处理命令，awk、sed 与 grep，号称 linux 三剑客。它们之间有不少相似点，但同样各具特色；三者均可以进行文本匹配，而 awk 和 sed 还可以进行文本编辑。 由于之前已经在生信入门系列之 linux 入门（三）：基础命令（中）文章中对 grep 命令进行了简单的讲解，所以今天主要给大家介绍一下三剑客中的第一剑：grep。 以拟南芥的 gff3 文件为例，进行讲解。 $ wget ftp://ftp.arabidopsis.org/home/tair/Genes/TAIR10_genome_release/TAIR10_gff3/TAIR10_GFF3_genes.gff 一般来说，对于一个文本文件，可以都要先用 less 简单看一下文件的内容： 利用之前学过的命令，并且可以很简单的统计出拟南芥有多少条染色体： $ cut -f 1 TAIR10_GFF3_genes.gff | uniqChr1Chr2Chr3Chr4Chr5ChrCChrM 查看各个染色体的基因数： $ grep &quot;^Chr1&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;9117$ grep &quot;^Chr2&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;6343$ grep &quot;^Chr3&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;7610$ grep &quot;^Chr4&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;5851$ grep &quot;^Chr5&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;8313$ grep &quot;ChrC&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;133$ grep &quot;^ChrM&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;146 其中， grep &quot;^Chr1&quot; TAIR10_GFF3_genes.gff 是将 TAIR10_GFF3_genes.gff 文件中所有含有以 Chr1 开头的行提取出来， 然后将这些信息通过管道传给 grep -c &quot;gene&quot;，统计其中含有 gene 的行数。 可以发现，上面的几个命令中，基本相同，只有“Chr”在变。所以可以通过一个简单的 for 循环，完成上面的操作。 for i in Chr1 Chr2 Chr3 Chr4 Chr5 ChrC ChrMdogrep &quot;^$i&quot; TAIR10_GFF3_genes.gff | grep -c &quot;gene&quot;done 如图所示： 其中的 $i 依次代表 Chr1、Chr2、Chr3、Chr4、Chr5、ChrC 和 ChrM。 下面继续讲解 grep 中的其他参数。 如下图所示，参数 -l 的作用是：如果文件中含有以 Chr1 开头的行（至少一行），则将文件名打印出来。我们之前已经知道，TAIR10_GFF3_genes.gff 文件中不含有以 chr1 开头的行（区分大小写），故没有结果。 试想，如果有成千上万个文件，我们想知道哪些文件含有某个特定的关键字（如上述的 Chr1），就可以结合 for 循环，将这些文件找出来。假如目录 test_dir 中含有几万个文件，则可以这样做： for i in $(ls test_dir)dogrep -l &quot;关键字&quot; $idone &gt; check_result.txt 把含有关键字的文件名存到 check_result.txt 中。这个 $(ls test_dir) 是什么意思呢？请看生信入门系列之——Shell 脚本编程（一）。 如果刚好相反，把不含某个关键字的文件名找出来，则用 -L 代替 -l。 也可以使用参数 -i 来忽略大小写： 在一些文件中，有时会有空白行，这时，用 grep 可以去掉： 其中的 ^$ 表示空白行，-v 则表示反向匹配，即将非空白行取出来。 如果你要问，^$ 为什么表示空白行，则可以理解：^ 表示匹配行首，$ 则表示匹配行尾，它们这样组合则表示行首和行为之间什么都没有，那就是空白行了。 其实，前面提到的 ^Chr1 和 ^$，都是正则表达式的用法（类似我们熟知的通配符 * 和 ?），sed、awk 和 grep 都支持正则表达式。 如果有文件 Chrs.txt，内容是： $ cat Chrs.txtchr1chr2chr3chr4chr5chr6chr7chr8chr9chr10 如果要匹配 chr1，按之前的做法： 显然，chr10 也含有 chr1，不是我们期望得到的，但可以这样做： \\&gt; 表示“词尾锚定”，即限定右边的边界。 如果想一次将 chr2 和 chr3 匹配出来，可以这样做： $ grep &quot;chr[23]&quot; Chrs.txtchr2chr3 [23] 表示 2 或 3 中的任意一个，当然可以不止两个： $ grep &quot;chr[23456]&quot; Chrs.txtchr2chr3chr4chr5chr6 但这样不是好办法，可以这样做： grep &quot;chr[2-6]&quot; Chrs.txtchr2chr3chr4chr5chr6 [2-6] 表示 2 到 6 中的任意一个。 相应地 [a-z] 表示小写字母 a 到小写字母 z 中地任意一个，等。 而 $ grep &quot;chr[^2-6]&quot; Chrs.txtchr1chr7chr8chr9chr10 则表示把不含有 chr2 到 chr6 关键字其他行取出来，其中的 ^ 表示 非，而不是表示匹配行首。这时和 grep -v 作用一样： $ grep -v &quot;chr[2-6]&quot; Chrs.txtchr1chr7chr8chr9chr10","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"外业-影集","slug":"外业-影集","date":"2017-04-21T14:43:58.000Z","updated":"2017-04-21T14:54:17.222Z","comments":true,"path":"blog/外业-影集/","link":"","permalink":"https://hui-liu.github.io/blog/外业-影集/","excerpt":"","text":"","categories":[{"name":"maolab","slug":"maolab","permalink":"https://hui-liu.github.io/categories/maolab/"}],"tags":[{"name":"照片","slug":"照片","permalink":"https://hui-liu.github.io/tags/照片/"}]},{"title":"从FASTA比对产生的氨基酸序列的alignment中删除frame shift","slug":"从FASTA比对产生的氨基酸序列的alignment中删除frame-shift","date":"2017-04-16T09:09:45.000Z","updated":"2017-04-16T09:15:19.084Z","comments":true,"path":"blog/从FASTA比对产生的氨基酸序列的alignment中删除frame-shift/","link":"","permalink":"https://hui-liu.github.io/blog/从FASTA比对产生的氨基酸序列的alignment中删除frame-shift/","excerpt":"","text":"用 FASTA 比对产生的protein-basedalignment中，会引入 frame shift，“/R” 表示在 R 对应的密码子中有一个碱基缺失，“\\R” 则表示在 R 对应的密码子中有一个碱基插入（可能插入或缺失两个或以上，不确定；以下程序认为只有一个插入或缺失一个碱基）。这些 indel 的存在使得序列无法按 codon alignment 对齐。以下脚本从 FASTA 比对产生的含有 frame shift 的 alignment 中删除frame shift ，得到无 frame shift 的 alignment。 import sysfrom Bio import SeqIOfrom Bio.Seq import Seqdef posFind(pssdSeq, pareSeq, key): # input pssd seq and it's parental seq and # frame shift symbol, return the real position # of pssd seq and and it's parental seq, and the # position of frame shift in aligned seq count = 0 pssdSeq_raw = pssdSeq # make a copy pssd_frameshift = [] # real position of pssd seq pssdSeq_raw_frameshift = [] # the raw aligned position pare_to_frameshift = [] pssdSeq = pssdSeq.replace(\"-\", \"\") control = True while control: if count &gt; 0: pos1 = pssdSeq.find(key, pos1 + 1) pos2 = pssdSeq_raw.find(key, pos2 + 1) if pos1 == -1: control = False else: pssd_frameshift.append(pos1) pssdSeq_raw_frameshift.append(pos2) count += 1 else: pos1 = pssdSeq.find(key) pos2 = pssdSeq_raw.find(key) pssd_frameshift.append(pos1) pssdSeq_raw_frameshift.append(pos2) count += 1 temp = [] # alignment frame shift pos (i.e. \"/R\" position) aligned_frameshif_pos = [[k, k+1] for k in pssdSeq_raw_frameshift] for pos in pssdSeq_raw_frameshift: temp.append(pos+1) for s in temp: num = 0 for index, i in enumerate(pareSeq): if i == \"-\": num += 1 if index == (s-1): pare_to_frameshift.append(s - num) pare_to_frameshift = [t+1 for t in pare_to_frameshift[:]] pare_to_frameshift_cds = [] for j in pare_to_frameshift: pare_to_frameshift_cds.append([j*3-2,j*3]) pssd_frameshift = [n-m for m, n in enumerate(pssd_frameshift[:])] pare_to_frameshift_cds = [[w[0] - 1, w[1] - 1] for w in pare_to_frameshift_cds[:]] return pssd_frameshift, pare_to_frameshift_cds, aligned_frameshif_posdef fshiftCoor(frameshiftPos, posIns, posDel): # return the coordinate of frameshift in seq # 将 frameshift 对应的序列的坐标找出来， frameshiftpos_1based = [i + 1 for i in frameshiftPos] # 1-based posIns_1based = [i + 1 for i in posIns] posDel_1based = [i + 1 for i in posDel] if frameshiftPos == posIns: # only insertion \"\\\" frameshift_coor = [] for index1, pos1 in enumerate(frameshiftpos_1based): temp_lis = [pos1 * 3 - 2 + index1] + [pos1 * 3 + 1 + index1] frameshift_coor.append(temp_lis) elif frameshiftPos == posDel: # only deletion \"/\" frameshift_coor = [] for index2, pos2 in enumerate(frameshiftpos_1based): temp_lis = [pos2 * 3 - 2 - index2] + [pos2 * 3 - 1 - index2] frameshift_coor.append(temp_lis) else: if posIns[0] &gt; posDel[-1]: # Insertion in the right of the deletion xxx xx xxx xx xxxx frameshift_coor = [] for index3, pos3 in enumerate(posDel_1based): temp_lis = [pos3 * 3 -2 - index3] + [pos3 * 3 - 1 - index3] frameshift_coor.append(temp_lis) for index4, pos4 in enumerate(posIns_1based): temp_lis = [pos4 * 3 -2 + index4 - index3] + [pos4 * 3 + 1 + index4 - index3] frameshift_coor.append(temp_lis) elif posIns[-1] &lt; posDel[0]: # Insertion in the left of the deletion xxx xxxx xxx xxxx xx frameshift_coor = [] for index5, pos5 in enumerate(posIns_1based): temp_lis = [pos5 * 3 -2 + index5] + [pos5 * 3 + 1 + index5] frameshift_coor.append(temp_lis) for index6, pos6 in enumerate(posDel_1based): temp_lis = [pos6 * 3 -2 - index6 + index5] + [pos6 * 3 - 1 - index6 + index5] frameshift_coor.append(temp_lis) else: frameshift_coor = [] # xxx xxxx xxx xx xxx xxxx xxx xx xxx num_del = 0 num_ins = 0 for pos7 in frameshiftpos_1based: if pos7 in posIns_1based: temp_lis = [pos7 * 3 - 2 + num_ins - num_del] + [pos7 * 3 + 1 + num_ins - num_del] frameshift_coor.append(temp_lis) num_ins += 1 else: temp_lis = [pos7 * 3 - 2 - num_del + num_ins] + [pos7 * 3 + 1 - num_del + num_ins] frameshift_coor.append(temp_lis) num_del +=1 frameshift_coor = [[c[0]-1, c[1]-1]for c in frameshift_coor[:]] return frameshift_coordef cleanFrameshift(frameshiftCoor, chrCoor): # 根据 frameshift 在 序列上的坐标，找出不含frameshift # 的坐标 clean_frameshift = [] for index, j in enumerate(frameshiftCoor): if len(frameshiftCoor) == 1 and index == 0: clean_frameshift.append([int(chrCoor[0]) - 1, j[0]]) clean_frameshift.append([j[1] + 1, int(chrCoor[1]) + 1]) elif len(frameshiftCoor) &gt; 1 and index == 0: clean_frameshift.append([int(chrCoor[0]) - 1, j[0]]) elif len(frameshiftCoor) &gt; 1 and index == (len(frameshiftCoor) -1): clean_frameshift.append([frameshiftCoor[index-1][1] + 1, j[0]]) clean_frameshift.append([j[1] + 1, int(chrCoor[1]) + 1]) else: clean_frameshift.append([frameshiftCoor[index-1][1] + 1, j[0]]) # clean_frameshift = [[s[0]-1, s[1]-1] for s in clean_frameshift[:]] return clean_frameshift USAGE = \"\\nusage: python %s genPgeneResult_PSSD.txt ptaeda.v1.01_masked.trimmed.fa pta_cds.fas genFullAln_PSSD_frameshift.txt PSSD_frameshift_removed.txt PSSD_frameshift.fa\\n\" % sys.argv[0]if len(sys.argv) != 7: print USAGE sys.exit()# read the \"genPgeneResult.txt\" file and store the # coordinate informations of \"pssd\" into a dictionary \"COOR_DICT\" COOR_DICT = &#123;&#125;with open(sys.argv[1], 'r') as f1: for line1 in f1: lsplit1 = line1.split() if \"PSSD1\" == lsplit1[17] or \"PSSD2\" == lsplit1[17]: COOR_DICT[lsplit1[5]] = lsplit1[0:5] + lsplit1[6:8]# store the fasta file into memory using dictgenome_infa = SeqIO.parse(open(sys.argv[2]), 'fasta')cds_infa = SeqIO.parse(open(sys.argv[3]), 'fasta')#genome_DICT = &#123;&#125;for rec1 in genome_infa: genome_DICT[rec1.id] = str(rec1.seq)genome_infa.close()#cds_DICT = &#123;&#125;for rec2 in cds_infa: cds_DICT[rec2.id] = str(rec2.seq)cds_infa.close()#ALN_DICT = &#123;&#125;with open(sys.argv[4], 'r') as f2: for line2 in f2: if \"&gt;\" == line2[0]: aln_name = line2.split()[0][1:] ALN_DICT[aln_name] = [] else: ALN_DICT[aln_name].append(line2.rstrip())# extract the pair of partenal gene and pseudogeneOUT1 = open(sys.argv[5], 'w')OUT2 = open(sys.argv[6], 'w')for id, lis in COOR_DICT.items(): # scaffold249595===PITA_000071929-RA===1 ['scaffold249595', '10578', '10691', '+', 'PITA_000071929-RA', '131', '168'] if lis[4] not in cds_DICT: continue if id not in ALN_DICT: continue g_seq = genome_DICT[lis[0]][int(lis[1])-1: int(lis[2])] c_seq = cds_DICT[lis[4]][int(lis[5])*3-3: int(lis[6])*3] # reverse complement of sequences if seqStart larger than seqEnd if \"-\" == lis[3]: g_seq = str(Seq(g_seq).reverse_complement()) if int(lis[5]) &gt; int(lis[6]): c_seq = str(Seq(c_seq).reverse_complement())# chr_coor = lis[1:3] chr_coor = [1, len(g_seq)] # 1-based cds_coor = [1, len(c_seq)] align_range = [1, len(ALN_DICT[id][0])] pos_del = [] pos_ins = [] if \"/\" in ALN_DICT[id][1] and \"\\\\\" in ALN_DICT[id][1]: pos_del, parent_to_del, frameshif_pos_in_alignment_del = posFind(ALN_DICT[id][1], ALN_DICT[id][0], \"/\") pos_ins, parent_to_ins, frameshif_pos_in_alignment_ins = posFind(ALN_DICT[id][1], ALN_DICT[id][0], \"\\\\\") pos_lis = pos_del + pos_ins frameshift_pos = sorted(pos_lis) parent_to_indel = parent_to_del + parent_to_ins parent_to_indel = sorted(parent_to_indel) frameshif_pos_in_alignment = frameshif_pos_in_alignment_del + frameshif_pos_in_alignment_ins frameshif_pos_in_alignment = sorted(frameshif_pos_in_alignment)# print id, frameshift_pos elif \"/\" in ALN_DICT[id][1]: pos_del, parent_to_indel, frameshif_pos_in_alignment = posFind(ALN_DICT[id][1], ALN_DICT[id][0], \"/\") frameshift_pos = pos_del# print id, frameshift_pos elif \"\\\\\" in ALN_DICT[id][1]: pos_ins, parent_to_indel, frameshif_pos_in_alignment = posFind(ALN_DICT[id][1], ALN_DICT[id][0], \"\\\\\") frameshift_pos = pos_ins# print id, frameshift_pos # write out the alignment removed frame shift clean_frameshift_align = cleanFrameshift(frameshif_pos_in_alignment, align_range) g_seq_align = '' c_seq_align = '' for reg in clean_frameshift_align: g_seq_align += ALN_DICT[id][1][reg[0]: reg[1]] c_seq_align += ALN_DICT[id][0][reg[0]: reg[1]] OUT1.write(\"&gt;\" + id + \"\\n\" + c_seq_align + \"\\n\" + g_seq_align + \"\\n\") # frameshift_coor frameshift_coor = fshiftCoor(frameshift_pos, pos_ins, pos_del) # chr_coor removed frame shift site(s) # [[35911, 36065], [36067, 36196], [36198, 36240], [36242, 36280]] clean_frameshift = cleanFrameshift(frameshift_coor, chr_coor) clean_frameshift_to_cds = cleanFrameshift(parent_to_indel, cds_coor) # extract seq g_seq_clean = '' c_seq_clean = '' for frag1 in clean_frameshift: if len(frag1) == 1: g_seq_clean += g_seq[frag1[0]: frag1[1] + 1] else: g_seq_clean += g_seq[frag1[0]: frag1[1]] for frag2 in clean_frameshift_to_cds: if len(frag2) == 1: c_seq_clean += c_seq[frag2[0]: frag2[1] + 1] else: c_seq_clean += c_seq[frag2[0]: frag2[1]] OUT2.write(\"&gt;\" + id + \"\\n\" + c_seq_clean + \"\\n\" + g_seq_clean + \"\\n\")OUT1.close()OUT2.close()","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"R 抓取 plantrgdb 信息","slug":"R-抓取-plantrgdb-信息","date":"2017-04-14T13:51:21.000Z","updated":"2017-04-14T15:01:51.930Z","comments":true,"path":"blog/R-抓取-plantrgdb-信息/","link":"","permalink":"https://hui-liu.github.io/blog/R-抓取-plantrgdb-信息/","excerpt":"","text":"尚未理解 htmlTreeParse 和 xpathSApply 的用法，有机会再研究。 # 载入包library(XML)species &lt;- \"http://aegilops.wheat.ucdavis.edu/plantrgdb/download.php\"# 解析网页doc &lt;- htmlTreeParse(species, useInternal=TRUE, encoding=\"UTF-8\")# 抓取有用信息species &lt;- xpathSApply(doc, \"//a\", xmlGetAttr, 'href')# 物种的数量sp_num &lt;- length(species[grep(\"download\",species)][-1])# 对所有的物种进行循环for (sp in 1:sp_num)&#123; # 获取物种名 sp_name &lt;- strsplit(species[grep(\"download\",species)][-1], \"/\")[[sp]][2] print(sp_name) # 需要的信息有 9 列，先创建一个 9 列的空数组 sp_array &lt;- array(NA, c(0, 9)) # 为空数组列名重命名 colnames(sp_array) &lt;- c(\"Species\", \"Coordinates\", \"Strand\", \"Lost intron\", \"Parent coverage\", \"Parent identity\", \"Ka\", \"Ks\", \"Ka/Ks\") sp_url &lt;- paste(\"http://aegilops.wheat.ucdavis.edu/plantrgdb/browse_result.php?type=total&amp;species=\", sp_name, sep=\"\") # 对每一物种的网页进行解析 sp_doc &lt;- htmlTreeParse(sp_url, useInternal=TRUE, encoding=\"UTF-8\") temp &lt;- xpathSApply(sp_doc, \"//a\", xmlGetAttr, 'href') # 获取 retrocopies 数量 retro_num &lt;- length(grep(\"=\",temp)) # 对每一个 retrocopy 依次进行解析，并将结果写进 sp_array 这个数组里 for (i in 1:retro_num)&#123; retro_id &lt;- strsplit(temp[grep(\"=\",temp)],\"=\")[[i]][2] print(retro_id) retro_id_url &lt;- paste(\"http://aegilops.wheat.ucdavis.edu/plantrgdb/retrocopy_info.php?retrocopyID=\", retro_id, sep = \"\") sp_array &lt;- rbind(sp_array, t(readHTMLTable(retro_id_url, which = 1)[retro_id])) &#125; # 将 sp_array 中的信息写出到 csv 文件中 write.csv(sp_array, paste(sp_name, \".csv\", sep = \"\"))&#125;","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"R","slug":"R","permalink":"https://hui-liu.github.io/tags/R/"}]},{"title":"生信入门系列之-linux-入门（四）：基础命令（下）","slug":"生信入门系列之-linux-入门（四）：基础命令（下）","date":"2017-04-12T13:49:42.000Z","updated":"2017-04-13T06:31:54.200Z","comments":true,"path":"blog/生信入门系列之-linux-入门（四）：基础命令（下）/","link":"","permalink":"https://hui-liu.github.io/blog/生信入门系列之-linux-入门（四）：基础命令（下）/","excerpt":"","text":"在实际的 linux 服务器中运行软件时，经常会为一些长时间运行的任务而头疼。当关掉连接终端或者其它原因，如网络不稳定等，运行中的程序也会自动被中断。那么怎么去解决上述问题呢？可以用命令 nohup，像这样运行: nohup script.sh &amp; 其中 script.sh 是一个shell 脚本，当然也可以替换成其它的软件或命令。如下图所示： 其中的“13632”是PID，PID 是进程的代号，每个进程有唯一的 PID 编号。每个命令或程序运行后都会产生一个 PID。 可以用命令 ps 或 top 命令查看某个程序的 PID： ps 查看： top 查看： top 命令来实时监测系统正在运行什么程序以及系统现在的存储内存消耗等，其中有 PID 信息。 如果想中断某个程序，可以用 kill 命令： kill PID 有时需要在 Linux 下进行远程拷贝文件的命令，可以用命令 scp（本地有 linux 终端）： scp [参数] [原路径] [目标路径] 如果拷贝文件： # 将文件从本电脑上传到远程服务器 data 目录里# xxx.xxx.xxx.xxx 代表 ip 地址scp file1 liuhui@xxx.xxx.xxx.xxx:/home/liuhui/data# 将文件从远程服务器拷贝到本地电脑，并保存到 sample_data 目录里scp liuhui@xxx.xxx.xxx.xxx:/home/liuhui/data/file1 /home/liuhui/sample_data /home/liuhui/data/file1 的路径如果很长，可以用 readlink 命令获得（在这里不是必须的，但这个命令在 shell 脚本中用比较方便）： readlink -f filename ip 地址可以用 ifconfig 命令查看： 如果要拷贝目录， 则加一个参数 -r，和命令 cp 类似。 有时需要从网上下载软件，可以用 wget 命令（在 windows 也有同名软件）： wget url “url” 是文件的下载地址，鼠标右键可以获取： 运行 wget 命令： 如果文件很大，可以尝试用命令 axel 多线程下载（有些网站上的文件不支持这个命令，这是就用 wget）： 下载速度明显提升很多，因为我用参数 -n 指定适用 8 个线程（普通笔记本 4 个线程）。 在 shell 脚本（以后会有一个系列讲 shell 脚本）中经常会用到两个命令，basename 和 dirname。下面举例说明。 假如我有这个信息： /home/shengwu004/example/longReads.fa 通过 basename，可以得到： [shengwu004@fatnode example]$ basename /home/shengwu004/example/longReads.falongReads.fa 也可以去掉后缀： basename /home/shengwu004/example/longReads.fa .falongReads dirname 则相反： [shengwu004@fatnode example]$ dirname /home/shengwu004/example/longReads.fa/home/shengwu004/example 服务器的磁盘大小和使用情况是我们比较关心的，可以用 df 查看： 命令 du 可以查看某个目录中所有文件的大小，： 如果想了解某个命令的所有参数，可以用 man 查看，如： man du 按上下键翻页，按 q 退出。 用命令 history 可以查看所有用过命令： history 命令及简单描述 命令 nohup script.sh &amp; 后台运行命令或程序 top 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 ps 列出当前进程的快照，就是执行ps命令的那个时刻的那些进程 kill 根据PID，强制终止后台运行的命令或程序 scp 远程拷贝文件或目录 ifconfig 查看和配置网络设备 wget 下载文件 axel 多线程加速下载文件 readlink 获取文件的绝对路径和文件名的组合 basename 去掉路径信息 dirname 获得路径信息 df 查看磁盘空间占用情况 du 显示每个文件和目录的磁盘使用空间 man 查看命令帮助文档 history 查看历史命令","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"生信入门系列之 linux 入门（三）：基础命令（中）","slug":"生信入门系列之-linux-入门（三）：基础命令（中）","date":"2017-04-08T02:40:40.000Z","updated":"2017-05-23T12:38:31.927Z","comments":true,"path":"blog/生信入门系列之-linux-入门（三）：基础命令（中）/","link":"","permalink":"https://hui-liu.github.io/blog/生信入门系列之-linux-入门（三）：基础命令（中）/","excerpt":"","text":"这次接着讲 linux 基础命令。上次讲到了一些文件及目录操作相关的命令，这次接着讲。 首先要讲的一个命令是 ln，它的功能是为某一个文件建立一个同步的链接。当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的目录下用 ln 命令链接它就可以，这样就不会重复地占用磁盘空间了。它又分为软链接和硬链接。 注意：由于链接的同步性，当链接或源文件被修改时，其它的链接和源文件也发生相应的改动。不希望这样，就用命令 cp 代替，当然，代价是占用磁盘空间。 假如 sample_data 目录下有两个子目录 test 和 transcriptome （还记得怎么判断它们时文件还是目录吗？）： liuhui@ginkgo:~/sample_data$ ls -lhtotal 0drwxrwxrwx 2 root root 0 Apr 8 17:34 testdrwxrwxrwx 2 root root 0 Apr 8 17:34 transcriptome 也可以指定查看指定目录的属性： liuhui@ginkgo:~/sample_data$ ls -lh test/total 0liuhui@ginkgo:~/sample_data$ ls -lh transcriptome/*total 208K-rwxrwxrwx 1 root root 206K Apr 8 17:37 Trinity.fasta 也可以这样看，（* 是通配符，代表任意字符，即 0 到 多个）： liuhui@ginkgo:~/sample_data$ ls -lh *test:total 0transcriptome:total 208K-rwxrwxrwx 1 root root 206K Apr 8 17:37 Trinity.fasta 这样就知道每个目录下有什么文件或子目录了。 回到正题，首先讲软链接，软链接有如下属性： 1.软链接，以路径的形式存在。类似于 windows 中的快捷方式；所以，当移动源文件的位置或修改源文件名称时，软链接会失效。 2.软链接可以对一个不存在的文件名进行链接； 3.软链接可以对目录进行链接； 4.软链接可以 跨文件系统 ，硬链接不可以； 5.不增加源文件的链接数。 用法如下： ln -s file1 file2 其中 file1 是源文件，file2 是软链接文件。如图： 一般给源文件加上绝对路径，或者相对路径，但要类似这样： 因为软链接文件本质上就是一个指向源文件的路径；无论怎样操作，都要使得在软链接文件位置，能通过这个路径访问到源文件。 和软链接不同，硬链接有如下属性：1.硬链接，与源文件名称互为别名，不占用实际空间，直接指向文件在磁盘上的物理地址；所以无论怎样移动源文件或修改其名称，硬链接都不会失效。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 4.增加源文件的链接数。 用法如下： ln file1 file2 之前提到过文件或目录的权限，但没有细讲，这里把它讲清楚： 在上图文件或目录权限那一列信息中，权限分为三组，分别是：所有者权限、所属组权限以及其它用户的权限。其中的 r、w 以及x 分别代表文件的“读权限”，“写权限“以及”执行权限“，”-“指没有相应的权限。更详细的信息，请看以下表格： 代表字符 权限 数字 对文件的含义 对目录的含义 r 读权限 4 可以查看文件内容 可以查看目录中的内容 w 写权限 2 可以修改文件内容 可以在目录中创建、删除文件或目录 x 执行权限 1 可以执行文件 可以进入目录 用数字代表相应的字符，可方便的用命令行修改某个文件或目录的权限（三个数字的组合在一起，相加的和是唯一的）。要修改权限，需要用到命令 chmod，用法如下： chmod [mode] file 如： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rw-rw-r-- 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ chmod +x Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rwxrwxr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta 也可以通过数字： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-rwxrwxr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ chmod 555 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 2 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta 有时，我们需要找一个文件在哪，这时可以用命令 find： find [path...] [expression] 如： liuhui@ginkgo:~/sample_data$ find . -name Trinity.fasta./transcriptome/Trinity.fasta . 代表当前目录，即在当前目录这个路径下，搜索名为”Trinity.fasta“的文件。当不太记得文件名时，可以用通配符 *: liuhui@ginkgo:~/sample_data$ find . -name *fasta./test/Trinity_hard.fasta./test/Trinity_soft_1.fasta./test/Trinity_soft.fasta./transcriptome/Trinity.fasta 当要搜索的文件在环境变量里时，可以用命令 which 来搜索： which filename 如： liuhui@ginkgo:~/sample_data$ which chmod/bin/chmodliuhui@ginkgo:~/sample_data$ which find/usr/bin/findliuhui@ginkgo:~/sample_data$ which ln/bin/lnliuhui@ginkgo:~/sample_data$ which ls/bin/lsliuhui@ginkgo:~/sample_data$ which vcftools/home/liuhui/bin/vcftools_0.1.13/bin/vcftoolsliuhui@ginkgo:~/sample_data$ which samtools/home/liuhui/bin/samtools/samtools-0.1.19/samtools 注意：如有有童鞋不知道环境变量是什么的话，请先看本系列第一篇文章，“初识 linux 系统”。 如果我们想在文件中查找特定的内容时，可以用命令 grep 来实现： grep [参数] file 我们知道，fasta 文件的序列号以 &gt; 开头（不是重定向操作符），如果想讲序列号提取出来，可以这样： grep &quot;&gt;&quot; Trinity.fasta 显然，直接将搜索到的序列号打印到屏幕上不是一个明智的做法，所以，应该这样做： 即是将输出的结果重定向到一个文件里，然后可以这样查看： less -S Trinity.fasta 如果只想查看一下，可以这样做： grep &quot;&gt;&quot; Trinity.fasta | less -S 上述命令用到了一个在 linux 中很重要的操作：管道，即 |。它的作用是把一个命令的输出直接连接到另一个命令的输入。管道在 linux 中是重中之重。可以这样想象：前一个命令对文件操作所产生的信息流，从管道的前端流入，然后从管道的后端流出，这时后一个命令会接住这些信息流，并可以对这些信息流进行操作。 以上述命令为例，grep &quot;&gt;&quot; Trinity.fasta 中，grep 提取 Trinity.fasta 文件中含有 “&gt;” 的行，则所有包含有 “&gt;” 的行所组成的集合就会形成一股信息流，如果没有其它操作，这股信息流就会输出到屏幕上；当然了，也可以讲这股信息流保存到一个文件里，即重定向；还可以将其”灌输到“一个管道里 |，似的这股信息流在管道里流通，知道有另个以命令来”接收“这股信息，并做相应的处理，如这里是用 less -S 来进行不换行地分页浏览这股信息流。 也可以查看前几行： grep &quot;&gt;&quot; Trinity.fasta | head 在这里，我们指关心序列号，气后地那一串数字往往不是我们关心的，那有什么办法能将其剔除掉吗？看： liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | head&gt;TRINITY_DN63_c0_g1_i1&gt;TRINITY_DN66_c0_g1_i1&gt;TRINITY_DN66_c0_g2_i1&gt;TRINITY_DN66_c0_g2_i2&gt;TRINITY_DN68_c0_g1_i1&gt;TRINITY_DN68_c0_g1_i2&gt;TRINITY_DN68_c0_g2_i1&gt;TRINITY_DN68_c0_g3_i1&gt;TRINITY_DN68_c0_g4_i1&gt;TRINITY_DN42_c0_g1_i1 这里用到了命令 cut，将多余的部分 ”cut“ 掉了；怎么实现的，首先，序列号及其后面的那些数字串间的间隔，是有空格来分割的（即分隔符是空格）。cut 命令通过参数 -d 指定分隔符，即空格，用引号引住；然后再加一个参数 -f，指定前面由空格作为分隔符，所产生的很多列中的第一列（术语往往是说”域“）。如果这样， liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1,2 | head&gt;TRINITY_DN63_c0_g1_i1 len=484&gt;TRINITY_DN66_c0_g1_i1 len=709&gt;TRINITY_DN66_c0_g2_i1 len=316&gt;TRINITY_DN66_c0_g2_i2 len=292&gt;TRINITY_DN68_c0_g1_i1 len=7194&gt;TRINITY_DN68_c0_g1_i2 len=7076&gt;TRINITY_DN68_c0_g2_i1 len=520&gt;TRINITY_DN68_c0_g3_i1 len=508&gt;TRINITY_DN68_c0_g4_i1 len=542&gt;TRINITY_DN42_c0_g1_i1 len=280liuhui@ginkgo:~/sample_data/transcriptome$ grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1-2 | head&gt;TRINITY_DN63_c0_g1_i1 len=484&gt;TRINITY_DN66_c0_g1_i1 len=709&gt;TRINITY_DN66_c0_g2_i1 len=316&gt;TRINITY_DN66_c0_g2_i2 len=292&gt;TRINITY_DN68_c0_g1_i1 len=7194&gt;TRINITY_DN68_c0_g1_i2 len=7076&gt;TRINITY_DN68_c0_g2_i1 len=520&gt;TRINITY_DN68_c0_g3_i1 len=508&gt;TRINITY_DN68_c0_g4_i1 len=542&gt;TRINITY_DN42_c0_g1_i1 len=280 则取出第一和第二列，这里用逗号或横杠连接 ”1“ 和 ”2“，其中横杠用于链接连续的列，而逗号都行： # 横杠cut -f 1-10# 逗号cut -f 1,2,3,4,5,6,7,8,9,10# 当然逗号也有优势cut -f 1,3,5,8,10# 逗号与横杠混用cut -f 1,3,5,8-10# 这样不行哈cut -f 1-3-5-8-10 可能由同学想问，grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | head 的输出结果还有 ”&gt;” 啊，怎么去除呢？其实命令你学过啦，就是 cut： 这时，你这样操作： grep &quot;&gt;&quot; Trinity.fasta | cut -d &quot; &quot; -f 1 | cut -d &quot;&gt;&quot; -f 2 &gt; Trinity_ID 就得到你想要地序列号了。 对了，我想数一下”Trinity.fasta“ 文件中有几条序列，怎么做。这样做： liuhui@ginkgo:~/sample_data/transcriptome$ grep -c &quot;&gt;&quot; Trinity.fasta107 我只有序列号的那个文件呢？可以用命令 wc： wc -l file liuhui@ginkgo:~/sample_data/transcriptome$ wc -l Trinity_ID107 Trinity_ID 接着讲其它命令的用法。假如我有一个文件，其内容如下： liuhui@ginkgo:~/sample_data/test2$ cat fileaa 1ac 3ab 2ba 2ad 4ba 2ca 9da 10 我想按第一列排序，可以这样做： liuhui@ginkgo:~/sample_data/test2$ sort fileaa 1ab 2ac 3ad 4ba 2ba 2ca 9da 10 我想按第二列排序，那可以学学 cut，指定按第几列排序嘛，这样？ liuhui@ginkgo:~/sample_data/test2$ sort -k 2 fileaa 1da 10ab 2ba 2ba 2ac 3ad 4ca 9 实际上没有错，只是不是我们想要的，所以，得加个参数： liuhui@ginkgo:~/sample_data/test2$ sort -k 2 -n fileaa 1ab 2ba 2ba 2ac 3ad 4ca 9da 10 这里 -n 指按数值大小排列。 也可以按第二列逆序： liuhui@ginkgo:~/sample_data/test2$ sort -k 2 -n -r fileda 10ca 9ad 4ac 3ba 2ba 2ab 2aa 1 很明显，在上述文件中有一个重复项，可以这样删除： sort -u file sort 命令可以和 uniq 命令结合起来用， 去重（等同于 sort -u file）： 指显示重复行： 统计每行出现的次数： 如果只关心第二列的重复情况，可以这样： 接下来要讲的内容是文件的压缩或解压。但需要注意的是，不能对有硬链接的源文件或硬链接文件进行压缩或解压： 压缩命令可以用 gzip: gzip file 直接对文件进行压缩： liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ gzip Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 44K-r-xr-xr-x 1 liuhui liuhui 43K Apr 8 17:05 Trinity.fasta.gz 也可以加上参数 -c，这样可以保存原来的文件了。 liuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 208K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fastaliuhui@ginkgo:~/sample_data/transcriptome$ gzip -c Trinity.fasta &gt; Trinity.fasta.gzliuhui@ginkgo:~/sample_data/transcriptome$ ls -lhtotal 252K-r-xr-xr-x 1 liuhui liuhui 206K Apr 8 17:05 Trinity.fasta-rw-rw-r-- 1 liuhui liuhui 43K Apr 9 10:13 Trinity.fasta.gz 解压命令用 gunzip： gunzip file.gz 如图所示： 参数 -c 对 gunzip 同样适用： 可以这样查看压缩文件： zcat Trinity.fasta.gz | less -S 还有其它的压缩或解压命令，如 zip/unzip，bzip2/bunzip2 等，限于篇幅，在这里就不一一讲述了。 最后再讲一个命令，tar。这个命令的功能是将一个目录打包： -c 表示压缩，-v 表示显示压缩过程，-f 表示指定压缩文件，-z 表示用 gzip 压缩文件。 其逆向操作，只需改变一个参数，-z 变为 -x，后者表示从压缩的文件中提取文件： 命令及简单描述 命令 ln 创建软链接或硬链接 chmod 修改文件权限 find 查找文件 which 查找再环境变量中的文件 grep 利用关键字符进行文本搜索 cut 提取文件指定的列 wc 统计文件行数 sort 文件内容排序 uniq 对文本内容进行去重计数 gzip/gunzip 压缩或解压 zcat 查看 gzip 压缩的文件 tar 压缩或解压目录","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"生信入门系列之 linux 入门（一）：基础命令（上）","slug":"生信入门系列之-linux-入门（一）：基础命令（上）","date":"2017-04-05T12:11:58.000Z","updated":"2017-05-27T04:49:48.274Z","comments":true,"path":"blog/生信入门系列之-linux-入门（一）：基础命令（上）/","link":"","permalink":"https://hui-liu.github.io/blog/生信入门系列之-linux-入门（一）：基础命令（上）/","excerpt":"","text":"我们在启动终端时，会显示类似这样的内容：liuhui@ginkgo:~$ ，如图所示： ​ 其中，liuhui 表示用户名，gingko 表示计算机名称，~ 代表家目录，表示我当前的位置是 /home/liuhui，$ 为命令输入提示符（root 用户，为 #），表示在其后的光标提示符中输入命令。比如在家目录下有一个叫 sample_data 目录，那我输入： cd sample_data 就可以进到 sample_data 这个目录里面了，其中 cd 是英文 “change directory” 的缩写，用于切换工作目录，如图所示： ​ 命令 cd 对目录进行操作，可接相对路径或绝对路径。上述的 sample_data 其实就是一个相对路径；接绝对路径可以这样做： cd /home/liuhui/sample_data 结果是一样的。 注意：不要混淆了目录和相对路径这两个概念；目录就是一个文件夹，而相对路径，从字面上理解就是一个“路径”，可以从A到 B的路径，路径有一个个目录连接而成，单个目录是最简单的路径；就上述的例子而言，如果家目录下没有 sample_data 这个目录的话，那就会报错： -bash: cd: sample_data: No such file or directory 如果家目录下没有 sample_data 这个目录，而有一个叫 test 的目录，这个目录里有一个叫 sample_data 的目录，那么就应该这样做： cd test/sample_data 如图所示： 好的，回到正题，我们通过 cd 命令进入了 sample_data 这个目录中，我们首先想要知道的就是这个目录里有没有文件，如果有，有那些文件，而这些文件的内容又是什么？这时，可以用命令 ls（是 list directory contents 的缩写）结果如下图： 可以看出 sample_data 目录下有两个文件 / 目录，但其具体的属性，即详细信息我们却不知道。可以给命令 ls 加一个参数 -l，如图： 这就详细多了，比如我知道了文件的大小，文件最后修改的时间，知道除了文件之外，还有一个目录，说的不是太清楚，看图： 文件或目录的权限及链接数下次再讲，这次有个映像即可。但是还有一点问题，就是我们比较关心的文件大小似乎没有单位，有点看不懂啊。所以，还可以加个参数 -h（官方解释：human-readable），显示如下，不用说相信大家都清楚文件的大小是多少了： 当然还可以以时间排序或逆序： liuhui@ginkgo:~/sample_data$ ls -lhttotal 212Kdrwxrwxr-x 2 liuhui liuhui 4.0K Apr 5 16:24 transcriptome-rw-rw-r-- 1 liuhui liuhui 206K Apr 5 12:38 Trinity.fastaliuhui@ginkgo:~/sample_data$ ls -lhtrtotal 212K-rw-rw-r-- 1 liuhui liuhui 206K Apr 5 12:38 Trinity.fastadrwxrwxr-x 2 liuhui liuhui 4.0K Apr 5 16:24 transcriptome 所以，我们到这就明白了，ls 是查看某个目录里面有什么文件或子目录，但如果我想查看一个文件里有什么内容，该怎么办呢，这时有几种办法： 查看文件的头几行，用 head 命令（默认前 10 行）： liuhui@ginkgo:~/sample_data$ head Trinity.fasta&gt;TRINITY_DN63_c0_g1_i1 len=484 path=[663:0-65 669:66-161 667:162-216 660:217-233 661:234-257 666:258-332 642:333-356 670:357-368 668:369-395 665:396-419 647:420-423 648:424-483] [-1, 663, 669, 667, 660, 661, 666, 642, 670, 668, 665, 647, 648, -2]CCAGCGTGGGGCCGGGGGCCGGTGAGTGGCTACCCAGCACCGCGGACAGAGGGGCACCCCAAGACCTGTACTCTCTGCCTCTAGGAGGAAGGAGAGTGAAGGGGATAGGATATGAAGTGGGTGCCAGACAAGGTGTGGGGATGCTACCACCGATTTGTTCTCCCTACGGCACCAGCTGTAGCTTTGGAAGCCGCGCAGCCCCATCTTCCCTAATCTTAGCCCATCCCGTTACTATTTCCAGGATAGCCCATCATGCATAGAAAGAAAACAGAGCCCTAGGCAGAGGGAGCCATAACCCACAAGGCATTTGTAGAGAAATGGAAAAAGAGTCGCCCTAGGGTAGCAGCGCAGGGAGCAGGAGTCTCCTGTGTCCTGGTGAAGATGCACAGAACAGAAAGCTGGGCCTGCAAGCTGCCTGGCTTGTTTGCTTTGGCTTCCTCTCTCCAAAGCCTGACTGTCCTTGGAGTATTCTGGTCCTCTGTGG 当然也可以查看文件的最末尾的几行，用tail 命令（默认最后 10 行）： liuhui@ginkgo:~/sample_data$ tail Trinity.fastaATCAGATCTAATTCTTACATTTTGAGATAACTGTAAAAAGAGTGAAATTATGAAATGGCCAATATCTTTTATCAGTCTATTCTTTTGGAAGCTGTCATGCACTATACATTGTGTACAGTTAAAAGTATATATATATATATATTCTTACTGAGTGAACGCCTCCTCTCCCCACGCCTGTATGTCACTAGCATCTAAGGAGAATGCTCAAGGCCCAGTGCTGCTGCTGCTGTGGTTTATATGGGTTTTGTTCTGTTTTGTTTTTGTGTGGTAAATTGATATTTAAAAACAACAAAAACCACGACTACTGTTTACAGACTGAAAAAAAACAATCACTGCTTTTTATACTACTGAGATCCTAAGTCAAGACTTTGCAAAGCAGGAATCGGGTTCAAGTTACTTCTTTGCTGTGGACGGATAGTCCTCTGTAGTATCTCCACATGATGGAGAGTGCACAAACCTAGGTGTGCTGCCATCAATTTTGTATATTTTCATAATTTTAATTGTTCGAAATTGCATTATATTTTGCAATCACCACATTCAATCTGTATATGTCTTTCATTTCAACTTTTTCAATACAAAAAGGGG 查看整个文件中的内容，用 cat 命令（对于几 kb 以上的文件，不建议用这个）： cat Trinity.fasta 分页浏览文件内容，用 less 命令（输入 q 退出）： less Trinity.fasta 显示如下： 但有个问题，就是太长的行，如上图中的那些数字，会折成两行或以上（即折行），这个文件还好，但对于其它文件，如 vcf 文件，就有可能看起来很费眼神，这是可以给 less 加个参数 -S（大写的），效果如下： 这样就好很多了。 有时我们想创建一个文件，可以通过以下方法： touch 命令： touch new_file 还可以用重定向操作符，&gt;，创建一个文件： &gt; new_file2 文件编辑命令，nano，这样还可以直接在里面添加内容了（相当于 windows 的记事本），操作如下所示： nano new_file3 输入内容如图所示： 保存（y），然后回车： ls 查看一下： 对于小文件，可以用 cat 命令查看内容： 前面提到的重定向操作符 &gt;，还可以这样用，相当于将 new_file3 的内容拷贝给了 new_file2： 另外，cat 命令也可以同时查看两个或两个以上文件的内容： 这时，再通过重定向操作符 &gt; 就可以把两个或两个以上的内容合并到一个文件里了： 这几个命令颠来倒去的重合在一起用是不是很有趣，也很强大；其实这也是 linux 命令的强大之处：通过几个不同的命令组合在一起，往往会发挥出强大的功能。这个概念很重要，上面演示的只不过时冰山一角罢了。 有时我们想删除一个文件，这时可以用命令 rm来操作： 注意：这是永久性删除。 通过 linux 命令，可以很方便地对文件进行复制操作，命令是 cp： cp old_file new_copy_file 修改文件名同样很简单，通过 mv 命令实现： mv old_file_name new_file_name 当然，还可以通过 mv 命令将一个文件移到另一个文件夹里（相当于在 windows 里将一个文件剪切，然后粘贴到另一个文件夹里），下图就演示了将文件 Trinity.fasta 移到了目录 transcriptome 里： 注意：这里的 transcriptome 实际上就是一个相对路径；如果 sample_data 里没有这个目录，那当然会不会报错，只不过就相当于修改文件名了。 之前提到了怎么创建一个文件，这里当然要说一下怎么创建一个目录： mkdir directory_name 也可以优雅地删除一个目录，只比删除文件多了一个参数，-r： rm -r directory_name 好了，这次先讲到这里。 注意： （1）命令与文件之间要有一个空格； （2） 创建、复制或重命名一个文件或目录，如果文件或目录前没有路径名，则产生的文件就在当前目录里； （3）文件的命名不要出现空格或其它特殊字符，如“!”，“&amp;”等。 浓缩版： # (1) 切换工作目录 (最后一个斜杠可有可无，命令与路径之间要有空格)cd ~/sample_data/transcriptome/# 大家试下以下的三个用法，看看效果cdcd -cd ~# (2) 显示当前工作目录pwd# (3) 创建目录mkdir folder_name# (4) 创建文件touch file_name1&gt; file_name2nano file_name3# (5) 打印文件内容及合并另个两个或以上文件的内容# 打印文件内容cat file_name# 同时打印两个文件的内容，第二个文件的内容会紧跟在第一个文件内容的最后面cat file_name1 file_name2# 利用上一个用法并结合从定向操作符“&gt;”，可以合并两个文件的内容cat file_name1 file_name2 &gt; combined_file# (6) 分页浏览较大文件的内容less file_name# 不折行less -S file_name# 显示行号less -SN file_name# (7) 查看文件头几行head file_name# 查看前 20 行head -n 20 file_name# (8) 查看文件末尾几行tail file_name# 查看前 20 行tail -n 20 file_name# (9) 查看文件及目录属性lsls -lh# 只查看某个文件或目录ls -lh file_namels -lh directory_name# (10) 删除文件及目录# 删除文件rm file_name# 删除目录rm -r directory_name# (11) 移动文件目录或修改文件目录的名称# 移动文件到目录“directory_name” 里的子目录“directory_name” 里（前提是这个路径存在）mv file_name directory_name/sub_directory_name# 修改文件名mv old_file_name new_file_name# 移动目录，将“directory_name” 移到目录“directory_name” 里的子目录“sub_directory_name” 里（前提是这个路径存在）mv directory_name1 directory_name/sub_directory_name# 修改目录名，将“directory_name1” 的名称改为“directory_name2”（directory_name2不存在，否则就将“directory_name1” 移动到“directory_name2”里了）mv directory_name1 directory_name2# (12) 复制文件或目录# 复制文件到当前目录下，并重命名成 “file_name2”（必须重命名）cp file_name1 file_name2# 复制文件到另外一个目录里cp file_name1 directory_name/# 复制文件到另外一个目录里并重命名cp file_name1 directory_name/file_name2# 复制目录cp -r directory_name1 directory_name2 命令及简单描述 目录操作 cd 切换工作目录 pwd 显示当前工作目录 mkdir 创建目录 文件操作 touch、&gt; 创建文件 nano 编辑文件 cat 打印文件内容及合并另个两个或以上文件的内容 less 分页浏览文件内容 head 查看文件头几行 tail 查看文件末尾几行 文件及目录操作 ls 查看文件及目录属性 rm 删除文件及目录 mv 移动文件目录或修改文件目录的名称 cp 复制文件或目录","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"linux 入门（一）：初识 linux 系统","slug":"linux-入门（一）：初识-linux-系统","date":"2017-04-01T15:08:56.000Z","updated":"2017-04-03T04:50:55.320Z","comments":true,"path":"blog/linux-入门（一）：初识-linux-系统/","link":"","permalink":"https://hui-liu.github.io/blog/linux-入门（一）：初识-linux-系统/","excerpt":"","text":"生信入门系列之 linux 入门（一）：初识 linux 系统​ Linux 是一个免费的操作系统，是生物信息分析中必不可少的工具。在 linux 系统中，所有的管理任务均可以在一个叫终端（terminal）的控制面板里完成，包括文件处理，软件安装以及用户管理。这个终端是交互的，即你运行特定的命令，相应的结果会在这个终端上显示出来。运行命令的方式是：在终端上输入你希望运行的命令，然后按回车键（Enter）。如果你想终止正在运行的命令，可以按 Chrl + C。 ​ 不同于 windows 系统，linux 的文件系统是一个目录树（directory tree）；如下图 所示，其文件系统为一个树状结构。最顶端 “root”，用斜杠 “/” 表示。一般来说，普通用户，无论是直接打开终端还是远程登陆服务器，所在的位置一般是在 /home/foo 下，其中的 foo 在这里指代用户名。 ​ linux 的树状文件系统（图片来自维基百科） ​ 对于 windows 用户来说，可以通过一个轻量级的软件 putty （约 500 kb；百度可下载；双击软件即可使用，无需安装）来远程登陆服务器，登陆方法如下图，图中的序号为操作顺序： 首先在 Host Name（or IP address）下方的框框中输入服务器的的 ID 地址，并单击 “Open” 选项； putty 会弹出一个警告框，点击否； 最后最出现下图 3 中的界面，在 “login as：“ 后输入你的用户名，回车后输入登陆密码（直接输入就行了，是看不到显示的）。 ​ 打开终端或是远程登陆服务器时，如果你想查看当前所在的位置，可以在终端输入如下命令，并按回车键： pwd ​ 这个 linux 命令意思是“打印当前工作目录”，是 “print working directory” 的英文缩写；其返回结果是一个绝对路径（就是从根目录开始，依次将各级子目录的名字组合起来），应该类似这样： /home/foo ​ 与上述的树状文件系统相互比照，是不是立马清楚自己到底在哪里了？就好像 windows 下，到底在哪个盘的哪个文件夹里一样。这个绝对路径很有用，它不仅让我们知道自己在哪儿，同时还可以告诉系统某个软件在哪儿，以及告诉软件要操作的文件在哪儿。举个例子吧，比如说我想调用一个软件，叫 vcftools，那么，我要运行它，只需要在终端输入如下命令并回车： vcftools --vcf input_data.vcf ​ 就可以轻轻松松算出 variants 的数目和 individuals 的数目。但很不幸的是，你也有可能得到如下结果： bash: vcftools: command not found ​ 大概意思就是，系统找不到这个命令在哪儿。可能有人会问了，既然是一个命令，为什么系统会找不到呢？其实，在linux 系统中，有一个非常核心的概念：一切皆文件！即在linux环境下，任何事物都以文件的形式存在。所以，如果你的从 vcftools 安装在 /home/foo/biosoft/vcftool-0.1.13/bin 这个绝对路径下，那么，你就可以这样运行它： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 就可以的结果啦。但也有可能得到如下结果： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf input_data.vcfstat error: No such file or directoryError: Can&apos;t determine file type of input_data.vcf ​ 这时也不要慌，只要在输入文件前加上绝对路径即可。加入 input_data.vcf 文件在 /home/foo/vcffile 下，可以这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 这时，如无意外，就可以得到如下结果了： VCFtools - v0.1.13(C) Adam Auton and Anthony Marcketta 2009Parameters as interpreted: --vcf /home/foo/vcffile/input_data.vcfUsing zlib version: 1.2.3.4Versions of zlib &gt;= 1.2.4 will be *much* faster when reading zipped VCF files.After filtering, kept 16 out of 16 IndividualsAfter filtering, kept 1116595 out of a possible 1116595 SitesRun Time = 5.00 seconds ​ 如果还报出一些奇奇怪怪的错误提示，那就首先检查一下您的输入法中是否为纯英文状态，中文和全角状态下的输入的空格都是会报错的。同时，linux 里，软件对字母大小写是敏感的，即 linux 认为 A 和 a 是两个不同的事物；也就是说大小写也是会造成错误的。 ​ 前面，我多次提到了绝对路径这个概念，不少心思敏捷的童鞋就会想了，有没有相对路径？有的。 ​ 举例说明，假如我们在 /home/foo 这个路径下，并且我们知道该路径下有 vcffile 和 biosoft 这两个目录；那么，我们可以这样运行上述的命令： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 可以看到，biosoft/vcftool-0.1.13/bin 和 vcffile 这两个路径都不是以斜杠 / 开头的，所以这两个路径都是相对路径。当然了，你也可也这样运行： biosoft/vcftool-0.1.13/bin/vcftools --vcf /home/foo/vcffile/input_data.vcf ​ 亦或这样： biosoft/vcftool-0.1.13/bin/vcftools --vcf vcffile/input_data.vcf ​ 也可以进到 vcffile 这个目录里，这样运行： /home/foo/biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 或者这样（ “..” 在这里代表上级目录，相应的，”../..“ 代表上级目录的上级目录）： ../biosoft/vcftool-0.1.13/bin/vcftools --vcf input_data.vcf ​ 总之，想怎么运行，看心情！ ​ 相信看到这里，会有记忆力超好的童鞋会问了，我该如何像运行 pwd 那样运行 vcftools 呢？而不是在它前面加上一大串绝对路径或是相对路径！ ​ 要回答这个问题，小编先给大家展示两个命令（不深入讲解）： ​ 第一个是： which pwd ​ 返回的应该是： /bin/pwd ​ 第二个是： echo $PATH ​ 会返回类似下面的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 眼尖的童鞋会发现，上面一串结果其实就是许多绝对路径通过 “:” 连接在一起的（叫做环境变量），其中有一个路径 “/bin” ；而 which pwd 返回的结果是 /bin/pwd。这两个有什么关系呢？其实不必深究，我们只需要知道，linux 把 pwd 看作一个文件（还记得前面说的“一切皆文件”吗），linux 系统会在上述的环境变量中从左往右依次查找，看某个路径下是否有 pwd 这个文件，然后执行这个命令。并且，环境变量是可以编辑的， 即可在环境变量 “PATH” 中添加特定的路径。同理，如果我们的 vcftools 软件（其实就是个文件）的路径也在上述的路径中，就可以在终端直接输入 vcftools 就可以运行了。 ​ 那么问题来了，我们该如何将特定软件的路径发到上述的环境变量 “PATH” 中呢？ ​ 只需要通过 export 命令，在终端中输入以下内容，回车后，就可以将 vcftools 的路径导入到上述的环境变量中： # vcftools （井号后的内容 linux 系统不会读取，可以做注释）export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; # 添加这一行就行了，export 后要加空格，不要换行。 ​ 就会得到类似这样的结果： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/foo/biosoft/vcftool-0.1.13/bin ​ 当然，如果你是这样输入的， export PATH=&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos;:$PATH ​ 那就应该得到类似这样的结果： /home/foo/biosoft/vcftool-0.1.13/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games ​ 就可以不加绝对路径运行 vcftools 了。像这样： vcftools --vcf input_data.vcf ​ 但上述做法有一个弊端，就是这个做法是暂时性的，就是说每次打开或登陆终端时，都要运行一下这个命令才行，非常麻烦。所以怎么办呢？ ​ 其实不难，在每个用户的家目录下，即上述的 /home/foo 下，都有一个非常重要的隐藏文件 “.bashrc”，里面有许多我们每次启动或登陆终端时，linux 系统都会默认自动运行的命令。所以，只需将 export PATH=$PATH:&apos;/home/foo/biosoft/vcftool-0.1.13/bin&apos; 添加到 “.bashrc” 文件的最后一行即可。这样我们每次启动或登陆终端时，系统就会自动运行这个命令了，这样就免去可多次手动添加的麻烦。如果不想重启终端，可以执行这个命令（相当于让系统执行一遍 “.bashrc” 中的命令）： source .bashrc ​ 怎么加呢？可以在家目录下，运行一个文本编辑命令 vim 或 nano： vim .bashrc 对于 vim 怎么使用，可以自行百度，有详细教程，这里不做赘诉（使用起来比较复杂）。 或另一个命令 nano： nano .bashrc ​ 这个比较简单，只需回车后： 按向下箭头（一直到文件最底部） 黏贴 export PATH=$PATH:&#39;/home/foo/biosoft/vcftool-0.1.13/bin&#39; 依次按 ctrl + x，y，Enter（即保存退出） 注意1：/home/foo/biosoft/vcftool-0.1.13/bin 要做根据自己的实际路径做相应地改动。 注意2： 在 linux 系统里，通过 nano 或是 vim 一般是通过上下左右等方向键或其它快捷方式移动光标。 重要知识点回顾： 终端 目录树 家目录 大小写敏感 绝对路径和相对路径 一切皆文件 环境变量及其编辑 ​下期预告：linux 基础命令","categories":[{"name":"生信","slug":"生信","permalink":"https://hui-liu.github.io/categories/生信/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/tags/linux/"}]},{"title":"python 学习笔记","slug":"python-学习笔记","date":"2017-03-16T05:16:25.000Z","updated":"2017-03-16T05:18:34.926Z","comments":true,"path":"blog/python-学习笔记/","link":"","permalink":"https://hui-liu.github.io/blog/python-学习笔记/","excerpt":"","text":"python 笔记1. 基础1.1 基本语法控制流相关# (1) 为何1 in [1,0] == True执行结果是False1 in [1,0] == True# 将被转为(1 in [1, 0]) and ([1, 0] == True)###### 同样的a &lt; b &lt; c# 会被转为(a &lt; b) and (b &lt; c) # b不会被解析两次# (2) 如何检测一个变量是否存在# (2.1) 检测本地变量if 'myVar' in locals(): # myVar exists. # (2.2) 检测全局变量if 'myVar' in globals(): # myVar exists.# (2.3) 检测一个对象是否包含某个属性if hasattr(obj, 'attr_name'): # obj.attr_name exists.# (3) Python中的三元运算符 a if test else b # 如果 test 为 True，返回 a，否则返回 b# 使用:&gt;&gt;&gt; 'true' if True else 'false''true'&gt;&gt;&gt; 'true' if False else 'false''false' 1.2 字符串相关# (1) 如何反向输出一个字符串&gt;&gt;&gt; 'hello world'[::-1]'dlrow olleh'# (2) 如何随机生成大写字母和数字组成的字符串'''6U1S754Z4UKKU911K4'''import string, random''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(6))# (3) 字符串的contains# (3.1)使用in关键字if not \"blah\" in somestring: continueif \"blah\" not in somestring: continue# (3.2) 使用字符串的find/index (注意index查找失败抛异常)s = \"This be a string\"if s.find(\"is\") == -1: print \"No 'is' here!\"else: print \"Found 'is' in the string.\" # (4) 如何判断一个字符串是数字def is_number(s): try: float(s) return True except ValueError: return False# (5) 字符串格式化 % vs format# 下列输出一致#!/usr/bin/pythonsub1 = \"python string!\"sub2 = \"an arg\"a = \"i am a %s\"%sub1b = \"i am a &#123;0&#125;\".format(sub1)c = \"with %(kwarg)s!\"%&#123;'kwarg':sub2&#125;d = \"with &#123;kwarg&#125;!\".format(kwarg=sub2)print aprint bprint cprint d# .format 还可以这样用，但用 % 时无法做到这点e = \"i am a &#123;0&#125; &#123;0&#125;\".format(sub1)# %只处理一个变量或一个元组, 你或许会认为下面的语法是正确的\"hi there %s\" % name#但当name恰好是(1,2,3)时，会抛出 TypeError 异常.为了保证总是正确的，你必须这么写\"hi there %s\" % (name,) # supply the single argument as a single-item tuple# (5) 将一个包含有字典的字符串转为一个字典&gt;&gt;&gt; s = \"&#123;'muffin' : 'lolz', 'foo' : 'kitty'&#125;\"&gt;&gt;&gt; import ast&gt;&gt;&gt; ast.literal_eval(s)&#123;'muffin': 'lolz', 'foo': 'kitty'&#125;# (6) 如何填充0到数字字符串中保证统一长度# (6.1) 对于字符串&gt;&gt;&gt; n = '4'&gt;&gt;&gt; print n.zfill(3)&gt;&gt;&gt; '004'# (6.2) 对于数字&gt;&gt;&gt; n = 4&gt;&gt;&gt; print '%03d' % n&gt;&gt;&gt; 004&gt;&gt;&gt; print \"&#123;0:03d&#125;\".format(4) # python &gt;= 2.6&gt;&gt;&gt; 004 1.3 文件相关# (1) 如何检查一个文件是否存在import os.pathprint os.path.isfile(fname)print os.path.exists(fname)# (2) 如何创建不存在的目录结构import os.pathif not os.path.exists(directory): os.makedirs(directory) # 需要注意的是，当目录在exists和makedirs两个函数调用之间被创建时，makedirs将抛出OSError# (3) 如何拷贝一个文件from shutil import copyfilecopyfile(src, dst)# (4) 如何找到一个目录下所有.txt文件# (4.1) 使用globimport globimport osos.chdir(\"/mydir\")for files in glob.glob(\"*.txt\"): print files# (4.2) 使用os.listdirimport osos.chdir(\"/mydir\")for files in os.listdir(\".\"): if files.endswith(\".txt\"): print files# (4.3) 或者遍历目录import osfor r,d,f in os.walk(\"/mydir\"): for files in f: if files.endswith(\".txt\"): print os.path.join(r,files)# (5) 如何逐行读取文件# (5.1) 先将文件读入内存，然后逐行读取for line in open(\"test.txt\").readlines(): print line# (5.2) 利用file的迭代器for line in open(\"test.txt\"): #use file iterators print line 2. 基本数据结构2.1 列表# (1) Python 中如何复制一个列表# (1) 切片操作&gt;&gt;&gt; a = [1, 2, 3, [4, 5]]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(a), id(b)(4292218700L, 4292555596L)# 当列表中还有列表时，则无法实现真正的拷贝了&gt;&gt;&gt; a[2] = 0&gt;&gt;&gt; a[3][1] = 6&gt;&gt;&gt; a[1, 2, 0, [4, 6]]&gt;&gt;&gt; b[1, 2, 3, [4, 6]]# (2) list()函数&gt;&gt;&gt; c = [\"a\", \"b\", \"c\", [\"d\", \"e\"]]&gt;&gt;&gt; c['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; d = list(c)&gt;&gt;&gt; d['a', 'b', 'c', ['d', 'e']]&gt;&gt;&gt; id(c), id(d)(4292555596L, 4292218732L)# 当列表中还有列表时，同样无法实现真正的拷贝了&gt;&gt;&gt; c[1] = 0&gt;&gt;&gt; c[3][1] = 0&gt;&gt;&gt; c['a', 0, 'c', ['d', 0]]&gt;&gt;&gt; d['a', 'b', 'c', ['d', 0]]# (3) “乘法”操作&gt;&gt;&gt; e = [1, 2, 3, [4, 5]]&gt;&gt;&gt; f = e * 1&gt;&gt;&gt; f[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(e), id(f)(4292721772L, 4292721260L)# 同样的问题&gt;&gt;&gt; e[1] = 0&gt;&gt;&gt; e[3][1] = 0&gt;&gt;&gt; e[1, 0, 3, [4, 0]]&gt;&gt;&gt; f[1, 2, 3, [4, 0]]# (4) copy.copy&gt;&gt;&gt; import copy&gt;&gt;&gt; g = [1, 2, 3, [4, 5]]&gt;&gt;&gt; h = copy.copy(g)&gt;&gt;&gt; h[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(g), id(h)(4292218860L, 4292218764L)# 还是一样&gt;&gt;&gt; g[1] = 0&gt;&gt;&gt; g[3][1] = 0&gt;&gt;&gt; g[1, 0, 3, [4, 0]]&gt;&gt;&gt; h[1, 2, 3, [4, 0]]# copy.deepcopy&gt;&gt;&gt; i = [1, 2, 3, [4, 5]]&gt;&gt;&gt; j = copy.deepcopy(i)&gt;&gt;&gt; j[1, 2, 3, [4, 5]]&gt;&gt;&gt; id(i), id(j)(4292215020L, 4292598732L)# 完全新的拷贝&gt;&gt;&gt; i[1] = 0&gt;&gt;&gt; i[3][1] = 0&gt;&gt;&gt; i[1, 0, 3, [4, 0]]&gt;&gt;&gt; j[1, 2, 3, [4, 5]]# (2) 列表的 append 和 extend 的区别&gt;&gt;&gt; x = [1, 2]&gt;&gt;&gt; x.append(3)&gt;&gt;&gt; x[1, 2, 3]&gt;&gt;&gt; x.append([4,5])&gt;&gt;&gt; x[1, 2, 3, [4, 5]]&gt;&gt;&gt;&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; x.extend([4, 5])&gt;&gt;&gt; x[1, 2, 3, 4, 5]# (3) 如何随机地从列表中抽取变量foo = ['a', 'b', 'c', 'd', 'e']from random import choiceprint choice(foo)# (4) 如何将一个列表切分成若干个长度相同的子序列# 想要得到这样的效果lis = range(1, 1000)print chunks(lis, 10) -&gt; [ [ 1..10 ], [ 11..20 ], .., [ 991..999 ] ]# (4.1) 使用yield:def chunks(lis, n): \"\"\" Yield successive n-sized chunks from lis. \"\"\" for i in xrange(0, len(lis), n): yield lis[i:i+n]list(chunks(range(10, 75), 10))# (4.2) 直接处理def chunks(l, n): return [l[i:i+n] for i in range(0, len(l), n)] 2.2 字典# (1) 使用列表解析创建一个字典d = &#123;key: value for (key, value) in sequence&#125;&gt;&gt;&gt; &#123;i : chr(65+i) for i in range(4)&#125;&#123;0: 'A', 1: 'B', 2: 'C', 3: 'D'&#125;&gt;&gt;&gt; &#123;(k, v): k+v for k in range(4) for v in range(4)&#125;&#123;(0, 1): 1, (1, 2): 3, (3, 2): 5, (0, 0): 0, (3, 3): 6, (3, 0): 3, (3, 1): 4, (2, 1): 3, (0, 2): 2, (2, 0): 2, (1, 3): 4, (2, 3): 5, (2, 2): 4, (1, 0): 1, (0, 3): 3, (1, 1): 2&#125;# (2) 如何在单一表达式中合并两个Python字典&gt;&gt;&gt; x = &#123;'a':1, 'b': 2&#125;&gt;&gt;&gt; y = &#123;'b':10, 'c': 11&#125;&gt;&gt;&gt; z = dict(x.items() + y.items())&gt;&gt;&gt; z&#123;'a': 1, 'c': 11, 'b': 10&#125;# (3) 如何映射两个列表成为一个字典&gt;&gt;&gt; keys = ['a', 'b', 'c']&gt;&gt;&gt; values = [1, 2, 3]&gt;&gt;&gt; dictionary = dict(zip(keys, values))&gt;&gt;&gt; print(dictionary)&#123;'a': 1, 'b': 2, 'c': 3&#125;# (4) 根据 dict 内值, 排序一个列表中的所有 dictlist_to_be_sorted = [&#123;'name':'Homer', 'age':39&#125;, &#123;'name':'Bart', 'age':10&#125;]# (4.1) 简单的做法newlist = sorted(list_to_be_sorted, key=lambda k: k['name'])# (4.2) 高效的做法from operator import itemgetternewlist = sorted(list_to_be_sorted, key=itemgetter('name'))# 根据值给字典排序x = &#123;1: 2, 3: 4, 4:3, 2:1, 0:0&#125;sorted(x.iteritems(), key=lambda k: k[1])","categories":[{"name":"notes","slug":"notes","permalink":"https://hui-liu.github.io/categories/notes/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"networkx 笔记","slug":"networkx-笔记","date":"2017-03-16T01:25:24.000Z","updated":"2017-03-16T05:10:09.554Z","comments":true,"path":"blog/networkx-笔记/","link":"","permalink":"https://hui-liu.github.io/blog/networkx-笔记/","excerpt":"","text":"networkx 复杂网络分析笔记主要参考1，2，3。 数据 facebook_combined.csv，relation_weight_sam.csv。 1. 建图networkx可以建立简单无向图graph，有向图digraph，可重复边的multi-graph。 1.1 简单无向图 graphimport networkx as nx#建立一个空的无向图GG=nx.Graph()#添加一个节点1G.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）G.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边G.add_edge(3,2)#输出全部的节点： [1, 2, 3]print G.nodes()#输出全部的边：[(2, 3)]print G.edges()#输出边的数量：1print G.number_of_edges()#输出点数print G.number_of_nodes() 无向图 1.2 有向图 digraph有向图的建立方式和无向图基本类似，只是在上述代码的第二行，将G = nx.Graph() 改为 G = nx.DiGraph() 。需要注意的是，此时再添加边3-2与边2-3，则被认为是两条不同的 import networkx as nxD=nx.DiGraph()#添加一个节点1D.add_node(1)#添加一条边2-3（隐含着添加了两个节点2、3）D.add_edge(2,3)#对于无向图，边3-2与边2-3被认为是一条边D.add_edge(3,2)#输出全部的节点： [1, 2, 3]print D.nodes()#输出全部的边：[(2, 3), (3, 2)]print D.edges()#输出边的数量：2print D.number_of_edges()#输出点数print D.number_of_nodes() 同时，有向图和无向图是可以相互转化的，分别用到Graph.to_undirected() 和 Graph.to_directed()两个方法。 有向图 1.3 加权图有向图和无向图都可以给边赋予权重，用到的方法是add_weighted_edges_from，它接受1个或多个三元组[u,v,w]作为参数，其中u是起点，v是终点，w是权重。例如： # 添加0-1、1-2和2-3三条边，权重分别是3.0和7.5G.add_weighted_edges_from([(0,1,3.0),(1,2,7.5),(2,3,1.0)])# 如果想读取权重，可以使用get_edge_data方法，它接受两个参数u和v，即边的起始点。print G.get_edge_data(1,2)# 选出边的权重超过一个阈值的边estrong = [(u,v) for (u,v,d) in G.edges(data=True) if d[\"weight\"] &gt; 3.0]print estrong # [(1, 2)] 加权图 这是一个加权图的代码 from matplotlib import useuse(\"Agg\")import matplotlib.pyplot as pltimport networkx as nximport pandas as pddf = pd.read_csv(\"../data/relation_weight_sam.csv\")G = nx.from_pandas_dataframe(df, 'node1', 'node2', edge_attr='weight' )# saves the positions of the nodes on the visualization# In detail positions is a dictionary where each node is# a key and the value is a position on the graph# &#123;'Fam38a_predicted': array([ 0.52246857, 0.4412573 ], dtype=float32),...&#125;positions = nx.spring_layout(G)# pass positions and set hold=Truenx.draw(G, pos=positions, hold=True, with_labels=False, node_size=30)weights = [w[2]['weight']*5 for w in G.edges(data=True)]#width can be array of floatsnx.draw_networkx_edges(G, pos=positions, width=weights)plt.savefig(\"../output/net_weight.png\")# 前十行数据，weight是通过\"numpy.random.rand()\"模拟的。\"\"\"node1 node2 weightAblim2 Acsl6 0.0656480910603Ablim2 Apeg1 0.0727940253706Ablim2 Atp2a2 0.5280955211Ablim2 Boll_predicted 0.200347948345Ablim2 Cap2 0.108700562945Ablim2 Copb1 0.546335670525Ablim2 Creld2 0.420412397031Ablim2 Dtna_predicted 0.24546480993Ablim2 Dusp8_predicted 0.932345236242\"\"\" 1.4 可重复边的 multi-graphimport networkx as nxM=nx.MultiGraph() 1.5 点这里的点可以是任意可区分的对象（hashable），比如数字，字符串，对象等。 G.add_node(1)G.add_node('first_node')#这里用一个对象多为key来唯一区别一个点#我们还能够用一个列表来批量加入点G.add_nodes_from([1,2,3])#还可以用一个图对象作为点，嵌入到其他图中G.add_node(D) #这里D作为一个点的key#或者把一个图的所有点赋予另一个图G.add_nodes_from(D) #这里返回D的所有点，赋予G#与加入相同的传递方法，我们也可以删除点G.remove_node(1)G.remove_nodes_from([1,2,3]) 1.6 边这里的边可以使用两个单独的对象作为输入 G.add_edge(1,2) #表示1，2之间有一条边。#如果不存在点1，2，则会自动加入点集合。#或者以元组的形式作为输入e=(1,2)G.add_edge(*e)#这里的*代表吧元组解包（unpack），当作一个个的值扔到函数中去。#如果不解包，等价于#G.add_edge(e)=G.add_edge((1,2))与参数传递的方式不符。#类似的，我们还可以使用包含元组的列表来传递参数G.add_edges_from([(1,2),(2,3)])#我们还可以报一个图的边赋予另一个图G.add_edges_from(H)#删除G.remove_edge(1,2)G.remove_edges_from([(1,2),(2,3)]) 1.7 访问node_list = G.nodes()edge_list = G.edges()#可以返回包含点与边的列表node = G.node[‘first_node’]#如上根据key返回点edge = G.edge['first_node']['second_node']#同样的方法，返回两个key之间的边 1.8 属性我们可以给图，点，边赋予各种属性，最简单的就是权值属性 G.add_node(1,time='5pm')#在添加时跟上属性G.add_nodes_from([1,2,3],time='5pm')#批量添加点是，跟上统一的属性G.add_nodes_from([(3,&#123;'time':'5pm'&#125;), (4,&#123;'time':'4pm'&#125;)])#或者写成元组列表[（key,dict），（key,dict）]的形式G.node[1]['time']#根据字典key访问属性值。#给边添加属性也类似G.add_edge(1,2,time='3am')G.add_edges_from([(1,2,&#123;'time'='5pm'&#125;),(2,3,&#123;'time'=3am&#125;)])#批量赋予属性G.edge[1][2][‘time’]#访问#我们还可以使用特定的函数批量返回属性，如time = nx.get_edge_attributes(G,'time')#返回得到以元组为key,time属性为值得一个字典time[(1,2)] 1.9 图算法NetworkX提供了常用的图论经典算法，例如DFS、BFS、最短路、最小生成树、最大流等等，非常丰富，如果不做复杂网络，只作图论方面的工作，也可以应用NetworkX作为基本的开发包。 #调用多源最短路径算法，计算图G所有节点间的最短路径path=nx.all_pairs_shortest_path(G)#输出节点0、2之间的最短路径序列： [0, 1, 2]print path[0][2] 1.10 画图nx.draw(G) # 方法，至少接受一个参数：待绘制的网络G matplotlib.show() #显示出来 画图参数运行样式 node_size: 指定节点的尺寸大小(默认是300) node_color: 指定节点的颜色 (默认是红色，可以用字符串简单标识颜色，例如’r’为红色，’b’为绿色等) node_shape: 节点的形状（默认是圆形，用字符串’o’标识） alpha: 透明度 (默认是1.0，不透明，0为完全透明) width: 边的宽度 (默认为1.0) edge_color: 边的颜色(默认为黑色) style: 边的样式(默认为实现，可选： solid|dashed|dotted,dashdot) with_labels: 节点是否带标签（默认为True） font_size: 节点标签字体大小 (默认为12) font_color: 节点标签字体颜色（默认为黑色） 运用布局 circular_layout：节点在一个圆环上均匀分布 random_layout：节点随机分布 shell_layout：节点在同心圆上分布 spring_layout： 用Fruchterman-Reingold算法排列节点（样子类似多中心放射状） spectral_layout：根据图的拉普拉斯特征向量排列节点 添加文本 用plt.title()方法可以为图形添加一个标题，该方法接受一个字符串作为参数。 fontsize参数用来指定标题的大小。例如：plt.title(“BA Networks”, fontsize = 20)。 如果要在任意位置添加文本，则可以采用plt.text()方法。 2. 四种网络模型NetworkX提供了4种常见网络的建模方法，分别是：规则图，ER随机图，WS小世界网络和BA无标度网络。 2.1 规则图规则图差不多是最没有复杂性的一类图，random_graphs.random_regular_graph(d, n)方法可以生成一个含有n个节点，每个节点有d个邻居节点的规则图。 下面一段示例代码，生成了包含20个节点、每个节点有3个邻居的规则图： import networkx as nximport matplotlib.pyplot as plt# regular graphy# generate a regular graph which has 20 nodes &amp; each node has 3 neghbour nodes.RG = nx.random_graphs.random_regular_graph(3, 20)# the spectral layoutpos = nx.spectral_layout(RG)# draw the regular graphynx.draw(RG, pos, with_labels = False, node_size = 30)plt.show() 2.2 ER随机图ER随机图是早期研究得比较多的一类“复杂”网络，模型的基本思想是以概率p连接N个节点中的每一对节点。用random_graphs.erdos_renyi_graph(n,p)方法生成一个含有n个节点、以概率p连接的ER随机图： import networkx as nximport matplotlib.pyplot as plt# erdos renyi graph# generate a graph which has n=20 nodes, probablity p = 0.2.ER = nx.random_graphs.erdos_renyi_graph(20, 0.2)# the shell layoutpos = nx.shell_layout(ER)nx.draw(ER, pos, with_labels = False, node_size = 30)plt.show() 2.3 WS小世界网络 用random_graphs.watts_strogatz_graph(n, k, p)方法生成一个含有n个节点、每个节点有k个邻居、以概率p随机化重连边的WS小世界网络。 下面是一个例子： networkx-笔记/import networkx as nximport matplotlib.pyplot as plt# WS network# generate a WS network which has 20 nodes,# each node has 4 neighbour nodes,# random reconnection probability was 0.3.WS = nx.random_graphs.watts_strogatz_graph(20, 4, 0.3)# circular layoutpos = nx.circular_layout(WS)nx.draw(WS, pos, with_labels = False, node_size = 30)plt.show() 2.4 BA无标度网络用random_graphs.barabasi_albert_graph(n, m)方法生成一个含有n个节点、每次加入m条边的BA无标度网络。 下面是一个例子： import networkx as nximport matplotlib.pyplot as plt# BA scale-free degree network# generalize BA network which has 20 nodes, m = 1BA = nx.random_graphs.barabasi_albert_graph(20, 1)# spring layoutpos = nx.spring_layout(BA)nx.draw(BA, pos, with_labels = False, node_size = 30)plt.show() 对BA模型实现代码的分析 #定义一个方法，它有两个参数：n - 网络节点数量；m - 每步演化加入的边数量def barabasi_albert_graph(n, m): # 生成一个包含m个节点的空图 (即BA模型中t=0时的m0个节点) G=empty_graph(m) # 定义新加入边要连接的m个目标节点 targets=range(m) # 将现有节点按正比于其度的次数加入到一个数组中，初始化时的m个节点度均为0，所以数组为空 repeated_nodes=[] # 添加其余的 n-m 个节点，第一个节点编号为m（Python的数组编号从0开始） source=m # 循环添加节点 while source&lt;n: # 从源节点连接m条边到选定的m个节点targets上（注意targets是上一步生成的） G.add_edges_from(zip([source]*m,targets)) # 对于每个被选择的节点，将它们加入到repeated_nodes数组中（它们的度增加了1） repeated_nodes.extend(targets) # 将源点m次加入到repeated_nodes数组中（它的度增加了m） repeated_nodes.extend([source]*m) # 从现有节点中选取m个节点 ，按正比于度的概率（即度优先连接） targets=set() while len(targets)&lt;m: #按正比于度的概率随机选择一个节点，见注释1 x=random.choice(repeated_nodes) #将其添加到目标节点数组targets中 targets.add(x) #挑选下一个源点，转到循环开始，直到达到给定的节点数n source += 1 #返回所得的图G return G from matplotlib import useuse(\"Agg\")import randomimport networkx as nxfrom networkx.generators.classic import empty_graphimport matplotlib.pyplot as pltdef barabasi_albert_graph(n, m): G=empty_graph(m) targets=range(m) repeated_nodes=[] source=m while source&lt;n: G.add_edges_from(zip([source]*m,targets)) repeated_nodes.extend(targets) repeated_nodes.extend([source]*m) targets=set() while len(targets)&lt;m: x=random.choice(repeated_nodes) targets.add(x) source += 1 return G ##G=nx.Graph()G = barabasi_albert_graph(400,6)pos = nx.spring_layout(G)nx.draw(G, pos, with_labels = False, node_size = 30)plt.savefig(\"../output/BA_400_6.png\") 3. 统计指标计算3.1 度、度分布 NetworkX可以用来统计图中每个节点的度，并生成度分布序列。 import networkx as nximport matplotlib.pyplot as plt #生成一个n=1000，m=3的BA无标度网络G = nx.random_graphs.barabasi_albert_graph(1000,3)#返回某个节点的度print G.degree(0)#返回所有节点的度print G.degree()#返回图中所有节点的度分布序列（从1至最大度的出现频次）print nx.degree_histogram(G)#返回图中所有节点的度分布序列degree = nx.degree_histogram(G)#生成x轴序列，从1到最大度x = range(len(degree))#将频次转换为频率y = [z / float(sum(degree)) for z in degree]#在双对数坐标轴上绘制度分布曲线plt.loglog(x,y,color=\"blue\",linewidth=2)#显示图表plt.show() 3.2 群聚系数# 平均群聚系数nx.average_clustering(G)# 各个节点的群聚系数nx.clustering(G) 3.3 直径和平均距离# 图G的直径（最长最短路径的长度）nx.diameter(G)# 图G所有节点间平均最短路径长度nx.average_shortest_path_length(G) 3.4 中心性一个图的直径是所有点之间最长的最短路径。在连接中心度，我们需要寻找一个点，这个点出现在很多点的最短路径中。出现的次数越多，连接中心性越高。这样的点，可以作为一个桥梁作用。意义：分析该节点对网络信息流动的影响，如：考察此人的社交能力或对于社会网络中信息流动的影响力。 betweenness centralityimport networkx as nximport matplotlib.pyplot as pltG=nx.Graph()print G.edges() # []G.add_edges_from([(1,2),(2,3),(2,4),(2,5),(1,3),(1,4),(3,5),(4,6)])print G.edges()#[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5), (3, 5), (4, 6)]# betweenness centralitybc = nx.betweenness_centrality(G)print sorted(bc.items(), key=lambda k: k[1], reverse=True)# [(4, 0.4), (2, 0.35000000000000003), (1, 0.1), (3, 0.05), (5, 0.0), (6, 0.0)]# spring layoutpos = nx.spring_layout(G)nx.draw(G, pos, with_labels = True, node_size = 100,font_size=6,font_color='b')plt.show() 另一个复杂的例子 import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df[\"node1\"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis(\"off\")def most_important(G): \"\"\" returns a copy of G with the most important nodes according to the pagerank \"\"\" ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')plt.savefig(\"../output/FB_BetCen.png\", dpi = 300)###\"\"\"node1,node20,10,20,30,40,50,60,70,8...2420,25432420,25552420,25672420,25922420,25972420,25982420,26092420,26172420,26292420,26422420,26432420,26532421,24372421,26342422,24412422,2558...\"\"\" 4. 社区发现（Community detection）import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df[\"node1\"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis(\"off\")#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig(\"../output/FB_commu.png\", dpi = 300)# get modularitymod = community.modularity(part,G)print(\"modularity:\", mod) 也可以在betweenness centrality的基础上画community detection: import networkx as nximport matplotlib.pyplot as pltimport communityimport pandas as pdimport sys# Exploratory Data Analysis# datadf = pd.read_csv(sys.argv[1])#node1 = list(df[\"node1\"])G = nx.from_pandas_dataframe(df, 'node1', 'node2', #edge_attr='weight', #create_using=nx.MultiGraph() )#Quick snapshot of the Networkprint nx.info(G)#Create network layout for visualizationsspring_pos = nx.spring_layout(G)plt.axis(\"off\")def most_important(G): \"\"\" returns a copy of G with the most important nodes according to the pagerank \"\"\" ranking = nx.betweenness_centrality(G).items() #print ranking r = [x[1] for x in ranking] m = sum(r)/len(r) # mean centrality t = m*10 # threshold, we keep only the nodes with 10 times the mean Gt = G.copy() for k, v in ranking: if v &lt; t: Gt.remove_node(k) return GtGt = most_important(G) # trimming# draw the nodes and the edges (all)nx.draw_networkx_nodes(G,spring_pos,node_color='b',alpha=0.2,node_size=8)nx.draw_networkx_edges(G,spring_pos,alpha=0.1)# draw the most important nodes with a different stylenx.draw_networkx_nodes(Gt,spring_pos,node_color='r',alpha=0.4,node_size=254)# also the labels this timenx.draw_networkx_labels(Gt,spring_pos,font_size=6,font_color='b')#part = community.best_partition(G)values = [part.get(node) for node in G.nodes()]nx.draw_networkx(G, pos = spring_pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)plt.savefig(\"../output/FB_BC_commu.png\", dpi = 300)# get modularitymod = community.modularity(part,G)print(\"modularity:\", mod)","categories":[{"name":"notes","slug":"notes","permalink":"https://hui-liu.github.io/categories/notes/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"如何在遍历时，正确删除列表中的 items","slug":"如何在遍历时，正确删除列表中的-items","date":"2017-03-08T09:21:23.000Z","updated":"2017-03-08T09:22:48.197Z","comments":true,"path":"blog/如何在遍历时，正确删除列表中的-items/","link":"","permalink":"https://hui-liu.github.io/blog/如何在遍历时，正确删除列表中的-items/","excerpt":"","text":"错误的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x: if i in y: x.remove(i)print x-----------------['a', 'c', 'd'] 正确的代码x = ['a', 'b', 'c', 'd']y = ['b', 'c']for i in x[:]: if i in y: x.remove(i)print x-----------------['a', 'd'] 实际上，id(x)与id(x[:])是不同的，所以只有在x的副本（x[:]）中遍历，然后在x中删除，才不会造成错误。","categories":[{"name":"code","slug":"code","permalink":"https://hui-liu.github.io/categories/code/"}],"tags":[{"name":"python","slug":"python","permalink":"https://hui-liu.github.io/tags/python/"}]},{"title":"根据一个list文件生成一个组合","slug":"根据一个list文件生成一个组合","date":"2017-03-05T08:35:07.000Z","updated":"2017-03-05T08:50:52.091Z","comments":true,"path":"blog/根据一个list文件生成一个组合/","link":"","permalink":"https://hui-liu.github.io/blog/根据一个list文件生成一个组合/","excerpt":"","text":"假设有一个 list 如下： $ cat aaabcd 期望生成如下组合： a ba ca db cb dc d 实现方法如下： #!/bin/bashset -- $(cat $1) # 将输入文件的每一行依次赋值给位置变量，如第一行赋值给 $1，第二行给 $2。。。for i in $* # $* 为所有位置变量的 listdoshift for j in $* do printf \"%s\\t%s\\n\" \"$i\" \"$j\" donedone $ ./pair_combination.sh aaa ba ca db cb dc d","categories":[{"name":"linux","slug":"linux","permalink":"https://hui-liu.github.io/categories/linux/"}],"tags":[{"name":"linux shell","slug":"linux-shell","permalink":"https://hui-liu.github.io/tags/linux-shell/"}]},{"title":"Detecting pervasive positive selection step by step","slug":"Detecting-pervasive-positive-selection","date":"2017-03-02T09:01:33.000Z","updated":"2017-03-03T14:03:45.599Z","comments":true,"path":"blog/Detecting-pervasive-positive-selection/","link":"","permalink":"https://hui-liu.github.io/blog/Detecting-pervasive-positive-selection/","excerpt":"","text":"Site-model: assumsing that the dn/ds rato is the same across branches, but different between sites(1) multiple sequence alignment speed: muscle &gt; mafft &gt; clustalW &gt; T-Coffee accuracy: mafft &gt; muscle &gt; T-Coffee &gt; clustalW cd /home/liuhui/nature_selection/exemple/data_for_codemlmafft-linsi ../input/HLA_DQB1.aa.fasta &gt; HLA_DQB1.aa.mafft.fasta (2) convert protein alignment to cds alignmentperl /home/liuhui/nature_selection/exemple/bin/pepMfa_to_cdsMfa.pl HLA_DQB1.aa.mafft.fasta ../input/HLA_DQB1.cds.fasta &gt; HLA_DQB1.cds.mafft.fasta (3) Remove spurious sequences and columns aa sequences (construct gene tree) trimal -automated1 -in HLA_DQB1.aa.mafft.fasta -out HLA_DQB1.aa.mafft.trimal.fasta -htmlout HLA_DQB1.aa.mafft.trimal.html -colnumbering &gt; HLA_DQB1.aa.mafft.trimal.cols cds sequences (for codeml) python /home/liuhui/nature_selection/bin/MSA_triplets_gaps_removed.py HLA_DQB1.cds.mafft.fasta HLA_DQB1.cds.mafft_removed_trigaps.fasta (4) convert fasta to phylip format/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.aa.mafft.trimal.fasta HLA_DQB1.aa.mafft.trimal.phy # construct tree/home/liuhui/nature_selection/exemple/bin/convert_fasta2phylip.py HLA_DQB1.cds.mafft_removed_trigaps.fasta HLA_DQB1.cds.mafft_removed_trigaps.phy # for codeml (5) construct treephyml -i HLA_DQB1.aa.mafft.trimal.phy -q -d aa -m JTT -c 4 -a esed 's/\\()\\)[0-9]\\.[^:]*:/\\1:/g' HLA_DQB1.aa.mafft.trimal.phy_phyml_tree.txt &gt; HLA_DQB1.aa.mafft.trimal.tree (6) codeml# M0M1M2M3M7M8cd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M0M1M2M3M7M8cd HLA_DQB1_M0M1M2M3M7M8codeml HLA_DQB1_M0M1M2M3M7M8.ctl# M8acd /home/liuhui/nature_selection/exemple/output/mkdir HLA_DQB1_M8acd HLA_DQB1_M8acodeml HLA_DQB1_M8a.ctl (7) significant test np: the number of parameters lnL: log-likelihood value LRT: likelihood-ratio test Model_compared Model0 np0 lnL0 Model1 np1 lnL1 df LRT pvalueM7-M8 M7 44 -5047.785978 M8 46 -5011.936805 2 71.6983 2.69719269066922e-16M0-M3 M0 43 -5214.976615 M3 47 -5011.542624 4 406.868 9.12618975872726e-87M8-M8a M8a 45 -5031.655392 M8 46 -5011.936805 1 39.4372 3.38781154892534e-10M1a-M2a M1a 44 -5036.170805 M2a 46 -5014.302814 2 43.736 3.18308524710324e-10 (8) identification of sitesM2a Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 38 F 0.938 3.258 +- 0.761 55 L 0.999** 3.408 +- 0.518 66 Y 0.837 2.994 +- 0.991 86 D 0.997** 3.404 +- 0.527 99 G 0.978* 3.356 +- 0.615 99 G 0.978* 3.356 +- 0.615116 F 0.935 3.243 +- 0.766118 G 0.662 2.542 +- 1.171123 R 0.690 2.646 +- 1.182256 P 0.998** 3.406 +- 0.523257 Q 0.864 3.073 +- 0.947258 G 0.968* 3.334 +- 0.659259 P 0.776 2.838 +- 1.079260 P 0.971* 3.342 +- 0.644 M8 Bayes Empirical Bayes (BEB) analysis (Yang, Wong &amp; Nielsen 2005. Mol. Biol. Evol. 22:1107-1118)Positively selected sites (: P&gt;95%; *: P&gt;99%)(amino acids refer to 1st sequence: ENSP00000364080) Pr(w&gt;1) post mean +- SE for w 14 T 0.539 1.770 +- 1.006 38 F 0.985* 2.688 +- 0.473 42 G 0.649 1.992 +- 0.956 55 L 1.000** 2.715 +- 0.417 66 Y 0.962* 2.641 +- 0.547 86 D 0.999** 2.714 +- 0.419 99 G 0.996** 2.709 +- 0.431113 E 0.518 1.714 +- 0.948116 F 0.989* 2.694 +- 0.459117 R 0.585 1.855 +- 0.957118 G 0.927 2.564 +- 0.632123 R 0.837 2.393 +- 0.822256 P 1.000** 2.715 +- 0.418257 Q 0.956* 2.630 +- 0.569258 G 0.992** 2.700 +- 0.451259 P 0.947 2.609 +- 0.588260 P 0.993** 2.703 +- 0.443","categories":[{"name":"Evolution","slug":"Evolution","permalink":"https://hui-liu.github.io/categories/Evolution/"}],"tags":[{"name":"positive selection","slug":"positive-selection","permalink":"https://hui-liu.github.io/tags/positive-selection/"}]},{"title":"creat a blog","slug":"creat a blog","date":"2017-03-02T05:01:44.000Z","updated":"2017-03-16T08:02:24.927Z","comments":true,"path":"blog/creat a blog/","link":"","permalink":"https://hui-liu.github.io/blog/creat a blog/","excerpt":"","text":"1. hexo new \"new blog title\"2. edit your text using Typora3. hexo generate4. hexo deploy","categories":[{"name":"blog","slug":"blog","permalink":"https://hui-liu.github.io/categories/blog/"}],"tags":[{"name":"notes","slug":"notes","permalink":"https://hui-liu.github.io/tags/notes/"}]}]}